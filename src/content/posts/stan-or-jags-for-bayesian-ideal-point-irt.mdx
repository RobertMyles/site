---
title: Stan or JAGS for Bayesian ideal-point IRT?
date: "2016-04-13"
---

import { InlineMath, BlockMath } from "react-katex"

Anybody who has ever tried to run even a moderately-sized Bayesian IRT model in R (for ideal points as in the political science literature, or otherwise) will know that these models can take a _long_ time. Itâ€™s not Râ€™s fault: these are usually big models with lots of parameters, and naturally take longer.[^1] Not to mention the fact that Bayesian computation is more computationally intense than other methods. Historically (okay, Iâ€™m talking about the last twenty years, maybe â€˜historicallyâ€™ is a little strong), the sampling software [BUGS](http://www.mrc-bsu.cam.ac.uk/software/bugs/) (**B**ayesian **I**nference **U**sing **G**ibbs **S**ampling) and then [JAGS](http://mcmc-jags.sourceforge.net/) were used to run Bayesian models (JAGS is still pretty common, and BUGS too, though not as much). Lately, [Stan](http://mc-stan.org/) has been gaining ground, certainly as regards more complex modelling.

While the reasons for choosing Stan are often put down to speed, when running many types of models there is not actually a large difference, with JAGS actually being faster for some models, according to John Kruschke[^2]. Given the lack of a big difference between JAGS/BUGS and Stan, which sampling software should we use for IRT models? Well, first of all, a large part of the literature utilises either JAGS or BUGS, indeed, code is publicly available for many of these models, helping to spread the use of these two modelling languages.[^3] For beginners, this is a handy way to learn, and itâ€™s how I learned. Indeed, the language of JAGS/BUGS (Iâ€™m just going to use â€˜JAGSâ€™ to refer to both from now on) is a bit more intuitive for many people, and given the availability of othersâ€™ code, beginning with these models can then be reduced to just tinkering with small details of code that is already written.

Stan, on the other hand, is newer and has a syntax that is in some ways quite different from JAGS. Variables need to be declared, as does their type (something not many R users are familiar with, I certainly wasnâ€™t). The model code is imperative, not declarative[^4], and there are specific â€˜blocksâ€™ to the code. Stan has a different default sampler and is generally argued by its creators to be much faster. Well, in my experience, there is actually no contest. As much as I liked JAGS when I started out, Stan is simply incomparable to JAGS in terms of speed for these modelsâ€“ Stan is much, much faster. I was analysing nominal vote data for the Brazilian Federal Senate[^5] (these data have plenty of missing values, which are handled easily in JAGS but have to be deleted out in Stan) and, through the use of the <a href="http://runjags.sourceforge.net/quickjags.html">runjags</a> package (and its `autorun` option), I discovered that it would take around 28 hours to run my two-dimensional model to reach signs of convergence (or signs of non-convergence, as [Gill](pan.oxfordjournals.org/content/16/2/153.full.pdf) puts it). As I was in the middle of writing a PhD thesis with lots of these models to process, that just wasnâ€™t an option. (Regardless, any time I let the model run like this, R crashed or became unresponsive, or the estimates were simply of bad quality.) So I started tinkering with the options in `runjags`, trying different samplers etc. Then I noticed exactly _why_ JAGS is so slow for these models.

In order to run a model, JAGS first compiles a Directed Acyclic Graph (DAG) of all the nodes in the model (software such as <a href="http://r-nimble.org/">NIMBLE</a> will let you print out the graph pretty easily). But since we have a _latent_ regression with an _unobserved_ regressor in the equation[^6]

<BlockMath math="y_{ij} = \beta_j\bf{x_i} - \alpha_j" />

then JAGS is [unable](https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/5c9e9026/) to build such a DAG. Since it canâ€™t build a DAG, it canâ€™t surmise that there is conjugacy in the model and then exploit that through Gibbs sampling. So JAGS just uses the default Metropolis-Hastings sampler (and given that it is called **J**ust **A**nother **Gibbs** **S**ampler, it kind of misses the point of using JAGS in the first place). This means that all the gains available through Gibbs sampling are simply not available for latent models of this type with JAGS, and hence the sampling process runs _very_ slowly. Iâ€™m not sure the literature was ever aware of this fact, either. Many papers and books extoll the virtues of Gibbs sampling (and spend pages and pages deriving the conditional distributions involved) and then show the reader how to do it in JAGS or BUGS (see Simon Jackmanâ€™s [book](http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470011548.html) for an example)[^7], but unbeknownst to these authors, their JAGS programs are not using Gibbs sampling.

So that leaves us with Stan. Use it! ðŸ˜Ž

_In a future post, Iâ€™ll show some examples of IRT ideal-point models in Stan. I have some on my [Github](https://github.com/RobertMyles/Bayesian-Ideal-Point-IRT-Models), and Pablo BarberÃ¡ also has some nice [examples](https://github.com/pablobarbera/quant3materials/tree/master/bayesian) (hat tip: I learned from him, amongst others. Thanks, Pablo!)._

_Update: [Guilherme Jardim Duarte](https://github.com/duarteguilherme/Quinn-Martin-Replication) also has some Bayesian IRT examples on his Github, in particular the dynamic model of [Martin & Quinn](http://mqscores.berkeley.edu/media/pa02.pdf), have a look._

[^1]: For more on how these models can have _tons_ of parameters, see [Clinton, Jackman, and Rivers (2004)](https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/ClintonJackmanRivers2004.pdf): â€˜The statistical analysis of roll-call dataâ€™, _American Political Science Review_, Vol. 98, No.Â 2.
[^2]: [Kruschke](http://doingbayesiandataanalysis.blogspot.com.br/) mentions this in his bookâ€¦not sure where, exactly.
[^3]: See this paper by [Curtis](https://www.jstatsoft.org/article/view/v036c01/v36c01.pdf) (pdf downloads automatically) or the book by [Armstrong et. al](https://www.crcpress.com/Analyzing-Spatial-Models-of-Choice-and-Judgment-with-R Armstrong-II-Bakker-Carroll-Hare-Poole-Rosenthal/9781466517158).
[^4]: See [here](http://stackoverflow.com/questions/129628/what-is-declarative-programming) for the difference.
[^5]: You can read about this research [here](%7B%7B%20site.url%20%7D%7D/assetsExplaining%20the%20Determinants%20of%20Foreign%20Policy%20Voting%20Behaviour%20in%20the%20Brazilian%20Houses%20of%20Legislature.pdf).[â†©](#a5)
[^6]: This is the canonical statistical model for Bayesian IRT. The data (<InlineMath math="y_{ij}"/>) are the votes, in binary form (1 = â€˜Yesâ€™; 2 = â€˜Noâ€™); the <InlineMath math="x_i"/> are the ideal points of the legislators; and <InlineMath math="\beta_j"/> and <InlineMath math="\alpha_j"/> are the _discrimination_ (slope) and _difficulty_ (intercept) parameters, respectively. See the article cited in footnote 1.
[^7]: I donâ€™t mean to denigrate Jackmanâ€™s book. Itâ€™s highly detailed and thorough, and he deserves a lot of credit for spearheading the use of these Bayesian IRT models in political science. Iâ€™ve cited his work numerous times, Iâ€™m a fan. <link rel="image_src" href="http://i.imgur.com/v7y6SVt.png?1" />
