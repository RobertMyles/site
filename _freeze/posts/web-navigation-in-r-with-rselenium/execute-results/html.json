{
  "hash": "494f36d741d6c359848baf6177eb335e",
  "result": {
    "markdown": "---\ntitle: \"Web Navigation in R with RSelenium\"\ndate: \"2016-04-27\"\ncategories: [webscraping]\nexecute:\n  eval: false\n---\n\n\nIt goes almost without saying that the internet itself is the richest database available to us. From a 2014 [blog post](https://aci.info/2014/07/12/the-data-explosion-in-2014-minute-by-minute-infographic/), it was claimed that *every minute* :\n\n-   Facebook users share nearly 2.5 million pieces of content.\n-   Twitter users tweet nearly 300,000 times.\n-   Instagram users post nearly 220,000 new photos.\n-   YouTube users upload 72 hours of new video content.\n-   Apple users download nearly 50,000 apps.\n-   Email users send over 200 million messages.\n-   Amazon generates over 80,000 dollars in online sales.\n\nRegardless of the accuracy of these claims, it is obvious to everyone that there is tons of information on the web. For researchers, then, the question is: how can you access all this information? You can of course go to specific, dedicated databases and download what youâ€™re looking for, for example from the World Bank [databank](https://databank.worldbank.org/data/home.aspx). However, there are drawbacks to this approach. It can become tiresome when you need to collect lots of data on different items (the World Bank databank is well organised, but not all databases are like thatâ€¦to put it politely). Some only let you download small, specific sections of a bigger database, meaning you have to return time and time again to the starting page to enter new information in order to retrieve the data you want. (Another thing is that weâ€™re not quite utilising the web *itself* as the database either.)\n\nTo deal with the first problem, you can automate the search process by driving a web browser with R.[^1] This is different from â€˜web-scrapingâ€™. Web-scraping takes the webpage as a html document and allows you to read information from it. Itâ€™s quite a straightforward process, with plenty of R packages around to help you do it. [rvest](https://github.com/hadley/rvest) in particular is quite easy, although Iâ€™ve found the [XML](https://cran.r-project.org/web/packages/XML/XML.pdf) package to be more powerful. (Web-scraping deals with the second issue above, in that it does treat the web itself as a database.)\n\n[^1]: It is often argued that R is not the best for this application, with Python often offered as a better alternative. In my experience, Iâ€™ve found R to be pretty good for this sort of thing, with delays being caused more by the browser/net speed than R. The scripts can be ugly, but using Selenium in Python looks pretty similar anyway. [This question](https://stackoverflow.com/questions/17540971/how-to-use-selenium-with-python) on Stack Overflow gives some instructions.\n\nTo drive a web browser in R, there are two packages (that Iâ€™m aware of) that can be used. One is [RSelenium](https://github.com/ropensci/RSelenium) by John Harrison, and [Rwebdriver](https://github.com/crubba/Rwebdriver) by Christian Rubba. I prefer `RSelenium` and so Iâ€™ll use this package in the examples below.\n\nIf you donâ€™t have it already installed, youâ€™ll need to download this package and load it into R.\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-1_25ef9202382344e5b38cc94540d25c8d'}\n\n```{.r .cell-code}\ninstall.packages(\"RSelenium\")\nlibrary(\"RSelenium\")\n```\n:::\n\n\nYou will also need to download the Selenium standalone server. You can get it from [here](https://www.seleniumhq.org/download/). Opening this file automatically from `RSelenium` can be problematic[^2], and so Iâ€™ve found the most straightforward way is to manually click on it and open it that way before you start.\n\n[^2]: See [this](https://github.com/ropensci/RSelenium/issues/54) discussion.\n\nTo get started with `RSelenium`, youâ€™ll need to give your browser somewhere to go. For this example, Iâ€™m going to go to the funding management section of Brazilian National Health Service, the *Fundo Nacional de SaÃºde*. From here, Iâ€™m going to get data for every municipality in every state over a period of some years. To do this manually would be a serious headache and would most likely lead to me making errors by forgetting where I am, which state is next, what municipality I just downloaded, and so on. Actually, you can be guaranteed Iâ€™d make those mistakes.\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-2_3e531afd92f6d456ba28ac3bf07f57c2'}\n\n```{.r .cell-code}\nURL <- \"https://www.fns.saude.gov.br/indexExterno.jsf\"\n#checkForServer(dir=\"[DIRECTORY WHERE THE SELENIUM SERVER IS]\", update=FALSE)\n#checkForServer(dir=\"[DIRECTORY WHERE THE SELENIUM SERVER IS]\", update=TRUE) # if you want to update\n#startServer(dir=\"[DIRECTORY WHERE THE SELENIUM SERVER IS]\") #none of these three are necessary if you click on the server first and manually open it.\n\nfprof <- makeFirefoxProfile(list(browser.download.dir = \"[DOWNLOAD DIRECTORY]\",\nbrowser.download.folderList = 2L,\nbrowser.download.manager.showWhenStarting=FALSE,\nbrowser.helperApps.neverAsk.saveToDisk = \"application/octet-stream\"))\n\nremDr <- remoteDriver(extraCapabilities=fprof)\nremDr$open()\n```\n:::\n\n\nSo now your browser should be open. Here Iâ€™ve used a profile for Firefox because I will download files and I donâ€™t want to deal with the download window that pops up in Firefox (you need to enter your download folder where it says â€˜`[DOWNLOAD DIRECTORY]`â€™, by the way. And you can also run `RSelenium` on Chrome and [other browsers](https://rpubs.com/johndharrison/13885), and even use a [headless browser](https://rpubs.com/johndharrison/RSelenium-headless) which speeds things up.) If you didnâ€™t need to deal with download boxes and pop-ups and the like, you only need `remDr <- remoteDriver$new()`, which will automatically open up a Firefox browser window. These particular files were recognised by Firefox as being binary files, and so I have disabled the download box for files of the type â€œapplication/octet-streamâ€. Other file types need a different setting.\n\nThis website has a drop-down box on the left hand side that weâ€™re going to use. What we will input into this is, in turn, a list of years, states, and municipalities. After that we will click on â€œConsultarâ€ (for those of you who donâ€™t speak Portuguese, Iâ€™m quite sure you can figure out what that means). Clicking this will bring us to a new page, from which we can download the data weâ€™re looking for in a .csv file.\n\nSo letâ€™s create our inputs:\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-3_315279e80fd57597cb24d4da43b50fc6'}\n\n```{.r .cell-code}\nInputYear <- list(\"2016\", \"2015\", \"2014\", \"2013\", \"2012\", \"2011\", \"2010\", \"2009\")\n\nInput <- list(\"ACRE\", \"ALAGOAS\", \"AMAPA\", \"AMAZONAS\", \"BAHIA\", \"CEARA\", \"DISTRITO FEDERAL\", \"ESPIRITO SANTO\", \"GOIAS\", \"MARANHAO\", \"MATO GROSSO\", \"MATO GROSSO DO SUL\", \"MINAS GERAIS\", \"PARA\", \"PARAIBA\", \"PARANA\", \"PERNAMBUCO\", \"PIAUI\", \"RIO DO JANEIRO\", \"RIO GRANDE DO NORTE\", \"RIO GRANDE DO SUL\", \"RONDONIA\", \"RORAIMA\", \"SANTA CATARINA\", \"SAO PAULO\", \"SERGIPE\", \"TOCANTINS\")\n\nInput_Mun <- \"TODOS DA UF\" #this will select all municipalities\n```\n:::\n\n\nIn order to get all this done, I will use a for loop in R which will first loop over the years, and then states, thereby selecting all states in a given year. In the following code, you will see `RSelenium` commands that are quite different to regular commands in R. First of all, `RSelenium` operates by way of two environments: one is remoteDriver environment, the other a webElement environment. These have specific options available to them (see the help section on each for a list and explanations). Some of the most useful are `findElement()` (an option of remoteDriver), `sendKeystoElement()` and `clickElement()` (both options of webElement, as `remDr$findElement` returns an object of webElement class). We will use these to navigate around the page and click on specific elements.\n\nSpeaking of elements on a page, this is actually the most crucial part of the process to get right (and can be the most frustrating). Some have recommended [selectorgadget](https://selectorgadget.com/), but finding elements can be done in Firefox or Chrome without selectorgadget â€“ you just right-click the element in question and select â€œInspectâ€ or â€œInspect Elementâ€. This will bring up a chaotic-looking panel, full of html, css and javascript code. Luckily, there are easy options in Firefox and Chrome for finding what we need. After you right-click the element that you want (the one you would have clicked if you were navigating the page manually), click â€œInspectâ€ and then this element of the html code will be highlighted. Right-click on this again and you will see the option to copy. In Chrome, you will have the option to copy the xpath or css selector (â€œselectorâ€); in Firefox you can copy the css selector (â€œunique selectorâ€). I have used other options below to give more examples, such as â€˜idâ€™. This can be copied directly from the html code, and â€˜classâ€™ and â€˜nameâ€™ can be used in a similar fashion. In general, css selectors are the easiest to work with.\n\nA quick note on some other aspects of the code. `Sys.sleep` is used in order to be niceâ€“ you donâ€™t want to bombard the website with all of your requests in rapid-fire fashion; after all, they may block you. So this spaces out our commands. This is also useful for when you may have to wait for an element to load on the page before you can click on it. I have used `paste()` in order to include the loop counters in the css selectorâ€“ just a little trick to make things easier. Some elements have `\\\\` in the code: this is because the original had a single backslash, which is an escape character in R, and so the string is unreadable. Hence the added backslash. You will also see the use of `try()` â€“ in this case, there is a state that does not load like the others (the Federal District) and so this automated process will not work here. `try()` allows R to try anyway, and if it fails, the loop just continues to the next iteration.\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-4_589320a9e2bfbeb51258b20786a8bcae'}\n\n```{.r .cell-code}\nfor(i in 1:length(InputYear)){\n    for(j in 1:length(Input)){\n    remDr$navigate(URL)\n    #Year:\n    webElem <- remDr$findElement(using = \"id\", value = \"formIndex:j_idt48\")\n    webElem$clickElement() #click on the drop-down year box\n    Sys.sleep(2)\n    webElem <- remDr$findElement(using = \"id\", value=\"formIndex:j_idt48_input\")\n    Sys.sleep(2)\n    webElem$sendKeysToElement(InputYear[i]) #send the year to the box\n    webElem <- remDr$findElement(using = \"css\", value=\"li.ui-state-active\")\n    webElem$clickElement() #click on the active element (the year we sent)\n    Sys.sleep(2)\n    #State:\n    webElem <- remDr$findElement(using = \"id\", value = \"formIndex:sgUf\")\n    webElem$clickElement()\n    Sys.sleep(2)\n    webElem$sendKeysToElement(Input[j]) #enter the state into the drop-down box\n    CSS <- paste(\"#formIndex\\\\3a sgUf_panel > div > ul > li:nth-child(\", j+2, \")\", sep=\"\")\n    webElem <- remDr$findElement(using = \"css\", value = CSS)\n    Sys.sleep(1)\n    webElem$clickElement()\n    Sys.sleep(3)\n    #Municipality:\n    webElem <- remDr$findElement(using = 'id', value = 'formIndex:cbMunicipio')\n    webElem$clickElement()\n    Sys.sleep(2)\n    webElem <- remDr$findElement(using = 'css', value='#formIndex\\\\3a cbMunicipio_panel > div > ul > li:nth-child(2)')\n    webElem$sendKeysToElement(list(Input_Mun))\n    webElem$clickElement()\n    Sys.sleep(4)\n    #\"Consultar\":\n    webElem <- remDr$findElement(using = 'xpath', value = '//*[@id=\"formIndex:j_idt60\"]')\n    Sys.sleep(2)\n    webElem$clickElement()\n    Sys.sleep(6)\n    #Download the .csv:\n    webElem <- try(remDr$findElement(using = 'xpath', value = '//*[@id=\"formIndex\"]/div[4]/input'), silent=T)\n    try(webElem$clickElement(), silent=T)\n    Sys.sleep(3)\n}}\n```\n:::\n\n\nSo after all this, weâ€™ll have a bunch of .csv files in out download folder, that you can import into R and mess around with. To load them all in together, you could use the following code:\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-5_297e79c6008e30c94d964d608693c4ac'}\n\n```{.r .cell-code}\nlibrary(\"readr\")\nsetwd(\"[THE DOWNLOAD FOLDER YOU USED]\")\nfileNames <- list.files(path = getwd(), pattern = \"*.csv\")\ndata <- rbindlist(lapply(fileNames, read_csv2,\ncol_names=c(\"Ano\", \"UF_MUNICIPIO\", \"IBGE\", \"ENTIDADE\", \"CPF_CNPJ\",\n\"Bloco\", \"Componente\", \"Acao_Servico_Estrategia\", \"Competencia_Parcela\",\n\"No_OB\", \"Data_OB\", \"Banco_OB\", \"Agencia_OB\", \"Conta_OB\", \"Valor_Total\",\n\"Desconto\", \"Valor_Liquido\", \"Observacao\", \"Processo\", \"Tipo Repasse\",\n\"No_Proposta\"), skip = 1, locale=locale(decimal_mark=\",\", grouping_mark=\".\")))\n```\n:::\n\n\nAnd there you go, all the data you wanted scraped automatically from the web. In this example, we were downloading a file, but you could be navigating around in order to arrive at a certain page and then to scrape the contents of that page. You can do that in a number of ways, by combining `RSelenium` and other packages, such as `XML` and `rvest`. For a solution using only `RSelenium`, we can first create an empty dataframe and then fill it with the `getElementText()` option of the webElement class. So, for example, I was getting vote proposal content from the Brazilian Senate. I used `RSelenium` to navigate to the pages that I wanted, as is shown above, and then I stored the Content and the Index of the vote (which were stored on the page as html text elements) as entries in the Index dataframe, using `webElem$getElementText()`. Afterwards, I used various combinations of `stringr` package functions and `gsub` to clean up the text.\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-6_80463ae55705f71c8f022a35fed1a657'}\n\n```{.r .cell-code}\nIndex <- data.frame(Content=NA, Index=NA)\nIndex[i,1] <- webElem$getElementText()\n...\nIndex[i,2] <- webElem$getElementText()\n```\n:::\n\n\nYou can also get the html and parse it using `XML`:\n\n\n::: {.cell hash='web-navigation-in-r-with-rselenium_cache/html/unnamed-chunk-7_e89cd8fb0e608fb2603ab842dc81832a'}\n\n```{.r .cell-code}\nelemtxt <- webElem$getElementAttribute(\"outerHTML\")\nelemxml <- htmlTreeParse(elemtxt, asText=TRUE, encoding=\"UTF-8\", useInternalNodes=TRUE)\nText <- html_text(elemxml, trim=TRUE)\n```\n:::\n\n\nAnd then you have the text from the webpage stored as data in R. Magic! ðŸ¤Ÿ\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}