[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! I‚Äôm Rob, this is my site. I blog here from time to time on topics that interest me, purely for my own amusement. If you‚Äôd like to get in touch, email me at robertmylesmcdonnell@gmail.com.\nThis website was made with quarto."
  },
  {
    "objectID": "posts/shuffle-strings-r.html",
    "href": "posts/shuffle-strings-r.html",
    "title": "Shuffling Strings in R",
    "section": "",
    "text": "Let‚Äôs say you need to share some data that has some potentially identifiable sensitive information in it ‚Äì people‚Äôs addresses, phone numbers etc. Maybe these fields are not particularly important, but you don‚Äôt want to take them out exactly, and neither do you want to have to go through an encryption & decryption process‚Ä¶well, one quick and useful option is the stri_rand_shuffle() function from the stringi package.\nImagine you have the following fake data:\n\nlibrary(stringi); library(dplyr)\n\ndf &lt;- tibble(\n  name = c(\"John Smyth\", \"Alan Pear\", \"Don Baker\", \"Bjarne Andersson\"),\n  client_id = c(243, 22, 441, 994),\n  address_1 = c(\"2 Corner View Road\", \"106 Southfield Ave.\", \"213 North 25th Street\",\n                \"11 Apple Boulevard\"),\n  address_2 = c(\"Dunkirk Springs\", \"Ballyvourney\", \"Oakland Heights\", \"Rintinville\"),\n  address_3 = c(\"New York\", \"Cork\", \"Essex\", \"Stockholm\"),\n  phone_number = c(\"99-2278-122\", \"088-766653221\", \"112341-991\", \"011-221-324\"),\n  zip_code = c(11517, \"E45NN12\", \"WX133Y\", 213337),\n  registration = c(\"Full\", \"Part-time\", \"Full\", \"Part-time\"),\n  profile = c(\"Advanced\", \"Advanced\", \"Beginner\", \"Intermediate\")\n)\n\nhead(df)\n\n## # A tibble: 4 x 9\n##   name  client_id address_1 address_2 address_3 phone_number zip_code\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;\n## 1 John‚Ä¶       243 2 Corner‚Ä¶ Dunkirk ‚Ä¶ New York  99-2278-122  11517\n## 2 Alan‚Ä¶        22 106 Sout‚Ä¶ Ballyvou‚Ä¶ Cork      088-7666532‚Ä¶ E45NN12\n## 3 Don ‚Ä¶       441 213 Nort‚Ä¶ Oakland ‚Ä¶ Essex     112341-991   WX133Y\n## 4 Bjar‚Ä¶       994 11 Apple‚Ä¶ Rintinvi‚Ä¶ Stockholm 011-221-324  213337\n## # ‚Ä¶ with 2 more variables: registration &lt;chr&gt;, profile &lt;chr&gt;\n‚Ä¶and suppose we‚Äôre interested in Client ID, site/region, ZIP code, registration and profile. We can quickly scramble the identifying information we have in the other columns with string & dplyr:\n\ndf %&gt;%\n  mutate(phone_number = as.character(phone_number)) %&gt;%\n  mutate_at(vars(name, address_1, address_2, phone_number), stri_rand_shuffle)\n\n## # A tibble: 4 x 9\n##   name  client_id address_1 address_2 address_3 phone_number zip_code\n##   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;\n## 1 \"ntJ‚Ä¶       243 RV2 edwr‚Ä¶ nnuriSsk‚Ä¶ New York  1-928-22297  11517\n## 2 \" aA‚Ä¶        22 d6h itle‚Ä¶ eBlnyyru‚Ä¶ Cork      22610588676‚Ä¶ E45NN12\n## 3 e rk‚Ä¶       441 t 2 rthS‚Ä¶ etdHnkaa‚Ä¶ Essex     39-4121911   WX133Y\n## 4 nBjn‚Ä¶       994 p1erAaud‚Ä¶ vltiRlne‚Ä¶ Stockholm 3421-2102-1  213337\n## # ‚Ä¶ with 2 more variables: registration &lt;chr&gt;, profile &lt;chr&gt;\nIf you want a closer look at, for example, the address_1 column:\n\ndf %&gt;%\n  mutate(phone_number = as.character(phone_number)) %&gt;%\n  mutate_at(vars(name, address_1, address_2, phone_number), stri_rand_shuffle) %&gt;%\n  pull(address_1)\n\n## [1] \"rdrn woa e VoiRCe2\"    \".it0e6SoAf h ld1veu\"   \"S oe t2r3et215hNtrt h\"\n## [4] \"u vA 1reoealBpp1dl\"\nUsing this method, you can share the dataset without concern üòé."
  },
  {
    "objectID": "posts/UKElections2019.html",
    "href": "posts/UKElections2019.html",
    "title": "UK Elections 2019",
    "section": "",
    "text": "After the UK elections in 2017, I posted about how easy it was to plot the results in R. Given that the UK just had another election, I thought I‚Äôd update that post with another one. So here ya go.\nWhat we‚Äôll do is make a plot of the results using R, and then we‚Äôll compare it to the last election using static plots and a GIF. You‚Äôll need to install the relevant packages below if you plan to try this out yourself.\nThe data for the results is hosted on my github, here we‚Äôll just read it in from there. The only other work pre-plotting is joining the results and the geographic data and combining some smaller parties into an ‚ÄúOther‚Äù category, which is not really necessary, but leaves the plot cleaner. Actually, most of the work is choosing the colours!!\n\nxfun::pkg_attach(c(\"jsonlite\", \"dplyr\", \"rnaturalearthhires\", \"sf\", \"parlitools\", \"ggplot2\", \"patchwork\", \"magick\"))\n\nres_file &lt;- \"https://raw.githubusercontent.com/RobertMyles/blogdata/master/results.json\"\nuk &lt;- west_hex_map %&gt;%\n  select(ons = gss_code, geometry)\n\nresults &lt;- fromJSON(res_file) %&gt;%\n  as_tibble() %&gt;%\n  full_join(uk) %&gt;%\n  select(-candidates) %&gt;%\n  st_as_sf() %&gt;%\n  mutate(\n    party = case_when(\n      winningParty %in% c(\"Speaker\", \"Green\", \"Alliance\", \"PC\", \"SDLP\") ~ \"Other\",\n      TRUE ~ winningParty\n    )\n  ) %&gt;%\n  filter(!is.na(party))\n\nggplot(results) +\n  geom_sf(aes(fill = party), size = 0.2) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        panel.grid = element_blank(),\n        plot.title.position = \"plot\") +\n  guides(fill = guide_legend(title = \"Party\")) +\n  scale_fill_manual(values = c(\"#0084c6\", \"#880105\", \"#c70000\",\n                                \"#ee6f00\", \"#888888\", \"#236925\",\n                                \"#ffe500\")) +\n  ggtitle(\"UK Elections 2019\")\n\n\nOverwhelming rejection of Corbyn there.\nWith patchwork (recently CRANified, yay), we can compare 2017‚Äôs winners with the winners this year:\n\nfirst &lt;- ggplot(results) +\n  geom_sf(aes(fill = sittingParty), size = 0.2) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0.5)) +\n  guides(fill = guide_legend(title = \"Party\")) +\n  scale_fill_manual(values = c(\"#92B4F4\", \"#0084c6\", \"#880105\",  \"forestgreen\",\n                               \"grey88\", \"#c70000\", \"darkorange\", \"#BDF7B7\",\n                              \"#749C75\", \"#ffe500\", \"#CEBACF\")) +\n  labs(caption = \"UK Elections 2017\")\n\nsecond &lt;- ggplot(results) +\n  geom_sf(aes(fill = winningParty), size = 0.2) +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        panel.grid = element_blank(),\n        plot.caption = element_text(hjust = 0.5)) +\n  guides(fill = guide_legend(title = \"Party\")) +\n  scale_fill_manual(values = c(\"#92B4F4\", \"#0084c6\", \"#880105\",  \"forestgreen\",\n                               \"#c70000\", \"darkorange\", \"#BDF7B7\",\n                               \"#CEBACF\",  \"#749C75\", \"#ffe500\", \"grey88\")) +\n  labs(caption = \"UK Elections 2019\")\n\nfirst + second + plot_annotation(title =  \"Comparison of Results, UK elections 2017 & 2019\")\n\n\nWe can really see the change in the so-called ‚ÄòRed Wall‚Äô that has given the Conservatives such a huge victory. Let‚Äôs animate this change, because we can, y‚Äôallz. All we need to do is save the bare images and use the magick library to animate the images as a gif. The code is the following, but obviously you‚Äôll need to use your own paths and images.\n\nimg1 &lt;- image_read(\"images/img2019.png\")\nimg2 &lt;- image_read(\"images/img2017.png\")\nframes &lt;- image_morph(c(img1, img2, img1), frames = 25)\nimage_animate(frames, fps = 10)\n\n\nUntil next UK election‚Ä¶"
  },
  {
    "objectID": "posts/geo-ref.html",
    "href": "posts/geo-ref.html",
    "title": "Geo-reference an image in R",
    "section": "",
    "text": "R is actually great for working with spatial data (for example, see here and here for fantastic graphs and maps made with R), however, you often need data that is actually spatial to get started! What do you do if you have an image, a map, let‚Äôs say, that is not geo-referenced in any way?\nThe regular answer to this problem is to use software such as QGIS to manually enter GPS coordinates, with the help of Google Maps or something similar. But R can be used for this too, and it‚Äôs quite easy to do.\nFirst, we load some necessary packages. Here, I‚Äôm working with .tiff files that I will change into Geo-tiffs. For other formats, you will need some other packages (such as png, for example).\nlibrary(raster)\nlibrary(rgdal)\nNext, we read in the non-spatial image using the raster command. By plotting this in RStudio, the image can be cropped in the preview window, if you want to crop it down the area of interest. This is also useful if the image came with extra, non-map parts (logos etc.).\nMap &lt;- raster(\"1.tiff\")\nMap\nplot(Map)\nmap2 &lt;- crop(Map, drawExtent(show = TRUE, col = \"red\"))\nplot(map2)\nNow we can enter in the maximal points of the image, xmin/xmax and ymin/ymax, respectively. These coordinates refer to an area of S√£o Paulo, Brazil. The x-axis is longitude, the y-axis latitude. You can get the coordinates from www.gps-coordinates.net for the area you need. We also need to tell R what type of map projection we are going to write into the image. Here we‚Äôll use \"+proj=longlat +datum=WGS84\".\nxmin(map2) &lt;- -46.67449772357941\nxmax(map2) &lt;- -46.524503231048584\nymin(map2) &lt;- -23.638627166908787\nymax(map2) &lt;- -23.517227011061372\ncrs(map2) &lt;- \"+proj=longlat +datum=WGS84\"\nThis part is the main piece of work. But after it‚Äôs done, you‚Äôve got yourself a geo-referenced image. Here, we‚Äôll write it to the geo-tiff format I mentioned earlier.\nwriteRaster(map1, \"Gmap1.tiff\", \"GTiff\")\nSimple! üòà"
  },
  {
    "objectID": "posts/uk-elections-2024.html",
    "href": "posts/uk-elections-2024.html",
    "title": "Uk Elections 2024",
    "section": "",
    "text": "Well I did say I‚Äôd do another one of these come the next UK election, so here goes. The official data won‚Äôt be published until Friday the 12th of July but we can use our creativity to get it before then. Ok, I did so you don‚Äôt have to and you can get the results here.\nI‚Äôd ideally like to make use of parlitools‚Äô west_hex_map, as I did previously, but unfortunately that package is out of date with respect to the electoral areas of the UK. After a bit of searching, I did find a new hex map for this purpose here ‚Äì thank you Alasdair Rae & co. Alasdair also explains why you might want to use these hex maps instead of a regular geographically-accurate map in this blogpost (equal population areas).\nI‚Äôll combine this hex with the election results linked to above through the GSS Code. You can get this data from here.\nFirst, we‚Äôll load the libraries I‚Äôm going to use and get the data we need.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(jsonlite)\n\nresults_data &lt;- \"https://raw.githubusercontent.com/RobertMyles/blogdata/master/uk_elections_2024.json\"\nhexes &lt;- st_read(\"https://automaticknowledge.org/wpc-hex/uk-wpc-hex-constitcode-v5-june-2024.geojson\", quiet = TRUE)\n\nThe results are in a list, so I will use pluck() from the purrr package to take out the actual results (constit_summary ) and the full party name (sop &gt; parties). I‚Äôm going to rename some columns here to make joining easier later on.\n\nparties &lt;- fromJSON(results_data) |&gt; \n  pluck(\"sop\", \"parties\") |&gt; \n  select(party = name, party_abb = abbreviation)\n\nresults &lt;- fromJSON(results_data) %&gt;%\n  pluck(\"constit_summary\") |&gt; \n  rename(party_abb = winner_abbreviation) |&gt; \n  left_join(parties) |&gt; \n  mutate(\n    party = case_when(\n      party_abb == \"TUV\" ~ \"The Unionist Voice\",\n      party_abb == \"Ind\" ~ \"Independent\",\n      party_abb == \"Speaker\" ~ \"Speaker\",\n      TRUE ~ party\n    )\n  )\n\nhexes &lt;- hexes |&gt; rename(pcon24 = GSScode)\n\nOk, so now we can have a quick look at the results (Labour smashed the Conservative party, in case you didn‚Äôt know).\n\nresults |&gt; \n  group_by(party) |&gt; \n  count(sort = TRUE)\n\n# A tibble: 15 √ó 2\n# Groups:   party [15]\n   party                                  n\n   &lt;chr&gt;                              &lt;int&gt;\n 1 Labour                               411\n 2 Conservative                         121\n 3 Liberal Democrat                      71\n 4 Scottish National Party                9\n 5 Sinn Fein                              7\n 6 Independent                            6\n 7 Democratic Unionist Party              5\n 8 Green                                  4\n 9 Plaid Cymru                            4\n10 Reform UK                              4\n11 Social Democratic and Labour Party     2\n12 Alliance                               1\n13 Speaker                                1\n14 The Unionist Voice                     1\n15 Ulster Unionist Party                  1\n\n\nSo we‚Äôve got a few small parties here, and the Speaker. Let‚Äôs group them into an ‚ÄúOther‚Äù category.\n\nresults &lt;- results |&gt; \n  mutate(\n    party = case_when(\n      party_abb %in% c(\"Alliance\", \"Speaker\", \"UUP\", \"SDLP\", \"TUV\") ~ \"Other\",\n      TRUE ~ party\n      )\n    )\n\nOur election data looks pretty good now so we can join it with the hex map data. There are two constituencies that haven‚Äôt declared at the time of writing so we‚Äôll remove those.\n\nmap &lt;- hexes |&gt; left_join(results, by = \"pcon24\")\n\nmap &lt;- map |&gt; \n  filter(!is.na(party)) %&gt;%\n  st_as_sf()\n\nLet‚Äôs plot ‚Äôer up! (Colour scheme from here)\n\nggplot(map) +\n  geom_sf(aes(fill = party), size = 0.2) +\n  theme_void() +\n  guides(fill = guide_legend(title = \"Party\", position = \"left\")) +\n  scale_fill_manual(values = c(\n     \"#5691AF\", \"#975654\", \"#72A768\", \"#98B3D1\", \n     \"#DA5C5B\", \"#E0AD3B\", \"#888888\", \"#9DCB84\",\n     \"#54B3CC\", \"#F1D848\", \"#5A9464\"\n     )\n    )\n\n\n\n\n\n\n\n\nVery different to last time!"
  },
  {
    "objectID": "posts/congressbr.html",
    "href": "posts/congressbr.html",
    "title": "Brazilian Legislative Data with congressbr",
    "section": "",
    "text": "Recently, a paper by myself and two friends was published in the Latin American Research Review (you can read it here). As we write in the paper:\n\nThe purpose of making such a package is to put useful and interesting political science data in the hands of researchers. Our goal is to provide a suite of easy-to-use functions that even the novice R user can understand and use to produce analyses of Brazilian politics. This opens up the analysis of such data to more scholars than was previously possible, as studies such as those cited in the text have often been restricted to those with significant programming experience, or to those with the time and resources to collect data by hand.\n\nPlenty of social science researchers are not a) interested in programming and/or b) not very good programmers (probably because of a). We decided to make congressbr to help those researchers and to publicise the availability of this data (legislative analyses on Brazil can be theory-heavy and data-light‚Ä¶not saying this is a bad thing, just that we wanted to help introduce more data into things). We chose R because non-programmers can get set up and get working very quickly, particularly with RStudio. In this brief post, I‚Äôll show how can you make a neat map using congressbr and a few more R packages, cos hey, everybody loves political maps. Well, I do. For more information on congressbr, please see the paper or the package repository on GitHub.\nPerhaps the quickest way to get up and running is to load one of the datasets available in the package:\nlibrary(congressbr)\ndata(\"sen_nominal_votes\")\nhead(sen_nominal_votes)\n\n## # A tibble: 6 x 9\n##   vote_date           bill_id bill  legislature senator_id senator_name senator_vote senator_party\n##   &lt;dttm&gt;              &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;\n## 1 1991-06-06 00:00:00 19615   PLC:‚Ä¶ 49          31         Guilherme P‚Ä¶            1 PFL\n## 2 1991-06-06 00:00:00 19615   PLC:‚Ä¶ 49          47         Jose Sarney             1 PMDB\n## 3 1991-06-06 00:00:00 19615   PLC:‚Ä¶ 49          82         Amazonino M‚Ä¶            1 PDC\n## 4 1991-06-06 00:00:00 19615   PLC:‚Ä¶ 49          33         Humberto Lu‚Ä¶            1 PMDB\n## 5 1991-06-06 00:00:00 19615   PLC:‚Ä¶ 49          79         Valmir Camp‚Ä¶            1 PTB\n## 6 1991-06-06 00:00:00 19615   PLC:‚Ä¶ 49          84         Antonio Mar‚Ä¶            1 PMDB\n## # ‚Ä¶ with 1 more variable: senator_state &lt;chr&gt;\nSo what‚Äôs this? Well, this dataframe is a record of nominal votes in the Senado Federal from 1991 to 2017. It contains the datetime of the vote, the ID of the bill, the ‚Äòname‚Äô of the bill, senator IDs, names, party and state affiliation and recorded vote (1 = ‚ÄòYes‚Äô, 0 = ‚ÄòNo‚Äô, NA = didn‚Äôt vote). Let‚Äôs pick a random entry (I‚Äôm using knitr::kable() so that we can see all the columns of the result):\nlibrary(dplyr)\nset.seed(12345)\nsen_nominal_votes %&gt;%\n  sample_n(1) %&gt;%\n  knitr::kable()\n\n\n\n\nvote_date\n\n\nbill_id\n\n\nbill\n\n\nlegislature\n\n\nsenator_id\n\n\nsenator_name\n\n\nsenator_vote\n\n\nsenator_party\n\n\nsenator_state\n\n\n\n\n\n\n2015-11-24\n\n\n122739\n\n\nMPV:2015:00688\n\n\n55\n\n\n3695\n\n\nLindbergh Farias\n\n\n1\n\n\nPT\n\n\nRJ\n\n\n\n\n‚Ä¶aaaand we can see that Senator Lindbergh Farias voted ‚ÄòYes‚Äô to MPV 00688 in November 2015. So who‚Äôs this senator? Let‚Äôs see. We can use the ID to look the senator up:\nsen_senator(3695) %&gt;%\n  select(2, 4:6, office_email) %&gt;%\n  knitr::kable()\n\n\n\n\nname_full\n\n\ngender\n\n\ndate_of_birth\n\n\nplace_of_birth\n\n\noffice_email\n\n\n\n\n\n\nLuiz Lindbergh Farias Filho\n\n\nM\n\n\n1969-12-08\n\n\nJoao Pessoa\n\n\n lindbergh.farias@senador.leg.br \n\n\n\n\nYou could even email him if you wanted.\nSo this random entry is on an ‚ÄòMPV‚Äô type of vote. This stands for Medida Provis√≥ria, and is basically a President-introduced bill that goes to the top of the legislative agenda until it is voted on. Presidents can keep re-introducing it (which they do) and thereby dominate the legislative agenda. Voting for MPVs is usually taken as a sign of government support, although many MPVs can be uncontroversial so it‚Äôs not so clear-cut. Anyway, let‚Äôs make a map! We‚Äôll recode the NA values so that they are \"2\", since these could be deliberate absences and we could make use of this information.\nlibrary(stringr)\nstates &lt;- sen_nominal_votes %&gt;%\n  filter(str_detect(bill, \"MPV\")) %&gt;%\n  select(senator_vote, senator_state) %&gt;%\n  mutate(\n    senator_vote = ifelse(\n      is.na(senator_vote), 2, senator_vote) %&gt;%\n      as.character()\n    ) %&gt;%\n  count(senator_state, senator_vote) %&gt;%\n  group_by(senator_state) %&gt;%\n  mutate(\n    summ = sum(n),\n    perc = 100*(n/summ)\n    ) %&gt;%\n  ungroup()\nstates\n## # A tibble: 82 x 5\n##    senator_state senator_vote     n  summ  perc\n##    &lt;chr&gt;         &lt;chr&gt;        &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n##  1 AC            0               30    81  37.0\n##  2 AC            1               29    81  35.8\n##  3 AC            2               22    81  27.2\n##  4 AL            0               16    81  19.8\n##  5 AL            1               27    81  33.3\n##  6 AL            2               38    81  46.9\n##  7 AM            0               27    81  33.3\n##  8 AM            1               38    81  46.9\n##  9 AM            2               16    81  19.8\n## 10 AP            0               38    81  46.9\n## # ‚Ä¶ with 72 more rows\nSo with those totals we can integrate this data with some other things we‚Äôll use. First, let‚Äôs get the spatial data:\nlibrary(rnaturalearthhires)\nlibrary(sf)\nbr &lt;- states10 %&gt;%\n  st_as_sf() %&gt;%\n  filter(admin == \"Brazil\") %&gt;%\n  select(senator_state = postal, geometry)\nJoin these together:\nstates &lt;- states %&gt;%\n  full_join(br) %&gt;%\n  st_as_sf()\nNow let‚Äôs make our maps. We‚Äôll use {patchwork} to put them together.\nlibrary(ggplot2)\nagainst &lt;- states %&gt;%\n  filter(senator_vote == \"0\") %&gt;%\n  ggplot(aes(fill = perc)) +\n  geom_sf() +\n  scale_fill_distiller(palette = \"Reds\", direction = 1) +\n  theme_minimal() +\n  guides(fill = guide_legend(\n    reverse = TRUE, title = \"% voting against\"))\nforr &lt;- states %&gt;%\n  filter(senator_vote == \"1\") %&gt;%\n  ggplot(aes(fill = perc)) +\n  geom_sf() +\n  scale_fill_distiller(direction = 1) +\n  theme_minimal() +\n  guides(fill = guide_legend(\n    reverse = TRUE, title = \"% voting for\"))\nabsent &lt;- states %&gt;%\n  filter(senator_vote == \"2\") %&gt;%\n  ggplot(aes(fill = perc)) +\n  geom_sf() +\n  scale_fill_distiller(palette = \"Greens\", direction = 1) +\n  theme_minimal() +\n  guides(fill = guide_legend(\n    reverse = TRUE, title = \"% absent\"))\nlibrary(patchwork)\nagainst + forr + plot_layout(ncol = 2)\n\nabsent\n\nThere you go! Some pretty basic plotting and data munging aside, the code necessary to create these is quite minimal, and the data easily available from congressbr."
  },
  {
    "objectID": "posts/meps.html",
    "href": "posts/meps.html",
    "title": "Theme-Specific Voting in the European Parliament",
    "section": "",
    "text": "Since it‚Äôs European Statistics Day, I thought I would make a quick post showing how to utilise some of the data that we have on the European Union in R. In particular, I will use European Parliament voting data from Simon Hix‚Äôs website. The data is freely available, so by copying and pasting the code below, you will be able to recreate the analysis I‚Äôve done here.\nWe‚Äôre going to be using Stan to make theme-specific ideal points for members of the European Parliament. You will need to install Stan and a C++ compiler to replicate the analysis.\nFirst, let‚Äôs load the R packages that we‚Äôre going to use. If you don‚Äôt have any of these, you will need to install them first, using either install.packages(\"name of package\") or by means of the ‚Äòinstall‚Äô button on the Packages window of the RStudio IDE.\nlibrary(data.table)\nlibrary(tidyverse)\nlibrary(dtplyr)\nlibrary(rstan)\nlibrary(stringi)\nAfter you download the data from Hix‚Äôs website, we can import it into R. I will then merge everything together.\nrm(list = ls())\n\neu4 &lt;- as_tibble(fread(\"~/Downloads/ep6/RCVS2004Full.csv\", header=T))\neu5 &lt;- as_tibble(fread(\"~/Downloads/ep6/RCVS2005Full.csv\", header = T))\neu6 &lt;- as_tibble(fread(\"~/Downloads/ep6/RCVS2006Full.csv\", header = T))\neu7 &lt;- as_tibble(fread(\"~/Downloads/ep6/RCVS2007Full.csv\", header = T))\neu8 &lt;- as_tibble(fread(\"~/Downloads/ep6/RCVS2008Full.csv\", header = T))\neu9 &lt;- as_tibble(fread(\"~/Downloads/ep6/RCVS2009Full.csv\", header = T))\n\neu &lt;- eu4 %&gt;%\n  full_join(eu5) %&gt;%\n  full_join(eu6) %&gt;%\n  full_join(eu7) %&gt;%\n  full_join(eu8) %&gt;%\n  full_join(eu9) %&gt;%\n  select(-V1) %&gt;%\n  rename(voter = `Vote ID`) %&gt;%\n  mutate(voter = stri_trans_general(voter, \"Latin-ASCII\"))\n\nrm(eu4, eu5, eu6, eu7, eu8, eu9)\nNow we have a data frame of each of the 940 legislators in the database, and their votes on 6200 votes. Next we‚Äôll create some id variables that we will use when we send the data to Stan.\nEU &lt;- gather(eu, vote_id, vote, `1`:`6200`) %&gt;%\n  mutate(vote_id = as.numeric(vote_id),\n         voter_id = as.numeric(as.factor(voter)))\nNow we have each voter (the M.E.P., voter), the id of the bill being voted on (vote_id), how the individual voted (vote) and the id of each voter. In these data, 1 is a ‚Äòyes‚Äô vote, while 0 is ‚Äòno‚Äô. The full list from Hix‚Äôs website contains the following info:\nThe codes for the MEP vote decisions are as follows:\nEP1, EP2 and EP5: 1=Yes, 2=No, 3=Abstain, 4=Present but did not vote, 0=Absent, 5=Not an MEP\nEP3 and EP4: 1 = Yes, 2 = No, 3 = Abstain, 4 = Present but did not vote, 0 = either Absent or Not an MEP\nHix & co. also provide us with information on the specific policy area for each vote. We can import it, tidy it up a little and merge it to the data we have.\ntheme &lt;- as_tibble(fread(\"~/Downloads/ep6/vc.csv\"))\n\ntheme &lt;- theme %&gt;%\n  rename(vote_id = `Vote Id`) %&gt;%\n  select(vote_id, Title, `Policy Area`, Result) %&gt;%\n  rename(topic = `Policy Area`) %&gt;%\n  mutate(topic_id = as.numeric(as.factor(topic)))\n\nrollcalls &lt;- full_join(EU, theme) %&gt;%\n  mutate(vote = ifelse(vote==1, 1, ifelse(vote==0, 0, NA))) %&gt;%\n  filter(!is.na(vote))\nNext, we need to prepare the data for Stan. Our model is a basic 2-parameter Item-Response theory model often used for creating ideal points. We write this in the Stan modelling language and save it as a string in R. In mathematical notation, the model is:\n\\[y_{ijk} = \\beta_j \\theta_{ik} - \\alpha_j,\\]\nwhere i is an index of voters, j an index of votes, and k an index of topics. \\(\\theta_{ik}\\) is our main object of interest: the ideal point of MEP i on topic k.\nFor those not familiar with Stan, the following Stan code has a data block, in which we declare what our variables are and their type (these are created in the section after). Then we have a parameters block where we declare our parameters.\nLastly, we have the model block where we have our model and the priors for each parameter. In an IRT model like this, we need to constrain the ideal points of at least 2 legislators. Since I am not an expert on these MEPs, I am just going to do this for the first two in the database (theta[1] and theta[2]).\nmep_model &lt;- \"\ndata {\n  int&lt;lower=1&gt; J;               //MEPs\n  int&lt;lower=1&gt; M;               //Proposals\n  int&lt;lower=1&gt; K;               //no. of topics\n  int&lt;lower=1&gt; N;               //no. of observations\n  vector[K] m0;                 // prior mean for theta\n  cov_matrix[K] M0;             // prior covar. for theta\n  int&lt;lower=1, upper=J&gt; j[N];   //MEP for observation n\n  int&lt;lower=1, upper=M&gt; m[N];   //proposal for observation n\n  int&lt;lower=1, upper=K&gt; k[N];   //topic for observation n\n  int&lt;lower=0, upper=1&gt; Y[N];   //vote of observation n\n}\nparameters {\n  real alpha[M];\n  real beta[M];\n  vector[K] theta[J];\n}\nmodel {\n  beta ~ normal(0, 10);\n  alpha ~ normal(0, 10);\n  for (n in 1:N)\n  Y[n] ~ bernoulli_logit(theta[j[n], k[n]]*beta[m[n]] - alpha[m[n]]);\n  theta ~ multi_normal(m0, M0);\n  theta[1,1] ~ normal(1, .01);\n  theta[2,1] ~ normal(-1, .01);\n}\"\nAbove, we have variables declared in our Stan model. Here, I define these objects in R. All of this then goes as a list to Stan.\nlibrary(rstan)\n\nN &lt;- nrow(rollcalls)\nM &lt;- max(rollcalls$vote_id)\nK &lt;- max(rollcalls$topic_id)\nJ &lt;- max(rollcalls$voter_id)\nY &lt;- rollcalls$vote\nm &lt;- rollcalls$vote_id\nk &lt;- rollcalls$topic_id\nj &lt;- rollcalls$voter_id\n\n# Mean and Covariances for theta\nm0 &lt;- rep(0, times=K)\nM0 &lt;- matrix(0, K, K)\ndiag(M0) &lt;- 1\n\nstan_data &lt;- list(J=J, N=N, M=M, j=j,\n                  Y=Y, m=m, K=K, k=k,\n                  m0=m0, M0=M0)\nNext, we run our model with Stan. Here I use Stan‚Äôs new ADVI feature, but the Stan folks don‚Äôt recommend this for inference. However, for a blog post it‚Äôs okay üòä.\nADVI is much faster than the already comparatively fast NUTS sampling that Stan does. Here we have a lot of data, though, so this next part will take a few hours to run. If you don‚Äôt fancy waiting so long, subset the data (maybe choose just one year) and run the Stan code on the smaller dataset.\nStan_Model &lt;- stan_model(model_name = \"meps\", model_code = mep_model)\n\nRes1 &lt;- vb(Stan_Model, data = stan_data, seed = 1234,\n          init = \"random\")\nSo one thing that we could do with the estimates from this model is plot the ideal points of an MEP as they vary over the themes that he/she voted on.\nWhat we will do is extract the elements of the summary that we want and then create the summary that we need to start making figures.\nsummary &lt;- list(summary(Res1, pars=\"theta\"))\nsummary &lt;- summary[[1]][1]\nsummary &lt;- as_data_frame(summary[[1]]) %&gt;%\n  mutate(names = theta_names,\n         voters = rep(unique(rollcalls$voter), each=21),\n         index = as.character(str_extract_all(names, \"\\\\.[0-9]*$\")),\n         index = gsub(\"\\\\.\", \"\", index),\n         index = as.integer(index))\n\ntopics &lt;- unique(rollcalls$topic)\nindex &lt;- unique(summary$index)\ntopic_index &lt;- tibble(topic = topics, index = index)\n\nmep_summary &lt;- full_join(summary, topic_index) %&gt;%\n  select(-c(names, index))\nThe following graphs are of Adamos Adamou and Filip Adwent, for the simple reason that they are the first names in the database. First, we create a plot for Adamou and then for Adwent, then we combine them. In the following code, I customize the font, but none of that is necessary.\nadamou &lt;- mep_summary %&gt;% filter(voters==\"ADAMOU, Adamos\")\n\n\nggplot(adamou, aes(x = mean, y = topic)) +\n  geom_segment(aes(yend = topic), color = \"#104E8B\",\n               xend = 0, alpha = 0.3) +\n  geom_point(size = 4, color = \"#104E8B\") + theme_bw() +\n  theme(legend.position = \"none\", axis.title.y = element_blank(),\n        axis.title.x = element_text(family = \"Georgia\", face='bold'),\n        axis.text.y = element_text(family = \"Georgia\", size = 12),\n        axis.text.x = element_text(family = \"Georgia\", size = 12)) +\n  xlab(\"Ideal Points, Adamos Adamou\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\")\n\nadwent &lt;- mep_summary %&gt;% filter(voters==\"ADWENT, Filip\")\n\n\nggplot(adwent, aes(x = mean, y = topic)) +\n  geom_segment(aes(yend = topic), color = \"#8B1A1A\",\n               xend = 0, alpha = 0.3) +\n  geom_point(size = 4, color = \"#8B1A1A\") + theme_bw() +\n  theme(legend.position = \"none\", axis.title.y = element_blank(),\n        axis.title.x = element_text(family = \"Georgia\", face='bold'),\n        axis.text.y = element_text(family = \"Georgia\", size = 12),\n        axis.text.x = element_text(family = \"Georgia\", size = 12)) +\n  xlab(\"Ideal Points, Filip Adwent\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\")\n\nWe can put these two together and see how they compare:\nggplot(adamou, aes(x = mean, y = topic)) +\n  geom_segment(aes(yend = topic), color = \"#104E8B\",\n               xend = 0, alpha = 0.3) +\n  geom_segment(data = adwent, aes(yend = topic),\n               xend = 0, colour = \"#8B1A1A\",\n               alpha = 0.3) +\n  geom_point(size = 4, color = \"#104E8B\") + theme_bw() +\n  theme(legend.position = \"none\", axis.title.y = element_blank(),\n        axis.title.x = element_text(family = \"Georgia\", face='bold'),\n        axis.text.y = element_text(family = \"Georgia\", size = 12),\n        axis.text.x = element_text(family = \"Georgia\", size = 12)) +\n  xlab(\"Ideal Points, Adamos Adamou & Filip Adwent\") +\n  geom_point(data = adwent, aes(x = mean, y =topic),\n             size = 4, color = \"#8B1A1A\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\")\n\nAnd of course you can customise these ggplot figures any way you like.\nHappy European Statistics Day! üëØ"
  },
  {
    "objectID": "posts/gauge.html",
    "href": "posts/gauge.html",
    "title": "Gauge-style plots with ggplot2",
    "section": "",
    "text": "I‚Äôve been working on a project where the client wanted a ‚Äúcockpit‚Äù style dashboard, with meter/gauge/speedometer type things. Even though this wasn‚Äôt likely to be implemented in R for the final version, I started thinking about how I could do this with ggplot2, influenced by some code from here. Base R graphics are likely a much better fit for something like this, as Gaston Sanchez‚Äôs example shows, but I actually quite like the result with ggplot2. It‚Äôs not so speedometery, so that‚Äôs a good thing in my book.\nAnywayz, here is what it looks like, with how I did it to follow:\n\nSo, it‚Äôs not very generalizable by nature, since labels for each bar start to get difficult with more than three or four categories, so I recommend doing this with just that low number of bars. You could go wild, of course, but then the labels are better off of the plot, and the bars start to lose their smooth lines. I‚Äôve used viridis for this, but it gives the false impression of a changing scale moving through the radius, so it‚Äôs better to sample these colours from a palette or do it manually.\n\nSo here‚Äôs the code to do the first plot (the colours are sampled, so running this you‚Äôll get different ones). The trick involves using a transparent bars that go to 100, and a second set that go to 50.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(scales)\n\n# example data:\ndf &lt;- data_frame(group = c(\"Cat. 1\", \"Cat. 2\", \"Cat. 3\", \"Cat. 4\"),\n                 value = sample(20:70, 4))\n\n# scale everything and set labels:\ndf &lt;- df %&gt;%\n  filter(!is.na(group)) %&gt;%\n  mutate(group2 = gsub(\" \", \"\\n\", group),\n         group = ifelse(toupper(group) == \"A\", \"Temp@\", group)) %&gt;%\n  add_row(group = \"A\", value = 0, .before = 1) %&gt;%\n  mutate(value_scale = rescale(value, to = c(0, 100)),\n         value_sc_half = rescale(value_scale, to = c(0, 50)),\n         aim = rep(50, nrow(.)))\n\ndf_sub &lt;- filter(df, group != \"A\")\n\n# colours:\ncol_vec &lt;- c(\"#440154FF\", \"#404788FF\", \"#2D708EFF\", \"#29AF7FFF\",\n             \"#55C667FF\",\"#2E6171\", \"#685369\", \"#191D32\")\n\n# graph\nggplot(df, aes(x = group, fill = group)) +\n  geom_bar(width = 0.85, stat=\"identity\", aes(y = value_scale),\n           colour = \"white\", fill = \"white\") +\n  geom_bar(data = df_sub, aes(y = aim), width = 0.85,\n           stat = \"identity\", fill = \"white\", colour = \"grey68\") +\n  geom_bar(data = df_sub, width = 0.85, stat = \"identity\",\n           aes(y = value_sc_half, fill = group2), colour = \"black\") +\n  coord_polar(theta = \"y\", start = -1.57) +\n  theme_classic() +\n  xlab(\"\") + ylab(\"\") +\n  theme(legend.position = \"none\") +\n  geom_text(data = df_sub, vjust = 1.4, size = 3,\n           aes(x = group, y = 0, label = group2)) +\n  geom_text(data = df_sub, aes(x = group, y = 50), label = \"Aim\",\n            colour = \"grey68\", vjust = -1) +\n  scale_fill_manual(values = sample(col_vec)) +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        axis.line = element_blank(),\n        plot.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = \"pt\"))"
  },
  {
    "objectID": "posts/prisonbrief.html",
    "href": "posts/prisonbrief.html",
    "title": "Analyzing Prison Data in R",
    "section": "",
    "text": "My good friend Danilo Freire and I have just finished a little R data package, called prisonbrief. We hope that it will be useful for R users, particularly researchers in the area, since this is still a much understudied topic. Why does prison population change? In many countries, it is rising, and has been for some time, and the determinants of this rise are still not well understood. All the data in the package come from the fantastic World Prison Brief website ‚Äì we thank them for making these data available. You can contribute to the project here.\nWith that in mind, I thought I‚Äôd give a quick introduction to the package here on my blog. The package is on CRAN, so it‚Äôs easily installed:\ninstall.packages(\"prisonbrief\")\n\nlibrary(prisonbrief)\nlibrary(dplyr)\nlibrary(ggplot2)\nWhile rising prisoner numbers are not news in the United States, we can take a look at other parts of the world using the wpb_table() function and specifying the region or country of interest (using a single country as an argument returns a les useful table ‚Äì we recommend requesting a region). We can have a look at Western Europe. Could the Social-Democratic EU be showing similar patterns? In the following code, I download the data for Europe and filter it to have the selection of countries I want.\nwesteros &lt;- wpb_table(region = \"Europe\") %&gt;%\n  filter(country %in%  c(\"Italy\", \"Portugal\", \"Spain\",\n                        \"Iceland\", \"Belgium\", \"France\",\n                        \"United Kingdom: Scotland\",\n                        \"United Kingdom: Northern Ireland\",\n                        \"United Kingdom: England & Wales\",\n                        \"Ireland, Republic of\",\n                        \"Germany\", \"Sweden\", \"Denmark\",\n                        \"Norway\", \"Netherlands\"))\nLet‚Äôs combine these data with the table of the USA. Then we can graph the prison population rate.\nusa &lt;- wpb_table(region = \"North America\") %&gt;%\n  filter(country == \"United States of America\")\n\nwe_usa &lt;- rbind(westeros, usa) %&gt;%\n  mutate(country = gsub(\"United Kingdom: \", \"\", country),\n         country = gsub(\", Republic of\", \"\", country))\nSharp-sighted useRs may notice we‚Äôre using rbind() here instead of something like dplyr::full_join() or even dplyr::bind_rows(). This is because of the sf geometry column ‚Äì sf is awesome, but it‚Äôs not quite ready yet, and the sf class gets stripped with these joins & binds from dlyr.\nggplot(we_usa, aes(x = reorder(country, prison_population_rate),\n                   y = prison_population_rate,\n                   fill = prison_population_rate)) +\n  geom_bar(stat = \"identity\", colour = \"#0B1D51\") +\n  theme_minimal() + labs(y = NULL, x = NULL) +\n  coord_flip() + theme(legend.position = \"none\",\n                       axis.text.y = element_text(colour = \"#0B1D51\"),\n                       panel.grid.major = element_line(colour = \"#0B1D51\"),\n                       panel.grid.minor = element_line(colour = \"#797596\")) +\n  geom_text(aes(label = prison_population_rate),\n            position = position_dodge(.9), colour = \"#0B1D51\",\n                    hjust = 1.2, size = 3.3) +\n  scale_fill_continuous(low = \"#CC978E\", high = \"#D44D5C\")\n\nWell, so far, quite as expected, apart from the almost-eviltastic number for the topspot ü§ò. Let‚Äôs take a look at female prisoners as a percentage of prisoner population:\nggplot(we_usa, aes(x = reorder(country, `female-prisoners`),\n                   y = `female-prisoners`,\n                   fill = `female-prisoners`)) +\n  geom_bar(stat = \"identity\", colour = \"#292F36\") +\n  theme_minimal() + labs(y = NULL, x = NULL) +\n  coord_flip() + theme(legend.position = \"none\",\n                       axis.text.y = element_text(colour = \"#292F36\"),\n                       panel.grid.major = element_line(colour = \"#FF6B6B\")) +\n  geom_text(aes(label = `female-prisoners`),\n            position = position_dodge(.9), colour = \"#292F36\",\n                    hjust = 1.2, size = 3.3) +\n  scale_fill_continuous(low = \"#FFFFFF\", high = \"#4ECDC4\")\n\nMuch less difference between the countries here, definitely, but we still see the USA far ahead as we might expect. We can also have alook at some other interesting data, that of pre-trial detainees and foreign prisoners:\nggplot(we_usa, aes(x = reorder(country, `pre-trial-detainees`),\n                   y = `pre-trial-detainees`,\n                   fill = `pre-trial-detainees`)) +\n  geom_bar(stat = \"identity\", colour = \"#0B5563\") +\n  theme_minimal() + labs(y = NULL, x = NULL) +\n  coord_flip() + theme(legend.position = \"none\",\n                       axis.text.y = element_text(colour = \"#5E5C6C\"),\n                       panel.grid.major = element_line(colour = \"#BEB8EB\")) +\n  geom_text(aes(label = `pre-trial-detainees`),\n            position = position_dodge(.9), colour = \"#084C61\",\n                    hjust = 1.2, size = 3.3) +\n  scale_fill_continuous(low = \"#A2BCE0\", high = \"#5299D3\")\n\nggplot(we_usa, aes(x = reorder(country, `foreign-prisoners`),\n                   y = `foreign-prisoners`,\n                   fill = `foreign-prisoners`)) +\n  geom_bar(stat = \"identity\", colour = \"#3C1518\") +\n  theme_minimal() + labs(y = NULL, x = NULL) +\n  coord_flip() + theme(legend.position = \"none\",\n                       axis.text.y = element_text(colour = \"#3C1518\"),\n                       panel.grid.major = element_line(colour = \"#69140E\")) +\n  geom_text(aes(label = `foreign-prisoners`),\n            position = position_dodge(.9),\n                    hjust = 1.1, size = 3.3) +\n  scale_fill_continuous(low = \"#F2F3AE\", high = \"#A44200\")\n\nHere we do see a radically different pattern, one perhaps legal experts of the countries involved may recognise (I‚Äôm not one, so I don‚Äôt). With regards to these two categories, Belgium, Italy, Denmark, Sweden and Norway all find themselves in the top seven in both.\nAnother important category of data returned by wpb_table() refers to the official capacity level of the penal institutions in the country.\nggplot(we_usa, aes(x = reorder(country, `occupancy-level`),\n                   y = `occupancy-level`,\n                   fill = `occupancy-level`)) +\n  geom_bar(stat = \"identity\", colour = \"#261447\") +\n  theme_minimal() + labs(y = NULL, x = NULL) +\n  coord_flip() + theme(legend.position = \"none\",\n                       axis.text.y = element_text(colour = \"#261447\"),\n                       panel.grid.major.x = element_line(colour = \"#C0BDA5\"),\n                       panel.grid.major.y = element_line(colour = \"#C0BDA5\")) +\n  geom_text(aes(label = `occupancy-level`),\n            position = position_dodge(.9),\n                    hjust = 1.1, size = 3.3, colour = \"#261447\") +\n  scale_fill_continuous(low = \"#FFCAD7\", high = \"#FF3864\")\n\nMost countries are very close to capacity, with only Holland ducking under 70%. France, although it spends an enormous percentage of its GDP on public services, is no stranger to criticism of its prison conditions.\nWell, we hope that prisonbrief will be useful for those studying prisons, or even for those needing quick and interesting datasets for teaching/learning R. For those interested in the topic at hand, Danilo has some interesting research that you can find here. His PhD supervisor, David Skarbek, is a well-known expert in the field and well worth checking out if prisons are your particular fancy!"
  },
  {
    "objectID": "posts/write-your-thesis-or-paper-in-rmarkdown.html",
    "href": "posts/write-your-thesis-or-paper-in-rmarkdown.html",
    "title": "Write your thesis or paper in R Markdown!",
    "section": "",
    "text": "There are many reasons why you would want to use some variant of Markdown for writing, and indeed, posts are common on the net as to why you should.1 A simple summary of the reasons are that Markdown is: 1) easy; 2) easy; 3) yup, you guessed it ‚Äì it‚Äôs easy.\nOne variant of Markdown is R Markdown, developed by the RStudio team, and in particular Yihui Xie, creator of the knitr R package. R Markdown is pretty much like regular Markdown, except you get a whole load of nice extra features, including the ability to run code chunks, produce .pdfs and presentations, and even .docx (if you really, really want to2). Indeed, the ioslides presentation format lets you use the power of html and css to make browser-based presentations.\nBut surely academic papers require certain formats, and sometimes mathematical expressions and funny Greek letters? Well, sure. Academic papers, particularly theses, often have set formats that you must adhere to. And since Markdown is quite a simple language, it doesn‚Äôt have the advanced power of \\(\\LaTeX\\) to position things in specific ways."
  },
  {
    "objectID": "posts/write-your-thesis-or-paper-in-rmarkdown.html#footnotes",
    "href": "posts/write-your-thesis-or-paper-in-rmarkdown.html#footnotes",
    "title": "Write your thesis or paper in R Markdown!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, this cheatsheet‚Ü©Ô∏é\nIn my experience, I think you‚Äôre just going to end up editing these in Word anyway so I don‚Äôt know that it‚Äôs worth the bother to do in RStudio. You can if you want, I suppose.‚Ü©Ô∏é\nI‚Äôm including extra things here that I used over the process of making the thesis pdf, but it wasn‚Äôt what I used exactly in the end. I had some problems, that I can‚Äôt recall right now, with rendering bibliographical items so I switched to the default \\(\\LaTeX\\) renderer, pdflatex. Using this means you can‚Äôt use other fonts, like the Linux Libertine font above.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/docker.html",
    "href": "posts/docker.html",
    "title": "Using Docker for Data Science",
    "section": "",
    "text": "In this post, I‚Äôll go through a few examples of how you can use Docker for data science, from running a simple script to making reports. It‚Äôs based on real usage, so I think there are a couple of things in there that are interesting. In fact, using Docker for data science involves some non-beginner uses of Docker ‚Äì sharing files between the host and the container, installing packages, connecting to databases etc ‚Äì and we‚Äôll go through these here. Of course, there are tons of posts and whatnot on using Docker with R or Python ‚Äì see here for an intro with R (or here); here for a more detailed piece, and here for a recently-published ArXiv paper that goes into detail on the state of Docker use with R. For Python users, since many come from computer science backgrounds, I assume you can already find the millions of posts on using Python with Docker ü§ì. This post is approached from the R side of things, but plenty of it is applicable to Docker and Data Science more generally.\nWe‚Äôll start off simple and increase in complexity until we have a realistic example of how you can use Docker for thy data science needs. If you want to jump to the final section, off you go!\nWe‚Äôll begin with a simple R script, then we‚Äôll share files between the container and the host. Then we will include some Python code, called from R. After that, we‚Äôll include some R packages and renv, a package manager for R, and take advantage of its caching abilities, and see how we can use local (private) packages too. We will then include a database connection and set all that up. It‚Äôs very possible you‚Äôd want to run something like this as a command line script, so we‚Äôll set it up as one, and see how we can pass in arguments to the Docker container. Last but not least, we‚Äôll see how we can share the container for use by someone else, or for putting on a remote server, quite a common use case. The R & Python scripts will be simple to keep focus on Docker.\nFirst of all, let‚Äôs get some terminology out of the way. The ‚Äòhost‚Äô is your computer; we will build a docker image, and then from this image, we can run ‚Äòcontainers‚Äô, which can be understood as instances of the image. The Docker documentation is actually pretty good, you can access it here. We‚Äôll use docker-compose, which is like a config file for your image, and the image itself is build from a Dockerfile. We won‚Äôt use compose at the start but it will be useful later. We‚Äôll stick to some basic commands, but if you find yourself with a whole load of memory use, see here for simple ways to clean up. I‚Äôm doing this on a Mac, but as far as I‚Äôm aware, things are pretty similar on Windows and Linux."
  },
  {
    "objectID": "posts/docker.html#complete-example-automated-report",
    "href": "posts/docker.html#complete-example-automated-report",
    "title": "Using Docker for Data Science",
    "section": "Complete Example ‚Äì Automated Report",
    "text": "Complete Example ‚Äì Automated Report\n\nI‚Äôll include all the code here so you can reproduce this yourself. Explanations will be minimal since we already went through that üëÆ‚Äç‚ôÇÔ∏è\nOk, let‚Äôs get set up. We‚Äôre not going to get mega-complex here, any of the extras you need, you should be able to use some of the examples above (including Python, local packages, DB connections etc.). We‚Äôll have one master RMarkdown file and a main.R file that will control all of the other things we do.\nmkdir reporting && cd reporting\ntouch Dockerfile && touch docker-compose.yml\nmkdir rmarkdown && mkdir R\ntouch rmarkdown/master.Rmd\ntouch R/main.R\nOk, let‚Äôs see what we‚Äôd need for a realistic reporting app. We‚Äôll use cli to get a nice CLI and optparse to parse command line options fed in. We‚Äôll need a logger, so I‚Äôll use log4r. I‚Äôll also use some tidyverse packages. Let‚Äôs put all this in a R/libraries.R file so that main.R doesn‚Äôt get cluttered up. Our project looks like this now:\ntree\n.\n‚îú‚îÄ‚îÄ Dockerfile\n‚îú‚îÄ‚îÄ R\n‚îÇ   ‚îú‚îÄ‚îÄ libraries.R\n‚îÇ   ‚îî‚îÄ‚îÄ main.R\n‚îú‚îÄ‚îÄ docker-compose.yml\n‚îú‚îÄ‚îÄ reporting.Rproj\n‚îî‚îÄ‚îÄ rmarkdown\n    ‚îî‚îÄ‚îÄ master.Rmd\n\n2 directories, 6 files\nlibraries.R looks like this:\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tibble\")\nlibrary(\"rmarkdown\")\nlibrary(\"log4r\")\nlibrary(\"cli\")\nlibrary(\"optparse\")\nlibrary(\"knitr\")\nlibrary(\"glue\")\nWe‚Äôll create a few more things to get going. First, create a reports/ folder in the project root, that‚Äôs where we‚Äôll put our rendered reports. In our R folder, also create a cmdargs.R, which is where we‚Äôll parse the command line arguments that we‚Äôll pass to our script. cmdargs.R looks like this:\nparse_cmd_args &lt;- function() {\n  option_list &lt;- list(\n    make_option(\n      c(\"-i\", \"--input\"),\n      type = \"character\",\n      help = \"The text inoput you'd like to see rendered\",\n      metavar = \"character\"\n    ),\n    make_option(\n      c(\"-g\", \"--graph\"),\n      type = \"character\",\n      default = \"yes\",\n      help = \"Would you like a graph? Or a table?\",\n      metavar = \"character\"\n    )\n  )\n\n  opt_parser &lt;- OptionParser(option_list = option_list)\n  opt &lt;- parse_args(opt_parser)\n\n  return(opt)\n}\nWe‚Äôre going to allow the user to choose between a graph or a table in their output, and to put in some input that will be rendered as text. The graph is chosen by default.\nOur RMarkdown file, master.Rmd, will produce a HTML page. It looks like this:\n---\ntitle: \"Awesome Report\"\nauthor: \"Report Guy\"\nparams:\n  input: NULL\n  graph: NULL\ndate: \"Produced on: `r format(Sys.time(), '%b %d, %Y')`\"\noutput: html_document\n---\n\nThis is your amazeballs report, automated with R, RMarkdown and Docker. Your input was:\n\n`r asis_output(input)`\n\n{r message=FALSE, warning=FALSE}\nmcars &lt;- mtcars %&gt;% rownames_to_column()\nif (graph == \"yes\") {\n  asis_output(\"You asked for a graph! So here you go: \")\n  ggplot(mcars, aes(x = wt, y = reorder(rowname, wt))) +\n    geom_col(fill = \"firebrick\", col = \"black\") +\n    theme_bw() +\n    labs(y = NULL, x = \"Weight of Car\")\n} else {\n  asis_output(\"You wanted a table! So here you go: \")\n  kable(mcars)\n}\nWhat‚Äôs happening here? Well, with params in the YAML header, we‚Äôre sending command line arguments to the document, which are our input and whether the user wants a graph or not (\"yes\" and \"no\"). input gets printed with r asis_output(input), and the code chunk prints either a plot or a table.\nAll of this is called from main.R. This file contains the CLI code so our script will print some pretty output to the terminal as it‚Äôs working, and it loads our other R files as well as calls the rendering of the RMarkdown file. The CLI stuff starts with cli_ or it‚Äôs a rule (I use an empty rule to get some blank space). Other than that, we‚Äôre just sourcing files, parsing the command line arguments, logging anything of interest, and sending everything off to render() for RMarkdown to take care of it.\n#!/usr/bin/env Rscript\nstart &lt;- Sys.time()\nsuppressWarnings(\n  suppressMessages(\n    suppressPackageStartupMessages(\n      source(\"R/libraries.R\")\n    )\n  )\n)\nsource(\"R/cmdargs.R\")\n\nrule(line = \" \")\nrule(\n  center = \"Awesome Automated Report\", col = \"cyan\",\n  width = console_width()\n)\nrule(line = \" \")\ncli_text(\"{col_grey('Reports started at ')}{col_magenta(start)}.\")\n\nif (!dir.exists(\"logs/\")) {\n  dir.create(\"logs/\")\n}\n\nlog_file &lt;- paste0(\"logs/awesome_report_\", start, \".log\")\nlogger &lt;- logger(\"INFO\", appenders = file_appender(log_file))\ninfo(logger, glue(\"Reports started at {start}.\"))\n\ncmdargs &lt;- parse_cmd_args()\ngraph &lt;- cmdargs$graph\ninput &lt;- cmdargs$input\n\nif (graph == \"yes\") {\n  cli_text(col_green(\"You asked for a graph, so we'll make one!\"))\n  info(logger, \"Graph chosen by user.\")\n} else {\n  cli_text(col_green(\"You asked for a table, so we'll make one!\"))\n  info(logger, \"Table chosen by user.\")\n}\nrule(line = \" \")\noutput_f &lt;- glue(\"report_at_{start}.html\")\nparam_list &lt;- list(graph = graph, input = input)\ncli_alert_info(col_cyan(\"Report generation started...\"))\n\nrender(\n  input = \"rmarkdown/master.Rmd\", output_format = \"html_document\",\n  output_file = output_f, envir = new.env(),\n  output_dir = glue(\"reports/\"),\n  quiet = TRUE, params = param_list\n)\n\ncli_alert_success(col_green(\"Reports finished!\"))\nrule(line = \" \")\nWhat does this look like when we run it? Since it‚Äôs got #!/usr/bin/env Rscript at the top, we can run this from the terminal with Rscript and supply our arguments with --:\n$ Rscript R/main.R --input=\"hello\" --graph=\"yes\"\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Awesome Automated Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nReports started at 2020-02-25 10:53:21.\nYou asked for a graph, so we'll make one!\n\n‚Ñπ Report generation started...\n‚úì Reports finished!\nGreat üëª. You can‚Äôt see the nice colours here, but they‚Äôll render in your local terminal üë®üèº‚Äçüé®.\nOur log file was written successfully, it has this in it, for no input and graph=\"no\":\nINFO  [2020-02-25 12:06:38] Reports started at 2020-02-25 12:06:37.\nINFO  [2020-02-25 12:06:38] Table chosen by user.\nWhat does the actual HTML look like? For when the user passes in --input=\"hello\" and graph=\"yes\", it looks like this:\n\nAnd for when we pass in \"Here's a table this time\" as input and graph=\"no\":\n\nVery nice! Ok, it says ‚Äúautomated with R, RMarkdown and Docker‚Äù, so let‚Äôs get to the Docker part.\nOur Dockerfile is as follows:\nFROM r-base:3.6.0\n\nWORKDIR /main\nCOPY . /main\n\nRUN apt-get update \\\n    && apt-get install -y --no-install-recommends \\\n    pandoc \\\n    && rm -rf /var/lib/apt/lists/*\n\nRUN R -e \"install.packages(c('dplyr', 'ggplot2', 'optparse', 'knitr', 'cli', 'knitr', 'rmarkdown', 'log4r'), repos = 'http://cran.rstudio.com/')\"\n\nENTRYPOINT [\"Rscript\", \"R/main.R\"]\n‚Ä¶with our docker-compose.yml as:\nversion: \"3.5\"\nservices:\n  report:\n    build:\n      context: .\n    volumes:\n      - .:/main\nWe can build it with docker-compose build (actually, this takes quite a while, using a tidyverse R Docker image is probably a better idea here) and run it with docker-compose run report. What do we get?\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Awesome Automated Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nReports started at 2020-02-25 22:43:13.\nYou asked for a graph, so we'll make one!\n\n‚Ñπ Report generation started...\n‚úî Reports finished!\nSince we supplied no arguments, we got a report with a graph, you can check it out in your reports/ folder. We can supply arguments:\ndocker-compose run report --graph=\"no\"\n\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Awesome Automated Report ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n\nReports started at 2020-02-25 22:47:50.\nYou asked for a table, so we'll make one!\n\n‚Ñπ Report generation started...\n‚úî Reports finished!\n‚Ä¶aaaand a nice report appears in the appropriate folder with a table in it.\nThese are obviously minimal reports, but all of the infrastructure is here to build as complicated a (static) report as you could wish with RMarkdown. Go at it! üöÄ"
  },
  {
    "objectID": "posts/docker.html#sharing-the-docker-image",
    "href": "posts/docker.html#sharing-the-docker-image",
    "title": "Using Docker for Data Science",
    "section": "Sharing the Docker Image",
    "text": "Sharing the Docker Image\nNow that you‚Äôve got a program that runs inside of a Docker image, you have a few options for sharing it. You can just copy the project folder over to wherever ‚Äì a remote server, for example ‚Äì and build the Docker image from the Dockerfile there. Or you can just save it and load it again. Much easier.\nYou save the Docker image to a tarball as so:\ndocker save reporting_report &gt; reports.tar\nYou then transfer this file over to wherever you need to run from. From this location, run:\ndocker load &lt; reports.tar\nNow this can be run from the other location with docker-compose run reports as before. You can schedule it with Airflow, for example.\n Aaalright, hope this was helpful! Take it easy now üê∂"
  },
  {
    "objectID": "posts/communism.html",
    "href": "posts/communism.html",
    "title": "Peace, Bread and Data!",
    "section": "",
    "text": "I really like this image by Tom Burns.1 The liberal2 in me appreciates making cheap fun of people who were horribly mistaken (Lenin; Marx, although I don‚Äôt mean to slight his contributions to social science), scum like Stalin, and Fidel Castro, who might have started out with a laudable takedown of a corrupt dictator, but who then became‚Ä¶a corrupt dictator, of course. The artist in me just loves the awesome colours. And it‚Äôs pretty funny.\nSo? Well, since I like making plots in R, and I really like these colours, and I have an interest in the political economic side of it all‚Ä¶ let‚Äôs make some graphs! Maybe we can see how Communist countries compare to the evil West on a series of socio-economic indicators.\nThe colour scheme in this image is the following:\nWe can use these hex values directly in the ggplot() function.\nThis page has a list of former and currently communist countries (no way China is a communist country now, but anyway), and the World Bank data catalog has a whole load of indicators we can use; the ones I use are available from this GitHub repo.3\nNow we‚Äôve got lots of interesting data to use. Let‚Äôs have a look at life expectancy.\nAh, that font‚Ä¶4\nWell, everybody‚Äôs life expectancy is going up, that‚Äôs good, but there‚Äôs a massive difference between communist and non-communist countries in Africa and Asia. Former communist countries do slightly better in Eastern Europe, although it‚Äôs pretty much even. That‚Äôs quite a notable trend over such a short time period, it‚Äôs a pity the data doesn‚Äôt go back further. This might also be nice as a boxplot:\nCommunist countries were/are well-known for having rather large armies. In the data I‚Äôve taken from the World Bank, we have two variables for exploring this theme, Armed forces personnel (% of total labor force) and Military expenditure (% of GDP). Let‚Äôs see how they rank:\nWell, look at that. With the exception of the USA, and the less-of-an-exception of Algeria and South Sudan, these are all neighbours. Must be fun over there in the Middle East.\nSo what about our komrades?\nI‚Äôm not sure any of these can be really be classed as communist, perhaps stubborn ol‚Äô Cuba. Perhaps the number of armed forces personnel will live up the stereotype.\nThere we go! The Democratic People‚Äôs Republic of Korea hasn‚Äôt let us down in our hunt to confirm what we already want to find. Some surprises in here: Montenegro, Singapore. I‚Äôm guessing mandatory military service for makes of a certain age exists in these countries. Or maybe Montenegro really doesn‚Äôt like Greece.\nAnd how about GDP? Maybe it‚Äôs a bit unfair to compare these, since communism was supposedly against all that filthy wealth generation. But let‚Äôs Czech it out. If you‚Äôre re-creating these plots, remember that I‚Äôm using readPNG() and rasterGrob() to get the images ready for use with annotation_custom().\nUnsurprisingly, the communist countries don‚Äôt compete well in terms of GDP per capita.\nHow about other, less financial indicators, like adult literacy levels?\nStill the pattern we noticed earlier: while there is a notable difference between communist and non-communist countries inside of regions, the regions themselves vary widely."
  },
  {
    "objectID": "posts/communism.html#east-v-west-germany-final-battle",
    "href": "posts/communism.html#east-v-west-germany-final-battle",
    "title": "Peace, Bread and Data!",
    "section": "East v West Germany: FINAL BATTLE",
    "text": "East v West Germany: FINAL BATTLE\nAnybody who has been paying attention the ‚Äúcredibility revolution‚Äù in economics/political science will have seen it said that the ideal for any comparison is an experiment. ‚ÄúNatural‚Äù experiments (‚Äònaturally‚Äô occurring, i.e.¬†not created by the researcher) are the closest we can get in many social scientific settings. Luckily for us, this topic has at least one A-grade natural experiment: the division of Germany into East and West, the former under Soviet control and the latter a liberal ‚Äúcapitalist‚Äù economy.\nFor data on the comparison of these two economies, I use this paper, which gives me an excuse to use the pdftools package in R, in order to extract the data out of the pdf.\n#library(pdftools)\n\ndownload.file(\"https://www.researchgate.net/profile/Pete_Mavrokordatos/publication/296467806_Germany_Twenty_Years_After_The_Union/links/56f13f5508aec9e096b31908/Germany-Twenty-Years-After-The-Union.pdf\", destfile = \"e_w_germany.pdf\", mode = \"wb\")\n\n\ne_w &lt;- data_frame(Year = parse_date_time(seq(1980, 1989, 1), \"Y\"),\n                  East = c(201.9, 206, 205.3, 209.2, 215.2, 221.9,\n                           225, 228.9, 231.3, 234),\n                  West = c(1251.6, 1252.8, 1241.1, 1263, 1298.4, 1325,\n                           1356, 1376, 1427, 1479))\nde &lt;- readPNG(\"rmd_images/de.png\")\nde &lt;- rasterGrob(de, interpolate=T)\nwg &lt;- readPNG(\"rmd_images/wg_eagle.png\")\nwg &lt;- rasterGrob(wg, interpolate=T)\neg &lt;- readPNG(\"rmd_images/eg.png\")\neg &lt;- rasterGrob(eg, interpolate=T)\n\n# png of eagles\nggplot(e_w, aes(x = Year)) +\n  theme_classic() +\n  theme(text = element_text(family = \"Kremlin\"),\n        panel.background = element_rect(fill = \"black\")) +\n  annotation_custom(de, xmin = -Inf, xmax = Inf,\n                    ymin = -Inf, ymax = Inf) +\n  annotation_custom(wg,\n                    xmin = as.numeric(as.POSIXct('1987-01-01',\n                                         origin = '1970-01-01')),\n                    xmax = as.numeric(as.POSIXct('1989-01-01',\n                                         origin = '1970-01-01')),\n                    ymin = 1000, ymax = 1500) +\n  annotation_custom(eg,\n                    xmin = as.numeric(as.POSIXct('1987-01-01',\n                                         origin = '1970-01-01')),\n                    xmax = as.numeric(as.POSIXct('1989-01-01',\n                                         origin = '1970-01-01')),\n                    ymin = 100, ymax = 700) +\n  geom_line(aes(y = East), colour = \"#ff0000\", size = 1.3, linetype = 2) +\n  geom_line(aes(y = West), colour = \"#ff0000\", size = 1.3, linetype = 2) +\n  ylab(\"Real GDP for East and West Germany\\n Billions hated US dollars\")\n\nVICTORY TO EVIL WEST!!\nAs usual, the Economist has some of the best visualizations on the subject. We could recreate this in R, but let‚Äôs just use the original. It‚Äôs from this article.\n\nWell, one thing that we can be sure about is that the commies cracked some great jokes.\n‚ÄúStalin himself cracked them, including this one about a visit from a Georgian delegation: They come, they talk to Stalin, and then they go, heading off down the Kremlin‚Äôs corridors. Stalin starts looking for his pipe. He can‚Äôt find it. He calls in Beria, the dreaded head of his secret police. ‚ÄòGo after the delegation, and find out which one took my pipe,‚Äô he says. Beria scuttles off down the corridor. Five minutes later Stalin finds his pipe under a pile of papers. He calls Beria ‚Äì ‚ÄòLook, I‚Äôve found my pipe.‚Äô ‚ÄòIt‚Äôs too late,‚Äô Beria says, ‚Äòhalf the delegation admitted they took your pipe, and the other half died during questioning.‚Äô üòÑ"
  },
  {
    "objectID": "posts/communism.html#footnotes",
    "href": "posts/communism.html#footnotes",
    "title": "Peace, Bread and Data!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúPeace, Bread and Data!‚Äù I take from the Bolsheviks‚Äô ‚ÄúPeace, Bread and Land!‚Äù.‚Ü©Ô∏é\nIn an old-school Scottish sense.‚Ü©Ô∏é\nYou can download the data and read it into R, or read directly from the ‚Äúraw‚Äù version of the github page (i.e., https://raw.githubusercontent.com/RobertMyles/Various_R-Scripts/). I use readr from the tidyverse package for this, since it can read straight from an url of a csv with read_csv.‚Ü©Ô∏é\nThis can be downloaded from many sources, as you can see from the code, it‚Äôs called ‚ÄúKremlin‚Äù. I think it‚Äôs great.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/map-making-electionsbr.html",
    "href": "posts/map-making-electionsbr.html",
    "title": "Map-making with R and electionsBR",
    "section": "",
    "text": "For those interested in Brazilian politics, there‚Äôs a great new package called electionsBR (those who understand Portuguese can find a post on it here). This package takes data from the Tribunal Superior Eleitoral and makes it available in a tidy format for users of R. Given my recent obsession with map-making, I think it‚Äôs only natural I‚Äôd want to make maps of Brazil with this package.\nSo, what can we do with it? Well, how about a map of how Brazilians voted in the general election of 2014? To do this, we can use electionsBR to get the election data, and a mixture of tidyverse and some mapping and plotting packages:\nlibrary(tidyverse)\nlibrary(electionsBR)\nlibrary(ggmap)\nlibrary(rgdal)\nlibrary(stringi)\nlibrary(scales)\nlibrary(maptools)\nlibrary(RColorBrewer)\nThe vote_mun_zone_fed() function takes a single argument, year, as an integer. There are quite a lot of data, so it takes a little while to download.\nMun &lt;- vote_mun_zone_fed(2014)\nOnce we have these data, we can use the tidyverse to clean it up and organize it they way we want. I‚Äôm going to change the character encoding to ASCII, using the stringi package, and select only the columns I need.\nMun &lt;- Mun %&gt;%\n  select(SIGLA_UF, DESCRICAO_CARGO, CODIGO_MUNICIPIO, TOTAL_VOTOS,\n         NUMERO_CAND, NOME_MUNICIPIO, NUM_TURNO, SIGLA_PARTIDO) %&gt;%\n  mutate(NOME_MUNICIPIO = stri_trans_general(NOME_MUNICIPIO, \"Latin-ASCII\"))\nOne interesting thing we could do with this dataset is map the percentage of the electorate that voted for Dilma. We‚Äôll need shapefiles for Brazil, which you can get from gadm.org.\nWe‚Äôll also need to isolate the vote for Dilma and then calculate the proportion in each municipality that voted for her. There were also two rounds of voting, so we can show each one. The code below does this for the first round, to do the same thing for the second round, we just change the first call to filter to NUM_TURNO == 2.\nPres1 &lt;- Mun %&gt;%\n  filter(DESCRICAO_CARGO == \"PRESIDENTE\", NUM_TURNO == 1,\n         SIGLA_UF != \"ZZ\") %&gt;%\n  group_by(NUMERO_CAND, CODIGO_MUNICIPIO) %&gt;%\n  mutate(SUM = sum(TOTAL_VOTOS)) %&gt;%\n  distinct(CODIGO_MUNICIPIO, .keep_all=T) %&gt;%\n  ungroup() %&gt;%\n  group_by(CODIGO_MUNICIPIO) %&gt;%\n  mutate(PERC = TOTAL_VOTOS/sum(TOTAL_VOTOS)*100) %&gt;%\n  arrange(SIGLA_UF, NOME_MUNICIPIO) %&gt;%\n  ungroup() %&gt;%\n  filter(NUMERO_CAND == 13)\nNext, we read in our shape files. We have some work to do to tidy up the names of the municipalities and to correct for coding errors.\nBRmap &lt;- readOGR(dsn = \"BRA_adm_shp\", layer = \"BRA_adm3\", verbose = FALSE)\nBRmap@data$NAME_2 &lt;- BRmap@data$NAME_2 %&gt;%\n  as.character() %&gt;%\n  stri_trans_general(\"Latin-ASCII\") %&gt;%\n  toupper()\nLet‚Äôs see what municipalities are missing from our electionsBR municipality data.\n'%ni%' &lt;- Negate('%in%')\n\nunique(BRmap@data$NAME_2[which(BRmap@data$NAME_2 %ni% Mun$NOME_MUNICIPIO)])\n\n  [1] \"BARRA DA CHOCA\"                \"DIAS D'VILA\"\n  [3] \"LIVRAMENTO DO BRUMADO\"         \"MUQUEM DE SAO FRANCISCO\"\n  [5] \"OLIVERIA DOS BREJINHOS\"        \"PAU BRAZIL\"\n  [7] \"QUIJINGUE\"                     \"ITAPAJE\"\n  [9] \"MISSO VELHA\"                   \"SAO JOAO DO BELM\"\n [11] \"SAO LUIZ DO CURU\"              \"GUIA BRANCA\"\n [13] \"ILHA TRINDADE\"                 \"ILHAS DE MARTIM VAZ\"\n [15] \"AMERICANO DO BRAZIL\"           \"BRASABRANTES\"\n [17] \"MATEIRA\"                       \"PORTEIRO\"\n [19] \"SANTA RITA DE ARAGUAIA\"        \"ALTO ALEGRE DO MARANHO\"\n [21] \"AMAPA DO MARANHO\"              \"ANAPUROS\"\n [23] \"BOM JARDIN\"                    \"HUMBERTO CAMPOS\"\n [25] \"MATES DO NORTE\"                \"VICTORINO FREIRE\"\n [27] \"BATAIPORA\"                     \"BARRA DOS BUGRE\"\n [29] \"POXOREO\"                       \"SAO FELIX XINGU\"\n [31] \"BANDIERA DO SUL\"               \"BRASOPOLIS\"\n [33] \"CACHOEIRA DE PAJES\"            \"CAMPOS VERDES DE GOIAS\"\n [35] \"CARAVALHOPOLIS\"                \"CASSITERITA\"\n [37] \"CHAVESLANDIA\"                  \"FELISBERTO CALDEIRA\"\n [39] \"FRANCISCO DUMON\"               \"GOUVEA\"\n [41] \"ITABIRINHA DE MANTENA\"         \"ITACARAMBIRA\"\n [43] \"PIEDADE DO PONTE NOVA\"         \"PIUI\"\n [45] \"QUELUZITA\"                     \"SAO FRANCISCO DE OLIVEIRA\"\n [47] \"SAO SEBASTIO DA VARGEM ALEGRE\" \"SAN ANTONIO DO ITAMBE\"\n [49] \"SAN ANTONIO DO RIO ABAI\"       \"SANTA RITA DO IBITIPOCA\"\n [51] \"SANTA RITA ITUETO\"             \"ALMERIM\"\n [53] \"BRAGANGA\"                      \"ME DO RIO\"\n [55] \"BOQUEIRAO DOS COCHOS\"          \"DESTERRO DE MALTA\"\n [57] \"MONGEIRO\"                      \"PEDRA LAVADRA\"\n [59] \"RIACHO\"                        \"SAO MIGUEL TAIPU\"\n [61] \"SERIDO\"                        \"ALTAMIRA DO PARAN\"\n [63] \"ARAPU\"                         \"ASSIS CHATEAUBRI\"\n [65] \"CAMPO\"                         \"CONSELHEIRO MAYRINCK\"\n [67] \"IVATUVA\"                       \"JABUTI\"\n [69] \"SAO ANTONIO DE SUDOESTE\"       \"SALTO DO LONDRA\"\n [71] \"SANTA CRUZ DE MONTE CASTE\"     \"SANTA ISABEL DO OESTE\"\n [73] \"TEXEIRA SOARES\"                \"TIBAJI\"\n [75] \"VENCESLAU BRAS\"                \"VILA ALTA\"\n [77] \"BARRA DE GUABIRA\"              \"CABO\"\n [79] \"CACHOERINHA\"                   \"IGARACU\"\n [81] \"LAGOA DO ITAENGA\"              \"SAO JOAO DO BELMONTE\"\n [83] \"SAO JOAQUIN DO MONTE\"          \"SITIO DOS MOREIRAS\"\n [85] \"TAMBE\"                         \"PEDRO LI\"\n [87] \"SAO JOAO PIAUI\"                \"SAO MIGUEL TAPUIO\"\n [89] \"CAMPOS\"                        \"CAREPEBUS\"\n [91] \"CONCEICAO MACABU\"              \"ENGENHEIRO PAULO DE FRONT\"\n [93] \"PARATI\"                        \"VALENCIA\"\n [95] \"ACU\"                           \"AUGUSTO SEVERO\"\n [97] \"GOVERNADOR DIX-SEPT ROSAD\"     \"JANUARIO CICCO\"\n [99] \"JARDIM-PIRANHAS\"               \"JUNCO\"\n[101] \"LAGOA DE ANTA\"                 \"LAGOAS DE VELHOS\"\n[103] \"SAO MIGUEL DE TOUROS\"          \"BAJE\"\n[105] \"BARO\"                          \"BOA VISTA DAS MISSES\"\n[107] \"CAMAGUA\"                       \"CAMPO REAL\"\n[109] \"CHIAPETA\"                      \"DILERMANO DE AGUIAR\"\n[111] \"ERVAL\"                         \"INHACOR\"\n[113] \"LAGOA MIRIM\"                   \"MARCIONILIO DIAS\"\n[115] \"MAXIMILIANO DE ALMAEIDA\"       \"PALMITINHOS\"\n[117] \"SAO MIGUEL DAS MISSES\"         \"UREA\"\n[119] \"VITORIA DAS MISSES\"            \"ALTA FLORESTA D'OESTE\"\n[121] \"ALVORADA D'OESTE\"              \"ESPIGAO D'OESTE\"\n[123] \"NOVA BRASILANDIA D'OESTE\"      \"SAO FELIPE D'OESTE\"\n[125] \"SANTA LUZIA D'OESTE\"           \"ALFREDO MARCONDE\"\n[127] \"APARECIDA DOESTE\"              \"BRODOSQUI\"\n[129] \"DULCINOPOLIS\"                  \"EMBU\"\n[131] \"ESTRELA DO OESTE\"              \"FERNO\"\n[133] \"FERRAZ DE VASCON\"              \"FLORINIA\"\n[135] \"GUARANI DO OESTE\"              \"IPAUCU\"\n[137] \"JABUTICABAL\"                   \"LUISIANIA\"\n[139] \"PALMEIRA DO OESTE\"             \"PARANAPAREMA\"\n[141] \"PIRACUNUNGA\"                   \"PONTES GESTRAL\"\n[143] \"QUITANA\"                       \"SAO LUIZ DO PARAITINGA\"\n[145] \"SALTO DO PIRAPORA\"             \"SANTA CLARA DO OESTE\"\n[147] \"SANTA RITA DO OESTE\"           \"GRAO PARA\"\n[149] \"LUIZ ALVES\"                    \"PAULO LOPEZ\"\n[151] \"PICARRAS\"                      \"PONTA ALTA\"\n[153] \"BUQUIM\"                        \"GRACHO CARDOSO\"\n[155] \"ITAPORANGA DAJUDA\"             \"NOSSA SENHORA APRECIDO\"\n[157] \"COUTO MAGALHAES\"               \"MOSQUITO\"\nHmmm, that‚Äôs a little annoying, but some are easy to fix, so in the end, we‚Äôll be missing only a few municipalities because of these coding differences. Some others are harder to figure out: I don‚Äôt know if the errors are in the TSE‚Äôs data, or in this geo-data. I don‚Äôt feel like spending a long time recoding stuff though, so let‚Äôs leave it aside for now.\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"ASSIS BRAZIL\"] &lt;- \"ASSIS BRASIL\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"JOINVILE\"] &lt;- \"JOINVILLE\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"MACEIO (CAPITAL)\"] &lt;- \"MACEIO\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"SAO GABRIEL DE CAHOEIRA\"] &lt;- \"SAO GABRIEL DA CACHOEIRA\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"NOVO BRAZIL\"] &lt;- \"NOVO BRASIL\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"PERI-MIRIM\"] &lt;- \"PERI MIRIM\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"SEM-PEIXE\"] &lt;- \"SEM PEIXE\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"BRAZIL NOVO\"] &lt;- \"BRASIL NOVO\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"OLHOS-D'AGUA\"] &lt;- \"OLHOS D'AGUA\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"OLHO-D'AGUA DO BORGES\"] &lt;- \"OLHO D'AGUA DO BORGES\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"SERRA DA SAUDAD\"] &lt;- \"SERRA DA SAUDADE\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"PEIXE BOI\"] &lt;- \"PEIXE-BOI\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"RICAHO DOS CAVALOS\"] &lt;- \"RIACHO DOS CAVALOS\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"BRAZILEIRA\"] &lt;- \"BRASILEIRA\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"SUL BRAZIL\"] &lt;- \"SUL BRASIL\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"FLORINIAPOLIS\"] &lt;- \"FLORIANOPOLIS\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"BON JESUS DOS PERDOES\"] &lt;- \"BOM JESUS DOS PERDOES\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"OLHO-D'AGUA DO BORGES\"] &lt;- \"OLHO D'AGUA DO BORGES\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"MISSO\"] &lt;- \"MISSAO\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"SALIDAO\"] &lt;- \"SOLIDAO\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"SAO JOAO DAS DUAS PONTE\"] &lt;- \"SAO JOAO DAS DUAS PONTES\"\nBRmap@data$NAME_2[BRmap@data$NAME_2==\"ORLEAES\"] &lt;- \"ORLEANS\"\nWe can use fortify to get all this into something useful for ggplot() to deal with. Then we can add in all the data we have for Dilma‚Äôs vote totals and then we‚Äôre ready to plot something.\nBrasil &lt;- fortify(BRmap, region = \"ID_2\") %&gt;%\n  mutate(id = as.integer(id)) %&gt;%\n  full_join(BRmap@data, by =c(\"id\" = \"ID_2\")) %&gt;%\n  select(c(id, long, lat, order, hole, piece, group, NAME_2)) %&gt;%\n  rename(NOME_MUNICIPIO = NAME_2)\n\nhead(Brasil)\n  id      long       lat order  hole piece group NOME_MUNICIPIO\n1  1 -67.10586 -9.688110     1 FALSE     1   1.1     ACRELANDIA\n2  1 -67.05984 -9.706651     2 FALSE     1   1.1     ACRELANDIA\n3  1 -66.80647 -9.814520     3 FALSE     1   1.1     ACRELANDIA\n4  1 -66.62003 -9.894039     4 FALSE     1   1.1     ACRELANDIA\n5  1 -66.58875 -9.903196     5 FALSE     1   1.1     ACRELANDIA\n6  1 -66.62333 -9.923209     6 FALSE     1   1.1     ACRELANDIA\n\n\nDilma_1 &lt;- left_join(Brasil, Pres1) %&gt;%\n  mutate(PERC = ifelse(is.na(PERC), mean(PERC, na.rm=T), PERC))\nggplot() +\n  geom_polygon(data = Dilma_1, aes(x = long, y = lat,\n                                   group = group, fill = PERC),\n               color = \"white\", size = 0.1) +\n  scale_fill_distiller(palette = \"RdBu\",\n                       breaks = pretty_breaks(n = 8)) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(fill = \"Dilma (%)\") +\n  theme_nothing(legend = TRUE) +\n  xlim(range(Dilma_1$long)) + ylim(range(Dilma_1$lat)) +\n  coord_map()\n\nWe can see that even in 2014, Dilma‚Äôs support in the South-east of the country was not overwhelming.\nWe can also use electionsBR to look at other items of interest, such as the share of the party vote. For example, perhaps you‚Äôre interested in whether the Communist Party of Brazil has strongholds in the country. All we need to do is subset the Mun dataframe that we downloaded earlier by DESCRICAO_CARGO == \"DEPUTADO FEDERAL\" and SIGLA_PARTIDO == \"PC do B\". Apart from these changes, everything else can be done in the same way. Once we have this dataframe (which I‚Äôll call pc), we plot it in the same way:\nggplot() +\n  geom_polygon(data = pc, aes(x = long, y = lat, group = group,\n                              fill = PERC),\n               color = \"white\", size = 0.1) +\n  scale_fill_distiller(palette = \"RdBu\",\n                       breaks = pretty_breaks(n = 8)) +\n  guides(fill = guide_legend(reverse = TRUE)) +\n  labs(fill = \"PC do B (%)\") +\n  theme_nothing(legend = TRUE) +\n  xlim(range(pc$long)) + ylim(range(pc$lat)) +\n  coord_map()\n\nNot a very Communist country, by the looks of things.\nWell, that‚Äôs a brief look at electionsBR. Data for other years and elections is available, as well as data at other administrative levels, and not just the President and Federal Deputies. The TSE also holds data on the background of the candidates and their campaign spending, all of which can be utilized with electionsBR. And if you fancy combining all this information with legislative behaviour from inside the Chamber of Deputies, just load the bRasilLegis package and you have a wealth of data on Brazilian Federal Deputies at your fingertips. Indeed, I‚Äôm proud to be involved in both packages. It‚Äôs great to help to make the access to these data easier for those interested in Brazilian politics.\nP.s. This blog post was written using R Notebooks. I‚Äôd have to say that I really like R Notebooks so far, especially the preview. Try it out.\nUpdate: it seems that some folks might be running into problems running the scripts above, with R spitting out: Error: isTRUE(gpclibPermitStatus()) is not TRUE. The solution to this is to make sure you have rgdal or rgeos or a similar mapping package installed."
  },
  {
    "objectID": "posts/mtcars.html",
    "href": "posts/mtcars.html",
    "title": "What do the mtcars actually look like?",
    "section": "",
    "text": "It popped into my head the other day that I had no idea what most of the cars in the mtcars dataset look like. Some Google image searches later, I had a folder of them (you can get them here ‚Äì mtcars.zip, they‚Äôre all free to use as far as I could tell from the image search) and thought I‚Äôd try to shove them into tibbles and plots somehow.\nHere‚Äôs the list of the images:\nimages/\n‚îú‚îÄ‚îÄ AMCJavelin.jpg\n‚îú‚îÄ‚îÄ CadillacFleetwood.jpg\n‚îú‚îÄ‚îÄ CamaroZ28.jpg\n‚îú‚îÄ‚îÄ ChryslerImperial.jpg\n‚îú‚îÄ‚îÄ Datsun710.jpg\n‚îú‚îÄ‚îÄ DodgeChallenger.jpg\n‚îú‚îÄ‚îÄ Duster360.jpg\n‚îú‚îÄ‚îÄ FerrariDino.jpg\n‚îú‚îÄ‚îÄ Fiat128.jpg\n‚îú‚îÄ‚îÄ FiatX1-9.jpg\n‚îú‚îÄ‚îÄ FordPanteraL.jpg\n‚îú‚îÄ‚îÄ HondaCivic.jpg\n‚îú‚îÄ‚îÄ Hornet4Drive.jpg\n‚îú‚îÄ‚îÄ HornetSportabout.jpg\n‚îú‚îÄ‚îÄ LincolnContinental.jpg\n‚îú‚îÄ‚îÄ LotusEuropa.jpg\n‚îú‚îÄ‚îÄ MaseratiBora.jpg\n‚îú‚îÄ‚îÄ MazdaRX4.jpg\n‚îú‚îÄ‚îÄ MazdaRX4Wag.jpg\n‚îú‚îÄ‚îÄ Merc230.jpg\n‚îú‚îÄ‚îÄ Merc240D.jpg\n‚îú‚îÄ‚îÄ Merc280.jpg\n‚îú‚îÄ‚îÄ Merc280C.jpg\n‚îú‚îÄ‚îÄ Merc450SE.jpg\n‚îú‚îÄ‚îÄ Merc450SL.jpg\n‚îú‚îÄ‚îÄ Merc450SLC.jpg\n‚îú‚îÄ‚îÄ PontiacFirebird.jpg\n‚îú‚îÄ‚îÄ Porsche914-2.jpg\n‚îú‚îÄ‚îÄ ToyotaCorolla.jpg\n‚îú‚îÄ‚îÄ ToyotaCorona.jpg\n‚îú‚îÄ‚îÄ Valiant.jpg\n‚îî‚îÄ‚îÄ Volvo142E.jpg\nOk, so let‚Äôs load some libraries and and see what they look like in a tibble:\nlibrary(tidyverse)\nlibrary(glue)\nlibrary(pander)\nlibrary(magick)\n\nmt &lt;- mtcars %&gt;%\n  rownames_to_column(\"model\") %&gt;%\n  mutate(imgnames = glue(\"images/{str_remove_all(model, ' ')}.jpg\")) %&gt;%\n  rowwise() %&gt;%\n  mutate(car = pandoc.image.return(imgnames))\nmt %&gt;% select(model, car) %&gt;% pander()\n\n\n\nmodel\ncar\n\n\n\n\nMazda RX4\n\n\n\nMazda RX4 Wag\n\n\n\nDatsun 710\n\n\n\nHornet 4 Drive\n\n\n\nHornet Sportabout\n\n\n\nValiant\n\n\n\nDuster 360\n\n\n\nMerc 240D\n\n\n\nMerc 230\n\n\n\nMerc 280\n\n\n\nMerc 280C\n\n\n\nMerc 450SE\n\n\n\nMerc 450SL\n\n\n\nMerc 450SLC\n\n\n\nCadillac Fleetwood\n\n\n\nLincoln Continental\n\n\n\nChrysler Imperial\n\n\n\nFiat 128\n\n\n\nHonda Civic\n\n\n\nToyota Corolla\n\n\n\nToyota Corona\n\n\n\nDodge Challenger\n\n\n\nAMC Javelin\n\n\n\nCamaro Z28\n\n\n\nPontiac Firebird\n\n\n\nFiat X1-9\n\n\n\nPorsche 914-2\n\n\n\nLotus Europa\n\n\n\nFord Pantera L\n\n\n\nFerrari Dino\n\n\n\nMaserati Bora\n\n\n\nVolvo 142E\n\n\n\n\nSo that table looks like crap ‚Äôcos it‚Äôs being rendered through MDX and Gatsby etc., but if you run that code in RStudio, you‚Äôll get something like this snapshot:\n\nNice! Try it out, some of these are beautiful cars.\nWe could make the images smaller and annotate them with the name of the car, which might make it possible to view the dataset and the car in the same tibble view.\nimgs &lt;- glue(\"images/{dir('images')}\")\nmap(imgs, ~{\n  fileout &lt;- str_remove(.x, \".jpg\")\n  anno &lt;- str_remove(fileout, \"images/\")\n  image_read(.x) %&gt;%\n    image_resize(\"125x125\") %&gt;%\n    image_border(color = \"white\", geometry = \"0x20\") %&gt;%\n    image_annotate(text = anno, size = 14, gravity = \"southwest\", color = \"black\") %&gt;%\n    image_write(path = glue(\"{fileout}-annotated.jpg\"))\n  }\n)\nmtcars %&gt;%\n  rownames_to_column(\"model\") %&gt;%\n  mutate(imgnames = glue(\"images/{str_remove_all(model, ' ')}-annotated.jpg\")) %&gt;%\n  rowwise() %&gt;%\n  mutate(car = pandoc.image.return(imgnames)) %&gt;%\n  select(car, everything(), -c(model, imgnames)) %&gt;%\n  pander(justify = rep(\"left\", 12), split.cells = rep(1, 12),\n         split.table = Inf)\nThat looks like this screenshot:\n\nNot bad. Ok, let‚Äôs see if we can include them in a plot, thanks to Claus Wilke‚Äôs ggtext package:\nlibrary(ggtext)\n\nimgs_tiny &lt;- glue(\"images/{dir('images', pattern = 'annotated')}\")\n\nmap(imgs_tiny, ~{\n  fileout &lt;- str_remove(.x, \"-annotated.jpg\")\n  image_read(.x) %&gt;%\n    image_resize(\"70x70\") %&gt;%\n    image_write(path = glue(\"{fileout}-tiny.jpg\"))\n  }\n)\nmt2 &lt;- mtcars %&gt;%\n  rownames_to_column(\"model\") %&gt;%\n  mutate(\n    images = glue(\"images/{str_remove_all(model, ' ')}-tiny.jpg\"),\n    images = glue(\"&lt;img src='{images}'/&gt;\")\n    )\n\nlabels0 &lt;- mt2 %&gt;%\n  arrange(mpg) %&gt;%\n  filter(am == 0) %&gt;%\n  pull(images)\nlabels1 &lt;- mt2 %&gt;%\n  arrange(mpg) %&gt;%\n  filter(am == 1) %&gt;%\n  pull(images)\nam0 &lt;- ggplot(mt2 %&gt;% filter(am == 0),\n              aes(x = fct_reorder(model, mpg),\n                  y = mpg, fill = mpg)) +\n  geom_col() +\n  scale_x_discrete(\n    name = NULL,\n    labels = labels0\n  ) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  theme_minimal() +\n  labs(y = \"Miles per Gallon\", title = \"Automatic Transmission\") +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = .75),\n    legend.position = \"none\"\n    )\n\nam1 &lt;- ggplot(mt2 %&gt;% filter(am == 1),\n              aes(x = fct_reorder(model, mpg),\n                  y = mpg, fill = mpg)) +\n  geom_col() +\n  scale_x_discrete(\n    name = NULL,\n    labels = labels1\n  ) +\n  theme_minimal() +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(y = \"Miles per Gallon\", title = \"Maunual Transmission\") +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = .7),\n    legend.position = \"none\"\n    )\nam0\n\nam1\n\nWell they‚Äôre quite hideous üôÑ. Maybe if we plot less of them on each graph, we might get something a bit nicer. We can group the cars by where they were made ‚Äì roughly Germany, Asia, the US and Europe without Germany.\neurope &lt;- c(\"De Tomaso\", \"Maserati\", \"Volvo\", \"Pantera\", \"Fiat\",\n            \"Lotus\", \"Ferrari\", \"Porsche\")\nasia &lt;- c(\"Datsun\", \"Toyota\", \"Honda\", \"Mazda\")\n\nmt3 &lt;- mtcars %&gt;%\n  rownames_to_column(\"model\") %&gt;%\n  mutate(\n    images = glue(\"images/{str_remove_all(model, ' ')}-annotated.jpg\"),\n    images = glue(\"&lt;img src='{images}'/&gt;\"),\n    carmaker = str_extract(model, \"[a-zA-Z]* \") %&gt;% str_trim(),\n    carmaker = case_when(\n      carmaker == \"Hornet\" ~ \"AMC\",\n      is.na(carmaker) ~ \"Plymouth\", # Valiant\n      carmaker == \"Duster\" ~ \"Plymouth\",\n      carmaker == \"Camaro\" ~ \"Chevrolet\",\n      carmaker == \"Ford\" ~ \"De Tomaso\",\n      TRUE ~ carmaker\n    ),\n    region = case_when(\n    carmaker %in% europe ~ \"Europe\",\n    carmaker %in% asia ~ \"Asia\",\n    carmaker == \"Merc\" ~ \"Germany\",\n    TRUE ~ \"US\"\n  ))\nlabs_eu &lt;- mt3 %&gt;%\n  filter(region == \"Europe\") %&gt;%\n  arrange(mpg) %&gt;%\n  pull(images)\neu &lt;- ggplot(mt3 %&gt;% filter(region == \"Europe\"),\n              aes(x = fct_reorder(model, mpg),\n                  y = mpg)) +\n  geom_col(fill = \"#d02a1e\", colour = \"#911d15\") +\n  scale_x_discrete(\n    name = NULL,\n    labels = labs_eu\n  ) +\n  theme_minimal() +\n  labs(y = \"Miles per Gallon\", title = \"European Cars\") +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = .35),\n    legend.position = \"none\"\n    )\nlabs_asia &lt;- mt3 %&gt;%\n  filter(region == \"Asia\") %&gt;%\n  arrange(mpg) %&gt;%\n  pull(images)\nasia &lt;- ggplot(mt3 %&gt;% filter(region == \"Asia\"),\n              aes(x = fct_reorder(model, mpg),\n                  y = mpg)) +\n  geom_col(fill = \"#daa471\", colour = \"#b7712f\") +\n  scale_x_discrete(\n    name = NULL,\n    labels = labs_asia\n  ) +\n  theme_minimal() +\n  labs(y = \"Miles per Gallon\", title = \"Asian Cars\") +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = .7),\n    legend.position = \"none\"\n    )\nlabs_us &lt;- mt3 %&gt;%\n  filter(region == \"US\") %&gt;%\n  arrange(mpg) %&gt;%\n  pull(images)\nus &lt;- ggplot(mt3 %&gt;% filter(region == \"US\"),\n              aes(x = fct_reorder(model, mpg),\n                  y = mpg)) +\n  geom_col(fill = \"#e8682c\", colour = \"#ae4412\") +\n  scale_x_discrete(\n    name = NULL,\n    labels = labs_us\n  ) +\n  theme_minimal() +\n  labs(y = \"Miles per Gallon\", title = \"American Cars\") +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = .35),\n    legend.position = \"none\"\n    )\nlabs_ger &lt;- mt3 %&gt;%\n  filter(region == \"Germany\") %&gt;%\n  arrange(mpg) %&gt;%\n  pull(images)\nger &lt;- ggplot(mt3 %&gt;% filter(region == \"Germany\"),\n              aes(x = fct_reorder(model, mpg),\n                  y = mpg)) +\n  geom_col(fill = \"#314f6d\", colour = \"#22374c\") +\n  scale_x_discrete(\n    name = NULL,\n    labels = labs_ger\n  ) +\n  theme_minimal() +\n  labs(y = \"Miles per Gallon\", title = \"German Cars\") +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = .35),\n    legend.position = \"none\"\n    )\neu\n\nasia\n\nus\n\nger\n\nThey‚Äôre not so bad, at least the ones with fewer bars.\nRecently, Mikefc/coolbutuseless tweeted about a cool new package of his called ggpattern. There‚Äôs an example here of flags inside bars, let‚Äôs see if we can get cars in bars.\nlibrary(ggpattern)\nmt4 &lt;- mt3 %&gt;%\n  mutate(images = strex::str_after_first(images, \"'\") %&gt;%\n           strex::str_before_first(\"-annotated\"),\n         images = glue(\"{images}.jpg\"))\n\nggplot(mt4 %&gt;% filter(region == \"Germany\"),\n       aes(x = fct_reorder(model, mpg), y = mpg)) +\n  geom_bar_pattern(stat = \"identity\",\n    aes(\n      pattern_filename = fct_reorder(model, mpg)\n    ),\n    pattern         = 'image',\n    pattern_type    = 'none',\n    fill            = 'grey80',\n    colour          = 'grey66',\n    pattern_scale   = -1,\n    pattern_filter  = 'point',\n    pattern_gravity = 'east'\n  ) +\n  theme_minimal() + labs(x = NULL, y = \"Miles per Gallon\") +\n  theme(legend.position = 'none') +\n  scale_pattern_filename_discrete(choices = mt4 %&gt;%\n                                     filter(region == \"Germany\") %&gt;%\n                                     arrange(mpg) %&gt;%\n                                     pull(images)) +\n  coord_flip()\n\nggplot(mt4 %&gt;% filter(region == \"US\"),\n       aes(x = fct_reorder(model, mpg), y = mpg)) +\n  geom_bar_pattern(stat = \"identity\",\n    aes(\n      pattern_filename = fct_reorder(model, mpg)\n    ),\n    pattern         = 'image',\n    pattern_type    = 'none',\n    fill            = 'grey80',\n    colour          = 'grey66',\n    pattern_scale   = -1,\n    pattern_filter  = 'point',\n    pattern_gravity = 'east'\n  ) +\n  theme_minimal() + labs(x = NULL, y = \"Miles per Gallon\") +\n  theme(legend.position = 'none') +\n  scale_pattern_filename_discrete(choices = mt4 %&gt;%\n                                    filter(region == \"US\") %&gt;%\n                                    arrange(mpg) %&gt;%\n                                    pull(images)) +\n  coord_flip()\n\nggplot(mt4 %&gt;% filter(region == \"Europe\"),\n       aes(x = fct_reorder(model, mpg), y = mpg)) +\n  geom_bar_pattern(stat = \"identity\",\n    aes(\n      pattern_filename = fct_reorder(model, mpg)\n    ),\n    pattern         = 'image',\n    pattern_type    = 'none',\n    fill            = 'grey80',\n    colour          = 'grey66',\n    pattern_scale   = -1,\n    pattern_filter  = 'point',\n    pattern_gravity = 'east'\n  ) +\n  theme_minimal() + labs(x = NULL, y = \"Miles per Gallon\") +\n  theme(legend.position = 'none') +\n  scale_pattern_filename_discrete(choices = mt4 %&gt;%\n                                    filter(region == \"Europe\") %&gt;%\n                                    arrange(mpg) %&gt;%\n                                    pull(images)) +\n  coord_flip()\n\nggplot(mt4 %&gt;% filter(region == \"Asia\"),\n       aes(x = fct_reorder(model, mpg), y = mpg)) +\n  geom_bar_pattern(stat = \"identity\",\n    aes(\n      pattern_filename = fct_reorder(model, mpg)\n    ),\n    pattern         = 'image',\n    pattern_type    = 'none',\n    fill            = 'grey80',\n    colour          = 'grey66',\n    pattern_scale   = -1,\n    pattern_filter  = 'point',\n    pattern_gravity = 'east'\n  ) +\n  theme_minimal() + labs(x = NULL, y = \"Miles per Gallon\") +\n  theme(legend.position = 'none') +\n  scale_pattern_filename_discrete(choices = mt4 %&gt;%\n                                    filter(region == \"Asia\") %&gt;%\n                                    arrange(mpg) %&gt;%\n                                    pull(images)) +\n  coord_flip()\n\nNot so bad, again with the ones with fewer bars.\nIn Mike‚Äôs example, he puts the flags at the end of the bars. let‚Äôs do that:\nggplot(mt4 %&gt;% filter(region == \"Asia\"),\n       aes(x = fct_reorder(model, mpg), y = mpg)) +\n  geom_bar_pattern(stat = \"identity\",\n    aes(\n      pattern_filename = fct_reorder(model, mpg)\n    ),\n    pattern         = 'image',\n    pattern_type    = 'none',\n    fill            = 'grey80',\n    colour          = 'grey66',\n    pattern_scale   = -2,\n    pattern_filter  = 'point',\n    pattern_gravity = 'east'\n  ) +\n  theme_minimal() + labs(x = NULL, y = \"Miles per Gallon\") +\n  theme(legend.position = 'none') +\n  scale_pattern_filename_discrete(choices = mt4 %&gt;%\n                                    filter(region == \"Asia\") %&gt;%\n                                    arrange(mpg) %&gt;%\n                                    pull(images)) +\n  coord_flip()\n\nImage deteriorates in quality but prob a better plot overall. We could also use the images as geoms themselves with the ggimage package:\nlibrary(ggimage)\nggplot(mt4, aes(x = wt, y = mpg)) +\n  geom_image(aes(image = images), size = 0.1) +\n  geom_label(aes(label = model), size = 2.5, nudge_y = -0.75) +\n  theme_minimal()\n\n‚Ä¶or maybe not.\nLike I said above, some of these cars are gorgeous, could be nice to see them in a little Shiny app or something.\nUpdate: Turns out Mara Averick, @dataandme on Twitter, posted pics of these cars back in 2018! Many are even the same photos. Nice to see I‚Äôm not the only one who wondered what they look like!"
  },
  {
    "objectID": "posts/suicides-in-ireland.html",
    "href": "posts/suicides-in-ireland.html",
    "title": "Suicides in Ireland",
    "section": "",
    "text": "The Irish radio station newstalk published this video the other day, in which director and actor Terry McMahon spoke out against the austerity programme running in Ireland since the aftermath of the financial crisis in 2008. Leaving aside his conflation of any type of business activity with immorality, McMahon claimed that ‚Äúausterity is murder‚Äù and detailed some alarming facts about suicide numbers in Ireland, clearly linking the two (i.e., austerity = more suicide).\nEven though I really hated the fact that irresponsible banks were socialized for their own recklessness, while the rest of the population suffered tax hikes and cuts to important services, there was something about this video that irritated me. McMahon states that we have let down ‚Äúthe best‚Äù of our society, being Padraig Pearse and his comrades of the 1916 Rising (which did not have popular support and took most Irish people by complete surprise). How, exactly? By not sticking to De Valera‚Äôs impoverishing mercantilism or by not leaving women in their constitutional place, the home? Leaving aside these irritations, I decided to take a closer look at the suicide claims made by McMahon. These data are publicly available, from the National Suicide Research Foundation.\nWe can do all this quite easily in R:\n\nlibrary(rvest)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(lubridate)\npage &lt;- read_html(\"https://nsrf.ie/statistics/suicide/\")\n\ntable &lt;- html_nodes(page,\n                    css = \"#ac_3163_collapse1 &gt; div &gt; table:nth-child(2)\")\n\ntable &lt;- html_table(table)[[1]]\n\ncolnames(table) &lt;- c(\"Year\", \"Total_number\", \"Total_rate_per_100000\",\n                     \"Male_number\", \"Male_rate_per_100000\",\n                     \"Female\", \"Female_rate_per_100000\")\n\nggplot(table, aes(x = Year)) +\n  geom_line(aes(y = Male_rate_per_100000), colour = \"navy\") +\n  geom_line(aes(y = Female_rate_per_100000), colour = \"goldenrod2\") +\n  scale_x_continuous(breaks = c(2001, 2004, 2007, 2010, 2013, 2015)) +\n  theme_classic() +\n  labs(y = \"Suicide Rate per 100,000\") +\n  annotate(\"label\", label = \"Male\", x = 2002, y = 18) +\n  annotate(\"label\", label = \"Female\", x = 2002, y = 6.5)\n\n\nJudging from 2001 onwards, suicide among males has in fact gone down, from highs in 2001 (22.4 per 100,000), 2004 (20.2; both years well before the austerity programme) and 20.2 per 100,000 in 2011, two years after the start of the austerity programme. After 2012, suicide among males thankfully declines by relatively quite a lot.\nAnother of McMahon‚Äôs claims is that more people committed suicide in 2016 than died in the Easter Rising of 1916. I‚Äôm not quite sure where he got these statistics. The National Suicide Research Foundation has until 2015, while the Central Statistics Office has the first two quarters of 2016 available on its website (first and second). According to these figures, there were 91 suicides in the first quarter and 75 in the second, being 166 in total. If we divide this number in two as our best guess for quarter 3, we get 83. People think that suicides go up at Christmas, but according to the website statnews.com, that is not true. Well, even allowing for 100 suicides in the 4th quarter, we get 166 + 83 + 100 = 349 (keep in mind this is quite an inflated guess, something more realistic would be 166 + 75 + 75 = 316). According to Wikipedia, ‚Äúalmost 500 people were killed in the Easter Rising‚Äù. So McMahon is incorrect on this point.\nMcMahon also said that during the last eight years of austerity more people have died by suicide in Ireland than died during the thirty-year-long troubles in Northern Ireland. A statistical breakdown of the deaths in the Troubles can be found here, which puts the total number of deaths at 3466. Using the data we got from the National Suicide Research Foundation:\n\nlibrary(knitr)\nkable(table[,1:3])\n\n\n\n\nYear\nTotal_number\nTotal_rate_per_100000\n\n\n\n\n2001\n519\n13.5\n\n\n2002\n478\n12.2\n\n\n2003\n497\n12.5\n\n\n2004\n493\n12.2\n\n\n2005\n481\n11.6\n\n\n2006\n460\n10.9\n\n\n2007\n458\n10.6\n\n\n2008\n506\n11.4\n\n\n2009\n552\n12.4\n\n\n2010\n495\n11.1\n\n\n2011\n554\n12.1\n\n\n2012\n541\n11.8\n\n\n2013\n487\n10.6\n\n\n2014\n459\n10.0\n\n\n2015\n451\n9.7\n\n\n\nSumming these numbers for the years 2009 to 2015 (the austerity programme) is straightforward:\n\ntable %&gt;%\n  select(Year, Total_number) %&gt;%\n  filter(Year &gt; 2008, Year &lt;= 2015) %&gt;%\n  summarise(total = sum(Total_number))\n\n##   total\n## 1  3540\nHere, McMahon is correct, which is shocking and sad (remember, we didn‚Äôt include 2016), but to actually tie these suicides to austerity causally, we would need a much more sophisticated approach. (Likewise with the decline since 2001.)\nThe point of this post is not to argue for the austerity programme (the banks should have paid for their mistakes and should not have been rescued. If they truly were ‚Äútoo big to fail‚Äù in terms of the Irish economy, then we could have rescued them and then liquidated them, returning the money to the state.) It is also not to argue for/against NAMA. Occupying a building for a few weeks puts the debate at centre-stage, for which McMahon and co. should be congratulated. But if we are to have a debate about the problems introduced by austerity, we should at least get the numbers right. Suicide for both men and women has declined in recent years in Ireland, and more people died in the Easter Rising than by suicide in 2016. Yes, more people died by suicide since 2009 than in the Troubles, which is tragic. I‚Äôll leave McMahon‚Äôs bizarre eulogies for an Ireland of poets and warriors, opposed to any type of commerce, aside (what finances the arts?). The crony capitalism that flourished in Ireland during the Bertie Ahern years damaged the country, no doubt. But no need to hark back to a simpler, poorer, time: just fix the mess and jail those responsible (including Ahern). But saying ‚Äúausterity is murder‚Äù and evoking the ‚Äòheroes‚Äô of 1916 is just pretty reckless, in my view."
  },
  {
    "objectID": "posts/easy-r-and-py.html",
    "href": "posts/easy-r-and-py.html",
    "title": "Easily Use Python and R together with {reticulate}",
    "section": "",
    "text": "I work in an environment where R and Python are used interchangeably, and most of the data scientists here have some familiarity with both languages. We regularly use one language to call the other and I‚Äôve been struck by just how easy this is, particularly with RStudio‚Äôs {reticulate} package (also with Python‚Äôs subprocess). We‚Äôve tried rpy2, but it‚Äôs a poor man‚Äôs reticulate in my opinion.\nHere‚Äôs a quick example of utilising a Python script in R with the use of reticulate, based on a real problem I had recently.\nlibrary(dplyr)\nlibrary(reticulate)\nI was working with some data that had a peculiar timestamp, which I could not parse correctly in R. I‚Äôm not saying this can‚Äôt be done in R, just that I couldn‚Äôt do it and didn‚Äôt have the time to delve into the complexities of date formats in R. I knew it parsed correctly with Pandas, so that‚Äôs what I used.\nSo here‚Äôs a tiny fake dataset that illustrates the issue:\ndf &lt;- tibble(\n  random_id = sample(letters, 10),\n  timestamp = c(1537363730375, 1537363680645, 1537363720707, 1537363740836, 1537363740176,\n  1537363700649, 1537363780495, 1537363730636, 1537363730041, 1537363767311)\n)\n\ndf %&gt;%\n  mutate(date = as.POSIXct(timestamp, origin = \"1970-01-01\"))\n\n## # A tibble: 10 x 3\n##    random_id     timestamp date\n##    &lt;chr&gt;             &lt;dbl&gt; &lt;dttm&gt;\n##  1 b         1537363730375 50687-02-12 16:39:35\n##  2 i         1537363680645 50687-02-12 02:50:45\n##  3 w         1537363720707 50687-02-12 13:58:27\n##  4 v         1537363740836 50687-02-12 19:33:56\n##  5 h         1537363740176 50687-02-12 19:22:56\n##  6 j         1537363700649 50687-02-12 08:24:09\n##  7 r         1537363780495 50687-02-13 06:34:55\n##  8 f         1537363730636 50687-02-12 16:43:56\n##  9 u         1537363730041 50687-02-12 16:34:01\n## 10 t         1537363767311 50687-02-13 02:55:11\nAs you can see, the dates are incorrect. Like I said, there‚Äôs probably a way to fix this in R, but after searching Stack Overflow and other resources, I didn‚Äôt find a solution so I quickly coded one up in Python:\nimport pandas as pd\n\ndef format_date(r_df):\n  r_df[\"timestamp\"] = pd.to_datetime(r_df[\"timestamp\"], unit = \"ms\")\n  return r_df\nPandas‚Äô unit argument to to_datetime() here is what solved it for me. We can just source this script with reticulate and this makes the function available to us in R, which we can use as part of a sequence of regular pipe operations:\nsource_python(\"/path/to/this/Python/script.py\")\n\ndf %&gt;%\n  format_date() %&gt;% # Python function\n  as_tibble() %&gt;%\n  pull(timestamp) %&gt;%\n  str()\n\n##  POSIXct[1:10], format: \"2018-09-19 13:28:50\" \"2018-09-19 13:28:00\" \"2018-09-19 13:28:40\" \"2018-09-19 13:29:00\" \"2018-09-19 13:29:00\" ...\nAs you can see, the timestamp is now formatted in a way we can easily use. So. Easy."
  },
  {
    "objectID": "posts/boozing-earth.html",
    "href": "posts/boozing-earth.html",
    "title": "Inhaling/Boozing Earth",
    "section": "",
    "text": "After seeing Nadieh Bremer‚Äôs great Breathing Earth infographic, I thought it would be cool to recreate it in R, as you do. Then I saw that it was made from lots of tif files‚Ä¶hmmm. I did some work with those before, ain‚Äôt doin it again voluntarily, no thanks.\nSo then I started thinking about something else that would be (kind of) similar and interesting. I saw the sf package and its interesting geom_sf() recently, and so I thought it would be a nice opportunity to try that out. Given we started with ‚Äòbreathing‚Äô Earth, the natural next step was to think of ‚Äòinhaling‚Äô Earth! A quick download of some cannabis data from here1 and we‚Äôre (almost) ready to go, just some cleaning, tidying and merging with the geometry data from the rnaturalearth package. This takes some tidying, unfortunately.2\nHmm, not a good candidate for animations‚Ä¶ Look at all that missing data.\nHow about booze? We can get some data from here, (filtered for ‚ÄòAll Types‚Äô), join the years available (three tables) and tidy it all up:\nThat works! Nice. Ok, it‚Äôs pretty simple, but given the hellish wrangling involved with some spatial polygon sets and administrative unit geographical data, I‚Äôm really impressed with how easy geom_sf() was to use. Good work, ggplot & sf folks!"
  },
  {
    "objectID": "posts/boozing-earth.html#footnotes",
    "href": "posts/boozing-earth.html#footnotes",
    "title": "Inhaling/Boozing Earth",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRefresh the page if it doesn‚Äôt log in automatically. You‚Äôre looking for ‚ÄúAnnual prevalence, adults‚Äù &gt; ‚ÄúCannabis‚Äù &gt; ‚ÄúDownload as Excel‚Äù. Sigh, just create an API, UN. AND these are .xls files. Grrrr!!! So you‚Äôre better off opening it up and saving it as a .csv.‚Ü©Ô∏é\nI save both of these as gifs and store them on imgur, since they don‚Äôt render correctly with blogdown for some reason].‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/Stan or JAGS for Bayesian ideal-point IRT.html",
    "href": "posts/Stan or JAGS for Bayesian ideal-point IRT.html",
    "title": "Stan or JAGS for Bayesian ideal-point IRT?",
    "section": "",
    "text": "Anybody who has ever tried to run even a moderately-sized Bayesian IRT model in R (for ideal points as in the political science literature, or otherwise) will know that these models can take a long time. It‚Äôs not R‚Äôs fault: these are usually big models with lots of parameters, and naturally take longer.1 Not to mention the fact that Bayesian computation is more computationally intense than other methods. Historically (okay, I‚Äôm talking about the last twenty years, maybe ‚Äòhistorically‚Äô is a little strong), the sampling software BUGS (Bayesian Inference Using Gibbs Sampling) and then JAGS were used to run Bayesian models (JAGS is still pretty common, and BUGS too, though not as much). Lately, Stan has been gaining ground, certainly as regards more complex modelling.\nWhile the reasons for choosing Stan are often put down to speed, when running many types of models there is not actually a large difference, with JAGS actually being faster for some models, according to John Kruschke2. Given the lack of a big difference between JAGS/BUGS and Stan, which sampling software should we use for IRT models? Well, first of all, a large part of the literature utilises either JAGS or BUGS, indeed, code is publicly available for many of these models, helping to spread the use of these two modelling languages.3 For beginners, this is a handy way to learn, and it‚Äôs how I learned. Indeed, the language of JAGS/BUGS (I‚Äôm just going to use ‚ÄòJAGS‚Äô to refer to both from now on) is a bit more intuitive for many people, and given the availability of others‚Äô code, beginning with these models can then be reduced to just tinkering with small details of code that is already written.\nStan, on the other hand, is newer and has a syntax that is in some ways quite different from JAGS. Variables need to be declared, as does their type (something not many R users are familiar with, I certainly wasn‚Äôt). The model code is imperative, not declarative4, and there are specific ‚Äòblocks‚Äô to the code. Stan has a different default sampler and is generally argued by its creators to be much faster. Well, in my experience, there is actually no contest. As much as I liked JAGS when I started out, Stan is simply incomparable to JAGS in terms of speed for these models‚Äì Stan is much, much faster. I was analysing nominal vote data for the Brazilian Federal Senate5 (these data have plenty of missing values, which are handled easily in JAGS but have to be deleted out in Stan) and, through the use of the runjags package (and its autorun option), I discovered that it would take around 28 hours to run my two-dimensional model to reach signs of convergence (or signs of non-convergence, as Gill puts it). As I was in the middle of writing a PhD thesis with lots of these models to process, that just wasn‚Äôt an option. (Regardless, any time I let the model run like this, R crashed or became unresponsive, or the estimates were simply of bad quality.) So I started tinkering with the options in runjags, trying different samplers etc. Then I noticed exactly why JAGS is so slow for these models.\nIn order to run a model, JAGS first compiles a Directed Acyclic Graph (DAG) of all the nodes in the model (software such as NIMBLE will let you print out the graph pretty easily). But since we have a latent regression with an unobserved regressor in the equation6\n\\[\ny_{ij} = \\beta_j\\bf{x_i} - \\alpha_j\"\n\\]\nthen JAGS is unable to build such a DAG. Since it can‚Äôt build a DAG, it can‚Äôt surmise that there is conjugacy in the model and then exploit that through Gibbs sampling. So JAGS just uses the default Metropolis-Hastings sampler (and given that it is called Just Another Gibbs Sampler, it kind of misses the point of using JAGS in the first place). This means that all the gains available through Gibbs sampling are simply not available for latent models of this type with JAGS, and hence the sampling process runs very slowly. I‚Äôm not sure the literature was ever aware of this fact, either. Many papers and books extoll the virtues of Gibbs sampling (and spend pages and pages deriving the conditional distributions involved) and then show the reader how to do it in JAGS or BUGS (see Simon Jackman‚Äôs book for an example)7, but unbeknownst to these authors, their JAGS programs are not using Gibbs sampling.\nSo that leaves us with Stan. Use it! üòé\nIn a future post, I‚Äôll show some examples of IRT ideal-point models in Stan. I have some on my Github, and Pablo Barber√° also has some nice examples (hat tip: I learned from him, amongst others. Thanks, Pablo!).\nUpdate: Guilherme Jardim Duarte also has some Bayesian IRT examples on his Github, in particular the dynamic model of Martin & Quinn, have a look."
  },
  {
    "objectID": "posts/Stan or JAGS for Bayesian ideal-point IRT.html#footnotes",
    "href": "posts/Stan or JAGS for Bayesian ideal-point IRT.html#footnotes",
    "title": "Stan or JAGS for Bayesian ideal-point IRT?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor more on how these models can have tons of parameters, see Clinton, Jackman, and Rivers (2004): ‚ÄòThe statistical analysis of roll-call data‚Äô, American Political Science Review, Vol. 98, No.¬†2.‚Ü©Ô∏é\nKruschke mentions this in his book‚Ä¶not sure where, exactly.‚Ü©Ô∏é\nSee this paper by Curtis (pdf downloads automatically) or the book by Armstrong et. al.‚Ü©Ô∏é\nSee here for the difference.‚Ü©Ô∏é\nYou can read about this research here.‚Ü©Ô∏é‚Ü©Ô∏é\nThis is the canonical statistical model for Bayesian IRT. The data (\\(y_{ij}\\)) are the votes, in binary form (1 = ‚ÄòYes‚Äô; 2 = ‚ÄòNo‚Äô); the \\(x_i\\) are the ideal points of the legislators; and \\(\\beta_j\\) and \\(\\alpha_j\\) are the discrimination (slope) and difficulty (intercept) parameters, respectively. See the article cited in footnote 1.‚Ü©Ô∏é\nI don‚Äôt mean to denigrate Jackman‚Äôs book. It‚Äôs highly detailed and thorough, and he deserves a lot of credit for spearheading the use of these Bayesian IRT models in political science. I‚Äôve cited his work numerous times, I‚Äôm a fan.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/modelscript.html",
    "href": "posts/modelscript.html",
    "title": "modelscript",
    "section": "",
    "text": "{modelscript} is a little RStudio add-in I wrote to help me with modelling. I‚Äôve been testing out the tidymodels framework for R (fantastic, btw) and I thought it would be handy to be able to create a new .R file with all the steps you‚Äôd need for most modelling tasks. You can install it with remotes::install_github(\"RobertMyles/modelscript\"). A new RStudio add-in will appear in the Addins menu that will create your R modelling script:\n\nFor anybody who‚Äôd like to make their own RStudio add-in, it was easy to do. In RStudio, you can create a new section in an .R file with control shift R, so I use this basic idea to create a template for all (or most, depending on the application) of the steps you‚Äôd typically use in any modelling task. Since we can interact with RStudio itself through rstudioapi, it was quite straightforward to set it up as an RStudio add-in.\nBasically, you create a new R package, create an inst/rstudio/addins.dcf file in the package, and fill in the following four entries:\nName:\nDescription:\nBinding:\nInteractive:\nMine is:\nName: modelscript\nDescription: Creates a modelling script template in a new .R file.\nBinding: modelscript\nInteractive: false\nThe Binding is the (R) function it calls, in this case a function called modelscript(). This function is quite straightforward, it uses rstudioapi::documentNew() to create a new .R file with the text I wanted used. Once you install modelscript, refresh your Rsession and a new entry on the RStudio Addins menu will appear, which you can click to create the .R file as in the GIF above.\nThe template itself contains a prep and a model building section. ‚Äòprep‚Äô contains subsections for:\n\nlibraries\ndata\nEDA\ntidy\nvisualize\n\n‚ÄòBuild Model‚Äô contains subsections for:\n\n1: Initial Split\n2: Preprocessing\n3: Model Specification\n4: Hyperparameter Tuning Specification\n5: Bundle into Workflow\n6: Cross Validation\n7: Tune\n8: Explore Tuning Results\n9: Finalize Workflow\n10: Final Fit\n11: Evaluate\n\nObviously these might not all be used, or other parts might be needed, or for example, the EDA - tidy - visualize part might be repeated, but it‚Äôs a nice helper for getting up and running. And you could of course modify it for your own use and then make your own package and add-in. ü§ì"
  },
  {
    "objectID": "posts/img-table-r.html",
    "href": "posts/img-table-r.html",
    "title": "Images as column headers in R",
    "section": "",
    "text": "Have you ever wanted to include an image as a column header in a data frame? Of course you have!\nAll joking aside, this is actually surprisingly common in corporate environments, where tables may have the company logo in the header, probably as the first ‚Äòcolumn‚Äô. You can actually do this in Rmarkdown, with a little help from rlang, tibble knitr and pander. Here‚Äôs how.\nI‚Äôm going to use Google‚Äôs logo cos I‚Äôm sure they wouldn‚Äôt mind. You need to download a copy of this and you‚Äôll need to size it to fit your table. The image below is 115x40 (width x height).\nFirst, load libraries and read in the image with pander, assuming it‚Äôs in your working directory:\nlibrary(pander)\nlibrary(knitr)\nlibrary(rlang)\nlibrary(tibble)\nlibrary(magrittr) # fo' da pipe\n\nimg &lt;- \"google_logo.png\" %&gt;% pander::pandoc.image.return()\nYou‚Äôll notice this is just markdown: img is \"![](google_logo.png)\". Ok, rlang, pander and knitr magic coming up üßôÔ∏è. The code we‚Äôll use is the following:\ntibble(\n  {{ img }} := c(\"Metric One\", \"Metric Two\"),\n  `Segment One` = c(100, 250),\n  `Segment Two` = c(20, 12)\n  ) %&gt;%\n  pandoc.table(\n    justify = c(\"left\", \"right\", \"right\"),\n    keep.line.breaks = TRUE\n  )\nWhere { img } uses curly-curly and the walrus operator to access the image variable we‚Äôve created. It gets rendered as markdown thanks to pander‚Äôs pandoc.table() and knitr helps with code chunk options. Importantly, these need to include results = 'asis'. So it‚Äôll look like this, minus the #:\n#```{r results='asis'}\n# amazing code...\n#```\nSo, putting all this together, we get:\ntibble(\n  {{ img }} := c(\"Metric One\", \"Metric Two\"),\n  `Segment One` = c(100, 250),\n  `Segment Two` = c(20, 12),\n  `Total for Segments` = c(120, 262)\n  ) %&gt;%\n  pandoc.table(\n    justify = c(\"left\", \"right\", \"right\", \"right\"),\n    keep.line.breaks = TRUE\n  )\n 31b8e172-b470-440e-83d8-e6b185028602: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:31b8e172-b470-440e-83d8-e6b185028602 \nNow that‚Äôs a nice table üòç. And so easy! Keep in mind the styling here is basic, but knitting this in RStudio gives me:\n\nNow that‚Äôs a nice table.\nYou‚Äôre not limited to tables by the way. Have a look at my Stack Overflow answer here that shows how easy it is to use images in the table cells. Long story short:"
  },
  {
    "objectID": "posts/UKElections2017.html",
    "href": "posts/UKElections2017.html",
    "title": "UK Elections 2017",
    "section": "",
    "text": "This post is a quickie to show how we can visualize the UK election results with just a few lines of R code. (Really, very few). 1\nWe can load in our usual tidyverse tools, along with a handy little data package, parlitools.\nlibrary(readr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(parlitools)\nlibrary(sf)\nThanks to this R Bloggers post, we have the data (the UK Electoral Commission must have it up by now anyway), so visualizing it is very easy:\nresults &lt;- read_csv(\"/Users/robert/Downloads/EconomistUK2017.csv\")\n\nuk &lt;- west_hex_map\n\nres &lt;- inner_join(results, uk, by = c(\"Constituency.ID\" = \"gss_code\")) %&gt;%\n  filter(!is.na(win)) %&gt;%\n  st_as_sf()\n\n\nggplot(res) +\n  geom_sf(aes(fill = win), size = 0.2) +\n  theme_minimal() +\n  guides(fill = guide_legend(title = \"party\")) +\n  scale_fill_manual(values = c(\"#006BA4\", \"#800B05\", \"#349B3A\", \"#888888\", \"#DB434E\",\n                    \"#E8B335\", \"#98B3D1\", \"#60B031\", \"#8DDC64\",\"#FCDD02\"))\nBam! Easy, quick and lovely üòÑ."
  },
  {
    "objectID": "posts/UKElections2017.html#footnotes",
    "href": "posts/UKElections2017.html#footnotes",
    "title": "UK Elections 2017",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm using the dev version of ggplot2 here, we need it for geom_sf().‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/tips-and-tricks-for-rmarkdown.html",
    "href": "posts/tips-and-tricks-for-rmarkdown.html",
    "title": "Tips and Tricks for R Markdown html",
    "section": "",
    "text": "Here are a couple of little tips and tricks that I‚Äôve picked up for use with RMarkdown html documents (including presentations and notebooks). This post is aimed at the R user who doesn‚Äôt know much, if anything, about html and css."
  },
  {
    "objectID": "posts/tips-and-tricks-for-rmarkdown.html#background-images",
    "href": "posts/tips-and-tricks-for-rmarkdown.html#background-images",
    "title": "Tips and Tricks for R Markdown html",
    "section": "Background images",
    "text": "Background images\nSometimes it‚Äôs useful (or just nice) to have a background image of some sort in a presentation or notebook. This could be the logo of your university or company, for example. To do this for a R Markdown document, you will need to do three things:\n\ncreate a separate .css file,\nhave/create an image, made suitably transparent,\nchange the YAML in the R Markdown document.\n\nYou can create a css file in any text editor. In this example I‚Äôm calling it ‚Äòcustom.css‚Äô. Include these lines (my image is called results.jpg):\nbody {\n  background-image: url(\"results.jpg\");\n  min-height: 500px;\n  /* Set background image to fixed (don't scroll along with the page) */\n  background-attachment: fixed;\n  background-position: right top;\n  /* Set the background image to no repeat */\n  background-repeat: no-repeat;\n  /* Scale the background image to be as large as possible */\n  background-size: cover;\n}\nThe image itself will need to be quite transparent. You can do that with imagemagick. On a mac terminal, to make the image 50% more transparent, the command is:\nconvert IMAGE -fill white -colorize 50% NEW_IMAGE\nAfter this, we just need to change our YAML at the top of the R Markdown document like so:\n---\ntitle: \"R Notebooks\"\noutput:\n  html_notebook:\n    css: custom.css\n---\nYou‚Äôll have something interesting like this:\n\nThis image could of course be anything, like a company logo, for example. You could also leave a large portion of it white to place the image in the corner or side of the screen."
  },
  {
    "objectID": "posts/tips-and-tricks-for-rmarkdown.html#two-columns",
    "href": "posts/tips-and-tricks-for-rmarkdown.html#two-columns",
    "title": "Tips and Tricks for R Markdown html",
    "section": "Two columns",
    "text": "Two columns\nAnother useful thing that we can do with css is create two columns, particularly useful in a presentation. In order to do that, add the following lines to the css file you‚Äôre using:\n#left {\n  left: -8.33%;\n  text-align: left;\n  float: left;\n  width: 50%;\n  z-index: -10;\n}\n\n#right {\n  left: 31.25%;\n  top: 75px;\n  float: right;\n  text-align: right;\n  z-index: -10;\n  width: 50%;\n}\nWhen you want to use these columns in your R Markdown document, use them like so, with a bit of html:\n&lt;div id=\"left\"&gt;\n  #content\n&lt;/div&gt;\n&lt;div id=\"right\"&gt;\n  #content\n&lt;/div&gt;\nFor example, this code:\n&lt;div id=\"left\"&gt;\n$$y_i \\backsim Normal(\\mu_i, \\sigma)$$\n$$\\mu_i = \\alpha + \\beta x_i$$\n$$\\sigma \\backsim Uniform(0, 1)$$\n$$\\beta \\backsim Normal(0, 10)$$\n$$\\alpha \\backsim Normal(0, 10)$$\n&lt;/div&gt;\n\n&lt;div id=\"right\"&gt;\n- `Likelihood`\n- `Linear model`\n- `sigma prior`\n- `beta prior`\n- `alpha prior`\n&lt;/div&gt;\nproduces this:\n\nLikewise, an external css file can be used to change defaults. I think the default R code snippet text size is a little small in the R Notebooks, as is all the text in an R Notebook if you‚Äôre using them for presentations. You can easily change the defaults by putting:\nbody {\n  /* Normal  */\n  font-size: 16px;\n}\ncode.r {\n  /* Code block */\n  font-size: 14px;\n}\nin the css file, which will make the normal text and the R code text bigger. If you want to make all the text slightly bigger in the entire document without an external css file, you can just put &lt;font&gt; tags at the start and end of the R Markdown document:\n&lt;font size=\"6\"&gt;\n  # content\n&lt;/font&gt;\nSimilarly, bits of html can come in handy when you want to change little elements of the document. &lt;br&gt; will give you a vertical space, and using &lt;bdi&gt; tags can be useful for changing the style of particular words, especially useful for words that are presented as ‚Äòcode‚Äô in back ticks. For example:\n&lt;bdi style=\"color:#1E90FF\"&gt;`variable_1`&lt;/bdi&gt;.\n\n&lt;bdi style=\"font-size:70%;\"&gt;(**`?dplyr::select`**)&lt;/bdi&gt;\nwill make ‚Äòvariable_1‚Äô appear in code-style text and be blue (variable_1), whereas ‚Äò?dplyr::select‚Äô will be bold and 70% of the size that it would otherwise be. ((?dplyr::select))"
  },
  {
    "objectID": "posts/tips-and-tricks-for-rmarkdown.html#revealjs-logos",
    "href": "posts/tips-and-tricks-for-rmarkdown.html#revealjs-logos",
    "title": "Tips and Tricks for R Markdown html",
    "section": "revealjs logos",
    "text": "revealjs logos\nThe image of the two columns above was part of a revealjs presentation done in R Markdown. revealjs gives you really slick options for presentations, but can be a lot of extra work in terms of customizing the output. Logos, for example, which are so common (and necessary) in presentations, are not included by default and can be tricky and annoying to include.\nWe can get what we want with a little html and some YAML options. In the example below, I used an external html file called Logo_prefix.html, which is referenced in the includes: section of the YAML header. The html file contains the following, which uses an image called logo.png. The size can be adjusted using the style=\"\" part below.\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n &lt;div class=\"bottombar\"&gt;\n    &lt;h1 class=\"title\"&gt;&lt;/h1&gt;&lt;image class=\"logo\" src=\"logo.png\" style=\"width:180px; height:85px;\"&gt;\n&lt;/div&gt;\n&lt;/html&gt;\nAnd the RMarkdown YAML I used (for a course in statistics in R in Portuguese; the Sys.Date() is a nice trick to put the current date on the presentation):\n---\ntitle: \"Introdu√ß√£o a estat√≠stica no `R`, Dia 2\"\nauthor: \"Robert McDonnell\"\ndate: \"`r Sys.Date()`\"\noutput:\n  revealjs::revealjs_presentation:\n    theme: sky\n    highlight: pygments\n    css: custom.css\n    includes:\n      before_body: Logo_prefix.html\n---\nThis includes a small logo in the upper right corner of every slide. To get this to work properly, you will need to dig into the css of the particular revealjs theme that you are using (here I‚Äôm using sky), I can‚Äôt even remember exactly how I did that‚Ä¶ like I said, customizing revealjs can be annoying."
  },
  {
    "objectID": "posts/renv.html",
    "href": "posts/renv.html",
    "title": "Taking RStudio‚Äôs renv for a spin",
    "section": "",
    "text": "I‚Äôve been working on a project recently where we‚Äôve been building a data analysis pipeline that involves bits of R code and bits of Python. Since the whole thing runs on Docker, on a secured server with no internet access, it‚Äôs been illuminating seeing the different ways that Python and R deal with packages/libraries and their dependencies. For manual documentation, I found pip show really handy, and so I wrote a little R package that does the same thing (you can see it here: www.github.com/RobertMyles/showpackage).\nAs luck would have it, I noticed that RStudio is just about to release renv onto CRAN. While I‚Äôve used some of the package managers available for R, I never really got into the habit of using them ‚Äì I suppose I found them underwhelming or too much hassle. On the other hand, renv reminds me of Node‚Äôs package management in JavaScript projects ‚Äì to the user, just a lockfile detailing what packages are used and what versions, and very easy to maintain. renv is designed this way and I can really see myself using it regularly."
  },
  {
    "objectID": "posts/renv.html#so-how-does-renv-work",
    "href": "posts/renv.html#so-how-does-renv-work",
    "title": "Taking RStudio‚Äôs renv for a spin",
    "section": "So how does renv work?",
    "text": "So how does renv work?\nBasically, you can use the R-projects style that is already easy to do from RStudio. Either create a new project or associate a new project with an existing folder. Imagine you have a project that will read data from a database, calculate some summaries and produce plots, all for an automated report. To do something like this, you‚Äôll probably use some tidyverse packages like dplyr and ggplot2, some document-making packages like knitr and rmarkdown, and maybe something like implyr and DBI/odbc for getting data. Maybe this will be a package for private use, and you may have devtools, tinytest/testthat and covr installed.\nWell, first you‚Äôll obviously need to install renv. This is easy to do with remotes::install_github(\"rstudio/renv\") (since it‚Äôs not on CRAN yet). Once you‚Äôve made an R-project from RStudio (File &gt; New Project &gt; etc.), you can simply type renv::init(). This will set up the infrastructure that renv will use to keep track of the packages you use, and create a private library for these. Depending on what you‚Äôre already using in this project (maybe nothing apart from base R), renv will install the packages it sees in your .R files into this private library.\nRunning renv::init() will show you something like this in your R console:\n\n&gt; renv::init()\n* Discovering package dependencies ... Done!\n* Copying packages into the cache ... * Querying repositories for available source packages ... Done!\n\nThe following package(s) will be added to the lockfile:\n                _           \n  askpass         [1.1]     \n  assertthat      [0.2.1]   \n  backports       [1.1.4]   \n  BH              [1.69.0-1]\n  brew            [1.0-6]   \n  callr           [3.3.1]   \n  checkmate       [1.9.4]   \n  cli             [1.1.0] \n  \n  ...\n  \n* Lockfile written to '~/renv_proj/renv.lock'.\n* Project '~/renv_proj' loaded. [renv 0.6.0-113]\n\nRestarting R session...\n\n* Project '~/renv_proj' loaded. [renv 0.6.0-113]\n\nThe packages that have been installed are recorded in a lockfile, which is a JSON file that looks like this (depending on what you‚Äôve installed):\n\n{\n  \"renv\": {\n    \"Version\": \"0.6.0-113\"\n  },\n  \"R\": {\n    \"Version\": \"3.6.0\",\n    \"Repositories\": [\n      {\n        \"Name\": \"CRAN\",\n        \"URL\": \"https://cran.rstudio.com\"\n      }\n    ]\n  },\n  \"Packages\": {\n    \"BH\": {\n      \"Package\": \"BH\",\n      \"Version\": \"1.69.0-1\",\n      \"Source\": \"CRAN\",\n      \"Hash\": \"0fde015f5153e51df44981da0767f522\"\n    },\n    \"R6\": {\n      \"Package\": \"R6\",\n      \"Version\": \"2.4.0\",\n      \"Source\": \"CRAN\",\n      \"Hash\": \"92b50d943a7c76c67918c1e1beb68627\"\n    }\n    ...\n\nAny time you update a package or install a new one, you can call renv::snapshot() to record the current state of packages to the lockfile (this can be done automatically with options(renv.config.auto.snapshot = TRUE)). If you need a previous version of the lockfile, use renv::restore(), which by default will choose the previous snapshot.\nHere‚Äôs an example of the console printout from installing knitr with renv:\n\n&gt; install.packages(\"knitr\")\n* Querying repositories for available source packages ... Done!\n* Querying repositories for available binary packages ... Done!\nRetrieving 'https://cran.rstudio.com/bin/macosx/el-capitan/contrib/3.6/knitr_1.24.tgz' ...\n    OK [downloaded 1.3 Mb in 2.3 secs]\nRetrieving 'https://cran.rstudio.com/bin/macosx/el-capitan/contrib/3.6/highr_0.8.tgz' ...\n    OK [downloaded 40.2 Kb in 1.3 secs]\nRetrieving 'https://cran.rstudio.com/bin/macosx/el-capitan/contrib/3.6/markdown_1.1.tgz' ...\n    OK [downloaded 195.2 Kb in 1.6 secs]\nRetrieving 'https://cran.rstudio.com/bin/macosx/el-capitan/contrib/3.6/xfun_0.8.tgz' ...\n    OK [downloaded 165 Kb in 2 secs]\nInstalling highr [0.8] from CRAN ...\n    OK (installed binary)\nInstalling xfun [0.8] from CRAN ...\n    OK (installed binary)\nInstalling markdown [1.1] from CRAN ...\n    OK (installed binary)\nInstalling knitr [1.24] from CRAN ...\n    OK (installed binary)\n* Lockfile written to '~/Documents/renv_proj/renv.lock'.\n\nIt‚Äôs pretty easy to see package dependencies with renv. We can see what dependencies come from our .R files, for example, which is a nice feature:\n\nlibrary(dplyr)\nlibrary(stringr)\n\nrenv::dependencies(\".\") %&gt;% filter(str_detect(Source, \"\\\\.R\")) %&gt;% pull(Package)\n# Finding R package dependencies ... Done!\n#  [1] \"data.table\"    \"dplyr\"         \"utils\"         \"datavalidator\" \"dplyr\"         \"lubridate\"     \"readr\"        \n#  [8] \"strex\"         \"datavalidator\" \"testthat\" \n\nAll in all, renv is a nice, well-designed package that is easy to use. I‚Äôve already used starting using it, and once it gets to CRAN, I‚Äôll be using it ‚Äòin production‚Äô. Nice job, RStudio!"
  },
  {
    "objectID": "posts/rmarkdown-conditional.html",
    "href": "posts/rmarkdown-conditional.html",
    "title": "Render RMarkdown Code Chunks Based on Output Document Type",
    "section": "",
    "text": "RMarkdown users ‚Äì did you know you can render code chunks based on the type of output you want to produce? It‚Äôs even easy-peasy ü§ì\nknitr makes available some parameters that you can access with opts_knit$get(). The one we want is ‚Äúrmarkdown.pandoc.to‚Äù. Once you have that, you can conditionally render code chunks with the eval argument of the code chunk.\nFor example, let‚Äôs say you were making a series of reports. You‚Äôd like to have some detail in case it was needed, but it would be nice to leave things simple in case that was needed too. In this case, you could make a MS Word document for the detail and a PowerPoint for the simpler presentation. Thinking back to R code, this might mean you include text and plots in the Word file and only the plots in the PowerPoint. So you‚Äôre knitting the same RMarkdown documents, but producing a Word and a PowerPoint version of the output, which you could do by running render() twice, once for each output type.\nThen in one of your code chunks, get the values we need:\nppt_true &lt;- opts_knit$get(\"rmarkdown.pandoc.to\") == \"pptx\"\ndocx_true &lt;- opts_knit$get(\"rmarkdown.pandoc.to\") == \"docx\"\nSince these will be TRUE or FALSE depending on which output you‚Äôre rendering to, we can include these in the code chunk like so (you‚Äôll need three backticks before the {):\n  {r eval = docx_true}\nAnd whatever you put in there will render only in the Word document. If this was text, for example, you could use knitr‚Äôs as_output():\n    {r eval = docx_true}\n    asis_output(\"This report shows lots of interesting things that I bet you never knew were possible.\")\n    \nWhereas your PowerPoint won‚Äôt have any of this text. E-Z P-Z. knitr is awesome."
  },
  {
    "objectID": "posts/web-navigation-in-r-with-rselenium.html",
    "href": "posts/web-navigation-in-r-with-rselenium.html",
    "title": "Web Navigation in R with RSelenium",
    "section": "",
    "text": "It goes almost without saying that the internet itself is the richest database available to us. From a 2014 blog post, it was claimed that every minute :\nRegardless of the accuracy of these claims, it is obvious to everyone that there is tons of information on the web. For researchers, then, the question is: how can you access all this information? You can of course go to specific, dedicated databases and download what you‚Äôre looking for, for example from the World Bank databank. However, there are drawbacks to this approach. It can become tiresome when you need to collect lots of data on different items (the World Bank databank is well organised, but not all databases are like that‚Ä¶to put it politely). Some only let you download small, specific sections of a bigger database, meaning you have to return time and time again to the starting page to enter new information in order to retrieve the data you want. (Another thing is that we‚Äôre not quite utilising the web itself as the database either.)\nTo deal with the first problem, you can automate the search process by driving a web browser with R.1 This is different from ‚Äòweb-scraping‚Äô. Web-scraping takes the webpage as a html document and allows you to read information from it. It‚Äôs quite a straightforward process, with plenty of R packages around to help you do it. rvest in particular is quite easy, although I‚Äôve found the XML package to be more powerful. (Web-scraping deals with the second issue above, in that it does treat the web itself as a database.)\nTo drive a web browser in R, there are two packages (that I‚Äôm aware of) that can be used. One is RSelenium by John Harrison, and Rwebdriver by Christian Rubba. I prefer RSelenium and so I‚Äôll use this package in the examples below.\nIf you don‚Äôt have it already installed, you‚Äôll need to download this package and load it into R.\ninstall.packages(\"RSelenium\")\nlibrary(\"RSelenium\")\nYou will also need to download the Selenium standalone server. You can get it from here. Opening this file automatically from RSelenium can be problematic2, and so I‚Äôve found the most straightforward way is to manually click on it and open it that way before you start.\nTo get started with RSelenium, you‚Äôll need to give your browser somewhere to go. For this example, I‚Äôm going to go to the funding management section of Brazilian National Health Service, the Fundo Nacional de Sa√∫de. From here, I‚Äôm going to get data for every municipality in every state over a period of some years. To do this manually would be a serious headache and would most likely lead to me making errors by forgetting where I am, which state is next, what municipality I just downloaded, and so on. Actually, you can be guaranteed I‚Äôd make those mistakes.\nURL &lt;- \"https://www.fns.saude.gov.br/indexExterno.jsf\"\n#checkForServer(dir=\"[DIRECTORY WHERE THE SELENIUM SERVER IS]\", update=FALSE)\n#checkForServer(dir=\"[DIRECTORY WHERE THE SELENIUM SERVER IS]\", update=TRUE) # if you want to update\n#startServer(dir=\"[DIRECTORY WHERE THE SELENIUM SERVER IS]\") #none of these three are necessary if you click on the server first and manually open it.\n\nfprof &lt;- makeFirefoxProfile(list(browser.download.dir = \"[DOWNLOAD DIRECTORY]\",\nbrowser.download.folderList = 2L,\nbrowser.download.manager.showWhenStarting=FALSE,\nbrowser.helperApps.neverAsk.saveToDisk = \"application/octet-stream\"))\n\nremDr &lt;- remoteDriver(extraCapabilities=fprof)\nremDr$open()\nSo now your browser should be open. Here I‚Äôve used a profile for Firefox because I will download files and I don‚Äôt want to deal with the download window that pops up in Firefox (you need to enter your download folder where it says ‚Äò[DOWNLOAD DIRECTORY]‚Äô, by the way. And you can also run RSelenium on Chrome and other browsers, and even use a headless browser which speeds things up.) If you didn‚Äôt need to deal with download boxes and pop-ups and the like, you only need remDr &lt;- remoteDriver$new(), which will automatically open up a Firefox browser window. These particular files were recognised by Firefox as being binary files, and so I have disabled the download box for files of the type ‚Äúapplication/octet-stream‚Äù. Other file types need a different setting.\nThis website has a drop-down box on the left hand side that we‚Äôre going to use. What we will input into this is, in turn, a list of years, states, and municipalities. After that we will click on ‚ÄúConsultar‚Äù (for those of you who don‚Äôt speak Portuguese, I‚Äôm quite sure you can figure out what that means). Clicking this will bring us to a new page, from which we can download the data we‚Äôre looking for in a .csv file.\nSo let‚Äôs create our inputs:\nInputYear &lt;- list(\"2016\", \"2015\", \"2014\", \"2013\", \"2012\", \"2011\", \"2010\", \"2009\")\n\nInput &lt;- list(\"ACRE\", \"ALAGOAS\", \"AMAPA\", \"AMAZONAS\", \"BAHIA\", \"CEARA\", \"DISTRITO FEDERAL\", \"ESPIRITO SANTO\", \"GOIAS\", \"MARANHAO\", \"MATO GROSSO\", \"MATO GROSSO DO SUL\", \"MINAS GERAIS\", \"PARA\", \"PARAIBA\", \"PARANA\", \"PERNAMBUCO\", \"PIAUI\", \"RIO DO JANEIRO\", \"RIO GRANDE DO NORTE\", \"RIO GRANDE DO SUL\", \"RONDONIA\", \"RORAIMA\", \"SANTA CATARINA\", \"SAO PAULO\", \"SERGIPE\", \"TOCANTINS\")\n\nInput_Mun &lt;- \"TODOS DA UF\" #this will select all municipalities\nIn order to get all this done, I will use a for loop in R which will first loop over the years, and then states, thereby selecting all states in a given year. In the following code, you will see RSelenium commands that are quite different to regular commands in R. First of all, RSelenium operates by way of two environments: one is remoteDriver environment, the other a webElement environment. These have specific options available to them (see the help section on each for a list and explanations). Some of the most useful are findElement() (an option of remoteDriver), sendKeystoElement() and clickElement() (both options of webElement, as remDr$findElement returns an object of webElement class). We will use these to navigate around the page and click on specific elements.\nSpeaking of elements on a page, this is actually the most crucial part of the process to get right (and can be the most frustrating). Some have recommended selectorgadget, but finding elements can be done in Firefox or Chrome without selectorgadget ‚Äì you just right-click the element in question and select ‚ÄúInspect‚Äù or ‚ÄúInspect Element‚Äù. This will bring up a chaotic-looking panel, full of html, css and javascript code. Luckily, there are easy options in Firefox and Chrome for finding what we need. After you right-click the element that you want (the one you would have clicked if you were navigating the page manually), click ‚ÄúInspect‚Äù and then this element of the html code will be highlighted. Right-click on this again and you will see the option to copy. In Chrome, you will have the option to copy the xpath or css selector (‚Äúselector‚Äù); in Firefox you can copy the css selector (‚Äúunique selector‚Äù). I have used other options below to give more examples, such as ‚Äòid‚Äô. This can be copied directly from the html code, and ‚Äòclass‚Äô and ‚Äòname‚Äô can be used in a similar fashion. In general, css selectors are the easiest to work with.\nA quick note on some other aspects of the code. Sys.sleep is used in order to be nice‚Äì you don‚Äôt want to bombard the website with all of your requests in rapid-fire fashion; after all, they may block you. So this spaces out our commands. This is also useful for when you may have to wait for an element to load on the page before you can click on it. I have used paste() in order to include the loop counters in the css selector‚Äì just a little trick to make things easier. Some elements have \\\\ in the code: this is because the original had a single backslash, which is an escape character in R, and so the string is unreadable. Hence the added backslash. You will also see the use of try() ‚Äì in this case, there is a state that does not load like the others (the Federal District) and so this automated process will not work here. try() allows R to try anyway, and if it fails, the loop just continues to the next iteration.\nfor(i in 1:length(InputYear)){\n    for(j in 1:length(Input)){\n    remDr$navigate(URL)\n    #Year:\n    webElem &lt;- remDr$findElement(using = \"id\", value = \"formIndex:j_idt48\")\n    webElem$clickElement() #click on the drop-down year box\n    Sys.sleep(2)\n    webElem &lt;- remDr$findElement(using = \"id\", value=\"formIndex:j_idt48_input\")\n    Sys.sleep(2)\n    webElem$sendKeysToElement(InputYear[i]) #send the year to the box\n    webElem &lt;- remDr$findElement(using = \"css\", value=\"li.ui-state-active\")\n    webElem$clickElement() #click on the active element (the year we sent)\n    Sys.sleep(2)\n    #State:\n    webElem &lt;- remDr$findElement(using = \"id\", value = \"formIndex:sgUf\")\n    webElem$clickElement()\n    Sys.sleep(2)\n    webElem$sendKeysToElement(Input[j]) #enter the state into the drop-down box\n    CSS &lt;- paste(\"#formIndex\\\\3a sgUf_panel &gt; div &gt; ul &gt; li:nth-child(\", j+2, \")\", sep=\"\")\n    webElem &lt;- remDr$findElement(using = \"css\", value = CSS)\n    Sys.sleep(1)\n    webElem$clickElement()\n    Sys.sleep(3)\n    #Municipality:\n    webElem &lt;- remDr$findElement(using = 'id', value = 'formIndex:cbMunicipio')\n    webElem$clickElement()\n    Sys.sleep(2)\n    webElem &lt;- remDr$findElement(using = 'css', value='#formIndex\\\\3a cbMunicipio_panel &gt; div &gt; ul &gt; li:nth-child(2)')\n    webElem$sendKeysToElement(list(Input_Mun))\n    webElem$clickElement()\n    Sys.sleep(4)\n    #\"Consultar\":\n    webElem &lt;- remDr$findElement(using = 'xpath', value = '//*[@id=\"formIndex:j_idt60\"]')\n    Sys.sleep(2)\n    webElem$clickElement()\n    Sys.sleep(6)\n    #Download the .csv:\n    webElem &lt;- try(remDr$findElement(using = 'xpath', value = '//*[@id=\"formIndex\"]/div[4]/input'), silent=T)\n    try(webElem$clickElement(), silent=T)\n    Sys.sleep(3)\n}}\nSo after all this, we‚Äôll have a bunch of .csv files in out download folder, that you can import into R and mess around with. To load them all in together, you could use the following code:\nlibrary(\"readr\")\nsetwd(\"[THE DOWNLOAD FOLDER YOU USED]\")\nfileNames &lt;- list.files(path = getwd(), pattern = \"*.csv\")\ndata &lt;- rbindlist(lapply(fileNames, read_csv2,\ncol_names=c(\"Ano\", \"UF_MUNICIPIO\", \"IBGE\", \"ENTIDADE\", \"CPF_CNPJ\",\n\"Bloco\", \"Componente\", \"Acao_Servico_Estrategia\", \"Competencia_Parcela\",\n\"No_OB\", \"Data_OB\", \"Banco_OB\", \"Agencia_OB\", \"Conta_OB\", \"Valor_Total\",\n\"Desconto\", \"Valor_Liquido\", \"Observacao\", \"Processo\", \"Tipo Repasse\",\n\"No_Proposta\"), skip = 1, locale=locale(decimal_mark=\",\", grouping_mark=\".\")))\nAnd there you go, all the data you wanted scraped automatically from the web. In this example, we were downloading a file, but you could be navigating around in order to arrive at a certain page and then to scrape the contents of that page. You can do that in a number of ways, by combining RSelenium and other packages, such as XML and rvest. For a solution using only RSelenium, we can first create an empty dataframe and then fill it with the getElementText() option of the webElement class. So, for example, I was getting vote proposal content from the Brazilian Senate. I used RSelenium to navigate to the pages that I wanted, as is shown above, and then I stored the Content and the Index of the vote (which were stored on the page as html text elements) as entries in the Index dataframe, using webElem$getElementText(). Afterwards, I used various combinations of stringr package functions and gsub to clean up the text.\nIndex &lt;- data.frame(Content=NA, Index=NA)\nIndex[i,1] &lt;- webElem$getElementText()\n...\nIndex[i,2] &lt;- webElem$getElementText()\nYou can also get the html and parse it using XML:\nelemtxt &lt;- webElem$getElementAttribute(\"outerHTML\")\nelemxml &lt;- htmlTreeParse(elemtxt, asText=TRUE, encoding=\"UTF-8\", useInternalNodes=TRUE)\nText &lt;- html_text(elemxml, trim=TRUE)\nAnd then you have the text from the webpage stored as data in R. Magic! ü§ü"
  },
  {
    "objectID": "posts/web-navigation-in-r-with-rselenium.html#footnotes",
    "href": "posts/web-navigation-in-r-with-rselenium.html#footnotes",
    "title": "Web Navigation in R with RSelenium",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is often argued that R is not the best for this application, with Python often offered as a better alternative. In my experience, I‚Äôve found R to be pretty good for this sort of thing, with delays being caused more by the browser/net speed than R. The scripts can be ugly, but using Selenium in Python looks pretty similar anyway. This question on Stack Overflow gives some instructions.‚Ü©Ô∏é\nSee this discussion.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/easy-web-scrape.html",
    "href": "posts/easy-web-scrape.html",
    "title": "Easier web scraping in R",
    "section": "",
    "text": "In an earlier post, I described some ways in which you can interact with a web browser using R and RSelenium. This is ideal when you need to access data through drop-down menus and search bars. However, working with RSelenium can be tricky. There are, of course, easier ways to get information from the internet using R.\nPerhaps the most straightforward way is to use rvest, in tandem with other packages of the Hadleyverse1, such as dplyr and tidyr for data preparation and cleaning after the webscrape. I‚Äôm going to use a simple example that I came across recently in my work, getting the name of each mayor in Brazil.\nFinding out who was elected to the mayor‚Äôs office in each municipality in Brazil is easy: that data exists and is available on the website of the Tribunal Superior Eleitoral. However, just because someone was elected to office (in this case in 2014) does not mean that they are still in office now, two years later. After searching around the web for a bit, I realised that this data is not available as a dataset.\nAfter wandering to the website of the IBGE, a Brazilian statistics agency, I found a way to get the name of the mayor currently in charge of each municipality. Each municipality has its own webpage on the IGBE‚Äôs dedicated Cidades@ site.\nFor example, you will see the a webpage for the municipality of Acrel√¢ndia, shown in the image below. As you can see, the name of the mayor (‚ÄúPrefeito‚Äù) is on the right-hand side of the page. Since we now know we can get this for each municipality, we have three tasks to do in order to get this info into R:\nThe url for Acrel√¢ndia is unique at: ‚Äúcodmun=120001‚Äù and ‚Äúsearch=acre|acrelandia‚Äù.\nThe number in ‚Äúcodmun‚Äù is available as the IBGE municipal code (although missing the final digit, strangely‚Ä¶but that‚Äôs not a problem, we just take it off the end for each one) and the rest is just the state and the municipality, all information that is easy to get from various sources. For this example, I‚Äôve uploaded this basic dataset to Github so we can use it here.\nIn the code snippet above, we‚Äôve taken out unnecessary columns, renamed one, changed the names of the municipalities to lower case (for the url), taken six numbers of the IBGE code for use in the webscrape and joined the state and municipality names together, with | seperating them, as in the url for each municipality webpage. We also need to create some empty data frames to fill, and remove the municipality of Bras√≠lia, which does not have a Prefeito, just a governor, which is all done below:\nNext comes our webscrape, which is incredibly easy with rvest (xml2 is likewise easy). The only hard part of this entire scrape is getting the words ‚ÄúPrefeito‚Äù along with the name of the mayor out of the document. This relies on regex, which can be tricky. But trial and error should lead you to the right answer for whatever you need. Or search Google, of course.\nWith a little tidying, we have a nice little dataset of each current mayors for each municipality in Brazil."
  },
  {
    "objectID": "posts/easy-web-scrape.html#footnotes",
    "href": "posts/easy-web-scrape.html#footnotes",
    "title": "Easier web scraping in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSupposedly, Hadley Wickham doesn‚Äôt actually like this term, but I‚Äôll use it anyway, I‚Äôm sure he wouldn‚Äôt mind üôÇ.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/betterDataViz.html",
    "href": "posts/betterDataViz.html",
    "title": "Improving your DataViz",
    "section": "",
    "text": "import SimpleBarChart from ‚Äú../graphs/SimpleBarChart.js‚Äù import data from ‚Äú../data/divorceTurnout2019.json‚Äù\nA while ago, I posted about including D3 charts in MDX documents. I was pretty chuffed at my little D3-React bar chart (with tooltips!), but that was mainly to do with me being technically able to do it, not because it was a beautiful piece of data visualization (it isn‚Äôt). In $DAYJOB, I‚Äôve been thinking more about dataviz and the science behind it, and along with a talented colleague of mine, I‚Äôm preparing a data visualization guide for our data science team. You‚Äôd think Data Scientists would be pretty well clued-in on dataviz, but that‚Äôs not always the case. I thought I‚Äôd revisit my own horrific dataviz crime and go through a step-by-step process of ways we could improve it. Good data visualization is always about the message you want to convey ‚Äì what interests me in these data is the overwhelming Yes vote, so that‚Äôs what I‚Äôll be concentrating on highlighting. It‚Äôs also a nice opportunity to highlight mistakes often made, and that I often make when making dataviz without thinking too deeply about it.1 I suppose one challenge we face here is that there is no external comparison ‚Äì the strong ‚ÄòYes‚Äô vote is not being compared to any previous referenda. We‚Äôll change that later on.\nTo remind you of what we‚Äôre talking about, here‚Äôs the chart I originally posted:"
  },
  {
    "objectID": "posts/betterDataViz.html#including-comparison-data",
    "href": "posts/betterDataViz.html#including-comparison-data",
    "title": "Improving your DataViz",
    "section": "Including Comparison Data",
    "text": "Including Comparison Data\nSince the main message I‚Äôm trying to get at is the strong ‚ÄòYes‚Äô vote, perhaps one way to do that would be to include the previous referendum on divorce in Ireland, which happened in 1995 and made it legal. (Not exactly the same question, but relevant, I think.) I‚Äôve scraped this dataset from Elections Ireland, and the code is at the bottom of this post. Including these data opens up a new possibility for plotting ‚Äì ‚Äòdumbbells‚Äô. Basically we want to show the change between red and blue below, red being the earlier vote from 1995:\n\ndiv1995 &lt;- read_csv(\"https://raw.githubusercontent.com/RobertMyles/blogdata/master/divorce1995.csv\")\n\ndivorce &lt;- full_join(df, div1995) %&gt;% select(-c(Dublin, y_axis_colour))\n\ndivorce %&gt;% \n  mutate(Constituency = forcats::fct_reorder(Constituency, Turnout)) %&gt;% \n  ggplot(aes(y = Constituency)) + \n  geom_segment(aes( yend = Constituency, x = 50, xend = Turnout),\n               colour = \"cornflowerblue\") +\n  geom_segment(aes(yend = Constituency, x = 50, xend = Turnout1995),\n               colour = \"red\") +\n    geom_point(aes(x = Turnout), colour = \"cornflowerblue\") +\n  geom_point(aes(x = Turnout1995), colour = \"red\") +\n  xlim(50, 75) +\n  labs(y = NULL, x = \"Turnout (%)\") +\n  theme_minimal()\n\nWe can do that in a few ways, one of which is the dumbbell plot. The ggalt library for R has a nice dumbbell geom which saves us some code, so I‚Äôll use that. The main idea behind a dumbbell plot here is that the change between 1995 and 2019 can be highlighted. While we‚Äôre at it, let‚Äôs tidy up the plot in general and make it more presentable. To do this, we‚Äôll lighten some of the text, make the y-axis text smaller so there‚Äôs no overlap, add some annotations to explain what‚Äôs happening, as well as better titles and a source citation for the data and expand the size a bit.\n\nlibrary(ggalt)\n\npointer &lt;- arrow(length = unit(5, \"pt\"), type = \"closed\")\nbl &lt;- \"#08415C\"\nrd &lt;- \"#CC2936\"\n\ndivorce %&gt;% \n  mutate(Constituency = forcats::fct_reorder(Constituency, Turnout)) %&gt;%\n  ggplot(aes(y = Constituency)) + \n  geom_dumbbell(aes(x = Turnout1995, xend = Turnout), colour = \"grey63\",\n                colour_x = rd, colour_xend = bl) +\n  theme_minimal() +\n  xlim(50, 75) +\n  geom_curve(\n    data = data.frame(), \n    aes(x = 56, y = 21, xend = 58.5, yend = 18), \n    colour = alpha(rd, 0.6), arrow = pointer\n  ) +\n  annotate(\"text\", label = \"Red points show \\nthe 1995 Divorce \\nReferendum turnout\",\n           x = 55, y = 25, colour = alpha(rd, 0.9), size = 3.5) +\n  geom_curve(\n    data = data.frame(), \n    aes(x = 69, y = 14, xend = 63.75, yend = 18), \n    colour = alpha(bl, 0.6), arrow = pointer) +\n  annotate(\"text\", label = \"Blue points show \\nthe 2019 Divorce \\nReferendum turnout\",\n           x = 70, y = 10, colour = alpha(bl, 0.9), size = 3.5) +\n  labs(title = \"Turnout 1995 v 2019\", subtitle = \"Irish Divorce Referenda, 1995 and 2019\",\n       x = \"Turnout in percentage points\", y = NULL, caption = \"Source: www.electionsireland.org\") +\n  theme(panel.grid.major = element_line(linetype = 2, colour = \"grey92\"),\n        axis.text.y = element_text(colour = \"grey21\", size = 8),\n        plot.subtitle = element_text(colour = \"grey21\"),\n        plot.caption = element_text(colour = \"grey21\")) \n\nThat‚Äôs not bad at all. We could highlight some specific constituencies (Wicklow, perhaps) and maybe remove the x-axis title and put ‚Äò%‚Äô beside the axis text numbers, but even without these changes it‚Äôs a big improvement. Another way we could show this information is with a centred bar chart, which goes either left or right depending on the change. We‚Äôll need to add another column to our data, but it‚Äôs trivial to do.\n\ndivorce %&gt;% \n  mutate(\n    difference = Turnout - Turnout1995,\n    is_positive = difference &gt; 0,\n    Constituency = forcats::fct_reorder(Constituency, difference)\n    ) %&gt;% \n  ggplot(aes(y = difference, x = Constituency, fill = is_positive, colour = is_positive)) +\n  geom_col() +\n  coord_flip() +\n  scale_color_manual(values = c(rd, bl)) +\n  scale_fill_manual(values = c(alpha(rd, 0.9), alpha(bl, 0.9))) +\n  theme_minimal() +\n  labs(title = \"Difference in turnout 1995 & 2019\", subtitle = \"Irish Divorce Referenda\",\n       y = \"Difference in turnout percentage\", x = NULL, caption = \"Source: www.electionsireland.org\") +\n  theme(panel.grid.major = element_line(linetype = 2, colour = \"grey92\"),\n        axis.text.y = element_text(colour = \"grey21\"),\n        plot.subtitle = element_text(colour = \"grey21\"),\n        plot.caption = element_text(colour = \"grey21\"),\n        legend.position = \"none\") \n\nLie our earlier plot, we might consider annotating this one to provide more information. A secondary y-axis might be nice to help readers connect the constituency to its difference in turnout. Other options are changing things like fonts and colours (and checking that these work for colour-blind people), but for now I think we‚Äôve done a pretty good job of taking a wall of bars and teasing apart the interesting stories behind it. We tried various different types of plot and shown the importance of being able to compare a story to something else. And we‚Äôve made some nice data viz!"
  },
  {
    "objectID": "posts/betterDataViz.html#scraping-the-1995-referendum-data",
    "href": "posts/betterDataViz.html#scraping-the-1995-referendum-data",
    "title": "Improving your DataViz",
    "section": "Scraping the 1995 Referendum Data",
    "text": "Scraping the 1995 Referendum Data\nWe can get the 1995 Divorce referendum data from the Elections Ireland website (thanks, Elections Ireland!). We need to match up some of the Constituency names that have changed, which is straightforward:\n\nlibrary(rvest); library(dplyr); library(stringr)\n\nurl &lt;- \"https://electionsireland.org/results/referendum/refresult.cfm?ref=1995R\"\n\nninety5 &lt;- read_html(url) %&gt;% \n  html_node(\"table.conlist\") %&gt;% \n  html_table(fill = TRUE) %&gt;% \n  slice(-c(1:3, 4:5)) %&gt;% \n  select(1, 7) %&gt;% \n  magrittr::set_colnames(value = c(\"Constituency\", \"Turnout\")) %&gt;% \n  slice(1:41) %&gt;% \n  mutate(Turnout = str_remove(Turnout, \"%\"),\n         Turnout = as.numeric(Turnout),\n         Constituency = case_when(\n         Constituency == \"Carlow Kilkenny\" ~ \"Carlow-Kilkenny\",\n         Constituency == \"Cavan Monaghan\" ~ \"Cavan-Monaghan\",\n         Constituency == \"Cork North Central\" ~ \"Cork North-Central\",\n         Constituency == \"Cork North West\" ~ \"Cork North-West\",\n         Constituency == \"Cork South Central\" ~ \"Cork South-Central\",\n         Constituency == \"Cork South West\" ~ \"Cork South-West\",\n         Constituency == \"Dublin South East\" ~ \"Dublin Bay South\",\n         Constituency == \"Dublin South West\" ~ \"Dublin South-West\",\n         Constituency == \"Dublin South\" ~ \"Dublin Rathdown\",\n         Constituency == \"Dublin North\" ~ \"Dublin Fingal\",\n         Constituency == \"Dublin North West\" ~ \"Dublin North-West\",\n         Constituency == \"Dublin South Central\" ~ \"Dublin South-Central\",\n         Constituency == \"D√∫n Laoghaire\" ~ \"Dun Laoghaire\",\n         Constituency == \"Sligo Leitrim\" ~ \"Sligo-Leitrim\",\n         TRUE ~ Constituency\n         ))\n\nNow we face a slightly trickier problem ‚Äì since 1995, some new constituencies have been created and others have been abolished (see here. For those that have been amalgamated, we can simply take the mean of the two previous constituencies.\n\nget_mean &lt;- function(constituency) {\n  ninety5 %&gt;% \n    filter(str_detect(Constituency, constituency)) %&gt;% \n    pull(Turnout) %&gt;% mean() %&gt;% round(2) -&gt; mean_turnout\n  return(mean_turnout)\n}\n\ndub_north_mean &lt;- ninety5 %&gt;% \n  filter(Constituency %in% c(\"Dublin North East\", \"Dublin North Central\")) %&gt;% \n  pull(Turnout) %&gt;%  mean() %&gt;% round(2)\nlim_mean &lt;- ninety5 %&gt;% \n  filter(Constituency %in% c(\"Limerick East\", \"Limerick West\")) %&gt;% \n  pull(Turnout) %&gt;%  mean() %&gt;% round(2)\ndonegal_mean &lt;- get_mean(\"Donegal\")\ntipp_mean &lt;- get_mean(\"Tipperary\")\nkerry_mean &lt;- get_mean(\"Kerry\")\nmayo_mean &lt;- get_mean(\"Mayo\")\n\nninety5 &lt;- ninety5 %&gt;% \n  add_row(\n    Constituency = c(\"Dublin Bay North\", \"Donegal\", \"Tipperary\", \"Kerry\", \"Mayo\",\n                     \"Limerick City\", \"Limerick\"),\n    Turnout = c(dub_north_mean, donegal_mean, tipp_mean, kerry_mean, mayo_mean,\n                lim_mean, lim_mean)\n  ) %&gt;% \n  filter(!Constituency %in% c(\"Tipperary North\", \"Tipperary South\",\n                              \"Mayo East\", \"Mayo West\", \"Kerry North\",        \n                              \"Kerry South\", \"Donegal North East\",\n                              \"Donegal South West\",\"Dublin North East\",\n                              \"Dublin North Central\", \"Limerick East\",\n                              \"Limerick West\"))\n\nFor constituencies that have since been split into two, we can just use the value for the original constituency. For the Longford-Roscommon-Westmeath-Galway mess, I‚Äôm just going to take the mean of the constituencies that have joined. The turnout was pretty similar anyway, so we‚Äôre not changing much here.\n\n# split Laois Offaly & Meath & Kildare\nninety5 &lt;- ninety5 %&gt;% \n  add_row(\n    Constituency = c(\"Laois\", \"Offaly\", \"Meath East\", \"Meath West\",\n                     \"Kildare North\", \"Kildare South\", \"Dublin Mid-West\"),\n    Turnout = c(rep(63.02, 2), rep(59.58, 2), rep(61.72, 2), 62.37)\n  ) %&gt;% \n  filter(!Constituency %in% c(\"Kildare\", \"Meath\", \"Laoighis Offaly\"))\n\n\n# longford-roscommon-westmeath-galway\nninety5 &lt;- ninety5 %&gt;% \n  add_row(\n    Constituency = c(\"Longford-Westmeath\", \"Roscommon-Galway\"),\n    Turnout = c(60.07, 60.17)\n  ) %&gt;% \n  filter(!Constituency %in% c(\"Longford Roscommon\", \"Westmeath\"))\n\nGots our data! As usual, it can be found here."
  },
  {
    "objectID": "posts/betterDataViz.html#footnotes",
    "href": "posts/betterDataViz.html#footnotes",
    "title": "Improving your DataViz",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA good example is the final plot here. I really should have had the bright colours in the gradient as the low end of the turnout scale, not the high end. Ah well.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/powerbi.html",
    "href": "posts/powerbi.html",
    "title": "Customize Interactive R Visuals in Power BI",
    "section": "",
    "text": "Some of us, through no fault of our own, have to work with things like Power BI. While it‚Äôs a powerful application, it‚Äôs just a little‚Ä¶you know. For anybody who works with R, Python or JavaScript or anything like that, it just feels like closing the black box a bit, not to mention reproducibility problems. The good thing about Power BI is that you can hack at it a little, and we can get the majority of R‚Äôs graphing power into it. It‚Äôs actually pretty straightforward to do, the only tricky thing being when the plot doesn‚Äôt come out like you expected ‚Äì then you‚Äôll have to retrace some steps. But otherwise, all you need is some familiarity with the command line, and now that RStudio has a terminal built in, you‚Äôve got no excuse!"
  },
  {
    "objectID": "posts/powerbi.html#some-comments",
    "href": "posts/powerbi.html#some-comments",
    "title": "Customize Interactive R Visuals in Power BI",
    "section": "Some Comments",
    "text": "Some Comments\nThese visualizations can be a bit of work to debug ‚Äì if something goes wrong, you will need to:\nA - remove the visual and delete the imported custom visual from the sidebar\nB - correct the R code\nC - build the package again\nD - import the new visual into Power BI.\nThe above ‚Äòtemplate‚Äô approach works the first time, but if you repeat it you will have two SampleRHTMLVisual folders. To avoid this, just copy and rename the template folder we created. After you rename it, you will need to replace the name, displayName and guid variables in pbiviz.json. For example:\n\"name\": \"sampleRHTMLVisual\",\n\"displayName\": \"sampleRHTMLVisual\",\n\"guid\": \"sampleRHTMLVisualD10B081869514400A096DAB09C8B2634\"\nwere renamed to:\n\"name\": \"boxplot\",\n\"displayName\": \"Box Plot\",\n\"guid\": \"boxplotlD10B081869514400A096DAB09C8B2634\"\nYou will also have to rename the references to the SampleRHTMLVisual folder in the node_modules &gt; package.json file.\nHaving more than one of these visuals in your PowerBI workspace can also be confusing, as they will all have the same icon in the sidebar. You can change this by downloading an icon png file and resizing it to 20 x 20 pixels. Name this file icon.png and place it in the assets subfolder of your visual folder.\nThere are tutorials here on building custom visualizations, including for R, based on the Typescript superset of JavaScript. Happy PoweR Bi-ing!"
  },
  {
    "objectID": "posts/r-to-gatsby.html",
    "href": "posts/r-to-gatsby.html",
    "title": "From R to Gatsby",
    "section": "",
    "text": "I don‚Äôt use Gatsby or JavaScript for this blog anymore, but this post is here for posterity anyway.\nThis post details how I use Gatsby.js to blog about R stuff. My site is deployed by Netlify, which builds it after any merges into the master branch repo on GitHub. I use a little tool I wrote called writeMDX to help me out.\nThe workflow is quite simple: write something -&gt; commit, upload to GitHub -&gt; Netlify builds and deploys. This post is about when that ‚Äòwrite something‚Äô part is about R and written in RMarkdown. Let‚Äôs go! ü¶î\n\nIf you‚Äôre an R user and you have a blog, there‚Äôs a fair chance you use blogdown. I used to, it‚Äôs very handy. (I‚Äôm not suggesting ditching blogdown btw, I think it‚Äôs fantastic. If that works for you, great.) What if you want to continue to write about R, use RMarkdown etc., but build your site using something like Gatsby?\nWell, Gatsby itself is pretty easy to get started with, and I‚Äôve found the docs quite good and the community helpful (have a look here to see how helpful people can be). The nice thing is that you have a lot of control over how your website looks and behaves.\nGatsby also utilises Facebook‚Äôs GraphQL to source data. The ‚Äòdata‚Äô here are elements of your site ‚Äì making it really useful to render items, internal links and so on by querying the data of the site. (If that last sentence makes no sense, see here.) For example, the list of my blog posts found here is done dynamically by querying the data of the site using GraphQL.\nThe actual query for the ‚Äòblog list‚Äô page of my site looks like this:\nexport const query = graphql`\n  query {\n    allMdx(sort: { fields: [frontmatter___date], order: DESC }) {\n      totalCount\n      edges {\n        node {\n          id\n          frontmatter {\n            title\n            date\n            featuredImage {\n              childImageSharp {\n                fixed(width: 100, height: 100) {\n                  ...GatsbyImageSharpFixed\n                }\n              }\n            }\n          }\n          fields {\n            slug\n          }\n          excerpt(pruneLength: 280)\n        }\n      }\n    }\n  }\n`\nFor a data scientist, that‚Äôs quite interesting ‚Äì it‚Äôs like a SQL query to get all the MDX files, count them, extract some fields and an excerpt. Pretty cool, eh?\n\nSo the world of Gatsby websites is exciting and interesting, but what if you want to blog about R things still? The great thing about blogdown is that it fits so snugly into the RStudio ecosystem. So that‚Äôs just not going to be the same with Gatsby.\nHowever, there are some other benefits. First of all, I use VS Code to work on any non-RMarkdown stuff for the site. VS Code for R is not great in my opinion, at least it doesn‚Äôt compare to RStudio. But for JavaScript, it‚Äôs fantastic.\nSecondly, since there is such a ginormous JS/web developer community, there are lots of little tools to help with developing sites. For example, I use Merge Schedule to automatically merge Pull Requests into the master branch of my site (although not particularly a JS-specific tool). This allows me to schedule blog posts, as I simply create a branch, write whatever I‚Äôm going to post, and push to the branch on GitHub. Using schedule /&lt;YEAR&gt;-&lt;MONTH&gt;-&lt;DAY&gt; in the PR description schedules it for merging. I‚Äôve only been using this for a short while, but it‚Äôs working well. This helps me to blog when I‚Äôm feeling productive and not have to do it when I‚Äôm not, so I hope to have a more regular blog schedule now, in terms of when the post appear on the site. Another one is Dependabot, which updates my JS packages for me. (While I‚Äôm here, using NPM has highlighted the amazingness of CRAN ‚Äì often I‚Äôve been left with something breaking because of a package ‚Äì that doesn‚Äôt happen often with R. Goooo CRAN! ü•≥)\nThirdly, I use MDX for a flavour of Markdown that allows you to embed React components in the Markdown. This is quite similar to how you can run R code in RMarkdown. Where this gets useful is when you create a D3 chart as a React component. It can then be used whenever you like in your MDX-Markdown posts, by importing it at the top of the post, under the YAML header. I‚Äôve written about this before, it‚Äôs a nice way to do such things.\nSo ok, all nice JS-associated things, but what about blogging about R, especially if you‚Äôve got R code examples? Well, my workflow is quite simple:\n\nI open up a new RMarkdown document in RStudio;\nI write whatever I‚Äôm going to write;\nI use writeMDX to write the .Rmd file to .mdx format;\nI take the .mdx file and put it in the directory I‚Äôm using for blog posts and Gatsby renders it as part of the site.\n\nWhat‚Äôs writeMDX? It‚Äôs just a little helper I wrote to take RMarkdown and change it to MDX. If you have an RMarkdown document with the following YAML header:\n---\ntitle: \"MDXtest\"\nauthor: \"Robert McDonnell\"\ndate: \"2/29/2020\"\noutput: html_document\nfeaturedImage: \"image/png.png\"\n---\nwriteMDX will change it to:\n---\ntitle: \"MDXtest\ndate: \"2020-02-29\"\nfeaturedImage: \"images/some_image.png\"\n---\nwhich is the format I use for my blog posts. What appears here is configurable, so you can change these as you need.\nLikewise for the document body. If we have:\n## writeMDX test\n\nThis is a test document for writeMDX. It has **bold**, *italic*\n\n- lists\n  - sublists\n- and so on\n\nAnd it has code. For R:\n``{r}\nx &lt;- 5\nprint(x)\n\nprint(head(mtcars))\n``\nIt will become:\n\nThis is a test document for writeMDX. It has **bold**, _italic_\n\n- lists\n  - sublists\n- and so on\n\nAnd it has code. For R:\n\n::: {.cell hash='r-to-gatsby_cache/html/unnamed-chunk-1_a465b710693caa5479950dde66942685'}\n\n```{.r .cell-code}\nx &lt;- 5\nprint(x)\n```\n:::\n\n    ## [1] 5\n\n::: {.cell hash='r-to-gatsby_cache/html/unnamed-chunk-2_2ea7fa01b3f700e8d3af2c51758ceb3b'}\n\n```{.r .cell-code}\nprint(head(mtcars))\n```\n:::\n\n    ##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n    ## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n    ## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n    ## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n    ## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n    ## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n    ## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n```\nWhich renders as:\nThis is a test document for writeMDX. It has bold, italic\n\nlists\n\nsublists\n\nand so on\n\nAnd it has code. For R:\nx &lt;- 5\nprint(x)\n## [1] 5\nprint(head(mtcars))\n\n    ##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n    ## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n    ## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n    ## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n    ## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n    ## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n    ## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\nThere‚Äôs usually some fix-up with code, indentations and so on, and a couple of other things, but normally not a lot (maybe placing images in the right place‚Ä¶writeMDX might take of these things in future. Now it‚Äôs quite minimal.).\nSo it‚Äôs really quite simple to include R in a Gatsby site. You can write RMarkdown as you would regularly, run whatever code you normally would and then just transform the result to mdx. (You can also just use regular markdown if you don‚Äôt want to use MDX.)\nThere are a couple of things that need to be done differently. For equations, RMarkdown has lovely simple syntax: simply put your equation between $ or $$ and RMarkdown takes care of it. With MDX, I do the following. First, import these components from react-katex:\nimport { InlineMath, BlockMath } from \"react-katex\"\nInlineMath is like RMarkdown‚Äôs $ and BlockMath is like $$. Then you just use it as you would use any other React component:\n&lt;BlockMath math=\"y_{ij} = \\beta_j\\bf{x_i} - \\alpha_j\" /&gt;\nWhich gives:\n\\[\ny_{ij} = \\beta_j\\bf{x_i} - \\alpha_j\n\\]\nLike I mentioned above, you might need to change the paths to any images, depending on your setup. Images created from code chunks have a specific way of being in organised in RMarkdown. I source images in blog posts from a subfolder named, surprisingly, images/. So if I make something with ggplot2 in a code chunk, this will need to go into images/ so the .mdx will find it.\nSo that‚Äôs how you can use Gatsby for writing about R-related stuff if you‚Äôre interested. I haven‚Äôt written about the actual Gatsby/React/JS side of things and how it was for a data scientist to learn all that, but I might do that in the future."
  },
  {
    "objectID": "posts/economist.html",
    "href": "posts/economist.html",
    "title": "Re-creating Plots from The Economist in R and ggplot2",
    "section": "",
    "text": "The Economist is well known for its graphs and images, and I personally like them a lot. I was doing some work on Brexit when I spied the image above, and thought how much I would like to make something similar. Since my go-to environment is R, and its go-to plotting package ggplot2, I thought I‚Äôd try to recreate the image using these tools. (Hat tip: I was half-way through doing this, and getting a little irritated with British place names, when I came across this fantastic RPub, which helped a lot with the area names. The code below that deals with cleaning up and merging the administrative area data comes from Benjamin. (*Edit: this code chunk was huge, so I think it‚Äôs better to source it from GitHub gists. That‚Äôs what the code below does now. The original is here)\nThe data that we start off with is available from the UK Electoral Commission.\nlibrary(devtools)\nsource_gist(\"d969a3681bccfc089a202478ff3c1306\",\n            filename = \"brexit.R\")\nSo, after all that data tidying, we‚Äôre ready to make our plot. Since the Economist uses the ITC Officina Sans font, you‚Äôll need that on your computer (or something similar). With the extrafont package, we can take avail of these, well, extra fonts.\nlibrary(extrafont)\nfont_import()\n# There are various folders on my mac with fonts in them:\nfont_import(\"/Users/robert/Library/Fonts/\")\nfont_import(\"/System/Library/Fonts/\")\nfont_import(\"/Library/Fonts/\")\nloadfonts()\n# see what's available:\nfonts()\nThe following code makes the plot that I wanted, with the image saved and the font that I need embedded in the pdf that is produced. In Rstudio, no text will appear, as this font is not supported. But it‚Äôll be in the pdf.\nMM &lt;- ggplot() +\n  geom_polygon(data = uk.plot1, aes(x = long, y = lat, group = group, fill = Import_shock), colour = \"white\", size = 0.1) +\n  scale_fill_distiller(palette = \"Reds\", breaks = pretty_breaks(n = 8)) +\n  geom_point(aes(x = cities.lon, y = cities.lat), color = \"black\", size = 2.5, shape = 21, fill = \"black\") +\n  geom_point(aes(x = cities.lon, y = cities.lat), color = \"white\", size = 1, shape = 21, fill = \"white\") +\n  theme_nothing(legend = T) +\n  annotate(\"text\", x=-4.75, y=52.5, label=\"WALES\", size=4, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=1.12, y=53.65, label=\"ENGLAND\", size=4, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=-2.2, y=57.9, label=\"SCOTLAND\", size=4, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=-4.7, y=51.42, label=\"Port Talbot\", size=3.45, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=.3, y=52.15, label=\"Northampton\", size=3.45, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"segment\", x = -3.3, xend = -2.5, y = 54, yend = 54) +\n  annotate(\"segment\", x = -2.5, xend = -2.5, y = 54, yend = 53.85) +\n  annotate(\"text\", x=-4.22, y=54, label=\"Blackburn\", size=3.45, family = \"ITCOfficinaSans LT Book\") +\n  xlim(range(uk.plot1$long)) + ylim(range(uk.plot1$lat)) +\n  theme(plot.background = element_rect(fill = \"#A4D3EE\", colour = \"#A4D3EE\"), panel.background = element_rect(fill = \"#A4D3EE\", colour = \"#A4D3EE\"), legend.position = \"none\") +\n  coord_map()\n\n\nggsave(\"Brexit_test.pdf\", MM)\nembed_fonts(\"Brexit_test.pdf\")\n\nIt‚Äôs not exactly the same (I could not get that legend to work right!), but I think it‚Äôs a pretty close match. Good, stuff, R üëè .\nP.s. I know there are themes available to get close to the Economist‚Äôs image style, but I wanted do it myself üòÉ. Anyway, if you‚Äôd like to include the theme_economist() function from the ggthemes package, it‚Äôs easy:\nEco &lt;- ggplot() +\n  geom_polygon(data = uk.plot1, aes(x = long, y = lat, group = group, fill = Import_shock), colour = \"white\", size = 0.1) +\n  scale_fill_distiller(palette = \"Reds\", breaks = pretty_breaks(n = 8)) +\n  geom_point(aes(x = cities.lon, y = cities.lat), color = \"black\", size = 2.5, shape = 21, fill = \"black\") +\n  geom_point(aes(x = cities.lon, y = cities.lat), color = \"white\", size = 1, shape = 21, fill = \"white\") +\n  theme_economist() +\n  annotate(\"text\", x=-4.75, y=52.5, label=\"WALES\", size=4, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=1.12, y=53.65, label=\"ENGLAND\", size=4, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=-2.2, y=57.9, label=\"SCOTLAND\", size=4, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=-4.7, y=51.42, label=\"Port Talbot\", size=3.45, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"text\", x=.3, y=52.15, label=\"Northampton\", size=3.45, family = \"ITCOfficinaSans LT Book\") +\n  annotate(\"segment\", x = -3.3, xend = -2.5, y = 54, yend = 54) +\n  annotate(\"segment\", x = -2.5, xend = -2.5, y = 54, yend = 53.85) +\n  annotate(\"text\", x=-4.22, y=54, label=\"Blackburn\", size=3.45, family = \"ITCOfficinaSans LT Book\") +\n  xlim(range(uk.plot1$long)) + ylim(range(uk.plot1$lat)) +\n  theme(axis.ticks = element_blank(), axis.title = element_blank(), axis.text = element_blank(), panel.grid.major.y = element_blank(), legend.position = \"none\") +\n  coord_map()\n\nggsave(\"Brexit_test_Econ.pdf\", Eco)\nembed_fonts(\"Brexit_test_Econ.pdf\")\nThe combination is better, actually:\n\nIf we put them side by side, you can see that we didn‚Äôt actually do a bad job:\n\nThere are (of course) also other great examples of using ggplot2 to recreate images, in this case textbook statistical distributions."
  },
  {
    "objectID": "posts/wetwinter.html",
    "href": "posts/wetwinter.html",
    "title": "How wet was winter 2023/2024?",
    "section": "",
    "text": "I‚Äôm back! It‚Äôs been years since I blogged on here. There are many reasons for that, being busy is a big one, kid, job etc.Getting a new laptop recently has renewed my interest in blogging again. I‚Äôve decided to change from gatsby.js to quarto for a simplified workflow. As much as I liked learning Javascript, keeping up with the changes in dependencies and the security vulnerabilities is too much of a commitment for something that‚Äôs supposed to be light-hearted and fun.\nSo let‚Äôs start off with a question that many in Ireland might have been asking themselves over the past few months ‚Äì just how wet was that freaking winter?? It certainly seemed worse than usual, but let‚Äôs see if we can find any data to help us figure that out. I‚Äôm getting this from Met Eireann, and it‚Äôs for the Shannon Airport weather station. Shannon, being on the West, can be quite a bit wetter than other parts of the country like Dublin or Waterford, so if 2024 was wet, I‚Äôd say we could probably expect it to be magnified here. Maybe not, I‚Äôll defer to meteorologists on that one.\n\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(glue)\n\nrain &lt;- read_csv(\"https://cli.fusio.net/cli/climate_data/webdata/dly518.csv\", skip = 24) |&gt; \n  select(date, rain)\n\nknitr::kable(head(rain))\n\n\n\n\ndate\nrain\n\n\n\n\n01-sep-1945\n0.2\n\n\n02-sep-1945\n0.0\n\n\n03-sep-1945\n0.0\n\n\n04-sep-1945\n0.0\n\n\n05-sep-1945\n0.0\n\n\n06-sep-1945\n0.0\n\n\n\n\n\nSo we‚Äôve got a little lit of work to do on those dates. We‚Äôve got three-letter abbreviations for each month:\n\nrain |&gt; \n  mutate(\n    months = str_extract(date, \"-[a-z]*-\") |&gt; str_remove_all(\"-\")\n  ) |&gt; \n  distinct(months) |&gt; \n  knitr::kable()\n\n\n\n\nmonths\n\n\n\n\nsep\n\n\noct\n\n\nnov\n\n\ndec\n\n\njan\n\n\nfeb\n\n\nmar\n\n\napr\n\n\nmay\n\n\njun\n\n\njul\n\n\naug\n\n\n\n\n\nI‚Äôm sure there‚Äôs a better way to do this, but since we‚Äôve only got 12 to change, let‚Äôs just do it this way.\n\nrain |&gt; \n  mutate(\n    date = str_replace(date, \"jan\", \"01\"),\n    date = str_replace(date, \"feb\", \"02\"),\n    date = str_replace(date, \"mar\", \"03\"),\n    date = str_replace(date, \"apr\", \"04\"),\n    date = str_replace(date, \"may\", \"05\"),\n    date = str_replace(date, \"jun\", \"06\"),\n    date = str_replace(date, \"jul\", \"07\"),\n    date = str_replace(date, \"aug\", \"08\"),\n    date = str_replace(date, \"sep\", \"09\"),\n    date = str_replace(date, \"oct\", \"10\"),\n    date = str_replace(date, \"nov\", \"11\"),\n    date = str_replace(date, \"dec\", \"12\"),\n    date = parse_date_time(date, \"dmy\")\n    ) -&gt; rain\n\nLet‚Äôs have a quick look at this dataset. We‚Äôll make things feel rainy with new blue defaults for geoms:\n\nblue &lt;- \"dodgerblue4\"\ntheme_set(theme_minimal())\nupdate_geom_defaults(\"point\", list(colour = blue))\nupdate_geom_defaults(\"bar\", list(colour = blue, fill = \"dodgerblue2\"))\nupdate_geom_defaults(\"line\", list(colour = blue))\n\nggplot(rain, aes(x = date, y = rain)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThat‚Äôs a long time series! Maybe let‚Äôs simplify things. We‚Äôll group by month and sum, that might give us something that‚Äôs easier to think about.\n\nmonth_rain &lt;- rain |&gt; \n  mutate(m_date = floor_date(date, \"month\")) |&gt; \n  group_by(m_date) |&gt; \n  summarise(month_sum = sum(rain)) |&gt; \n  ungroup()\n\nggplot(month_rain, aes(x = m_date, y = month_sum)) +\n  geom_line()\n\n\n\n\n\n\n\n\nOk, now we can start to see the cycle throughout the years, along with a few spikes for particularly dry or wet months. It‚Äôs a little hard to see the winter months here as everything is bunched up so we can split winter and non-winter apart. I‚Äôm including October to March in winter here, if we followed the Romans (‚ÄúHibernia‚Äù), we‚Äôd probably just use the whole year as winter!\n\nmonth_rain &lt;- month_rain |&gt; \n  mutate(\n    winter = if_else(month(m_date) %in% c(10, 11, 12, 1, 2), TRUE, FALSE)\n    )\n\nggplot(month_rain, aes(x = m_date, y = month_sum)) +\n  geom_line() +\n  facet_grid(~winter) +\n  geom_smooth()\n\n\n\n\n\n\n\n\nAlright, so now we can clearly see that winter, as you would expect, has an elevated level of rainfall, along with more spikes of high rainfall. Interestingly, there are plenty of dry spikes too, particularly in the early 1960s. Maybe 2024 just feels wetter in comparison to the last few years? Let‚Äôs shorten the timespan:\n\nmonth_rain |&gt; \n  filter(year(m_date) &gt; 2018, winter) |&gt; \n  ggplot(aes(x = m_date, y = month_sum)) +\n  geom_col()\n\n\n\n\n\n\n\n\n2023/4 was a bit more consistently wet but as we can see, it‚Äôs nothing particularly unusual. It‚Äôs odd, I remember 2020 as a dry winter and a great spring, excepting the obvious, of course.\nIf we look back a little longer, to 2010, and highlight 2024, we can see that, nope, 2023/4 just wasn‚Äôt that bad, although most years were drier. Compared to other years in Ireland, of course, not comparing to non-Hibernia.\n\nmonth_rain |&gt; \n  filter(year(m_date) &gt; 2010, winter) |&gt; \n  group_by(year = year(m_date)) |&gt; \n  summarise(avg = mean(month_sum)) |&gt; \n  mutate(twentyfour = ifelse(year == 2024, 1, 0)) |&gt; \n  ggplot(aes(x = year, y = avg, fill = twentyfour)) +\n  geom_line() +\n  geom_point(shape = 21) +\n  ylim(c(50, 150)) +\n  guides(fill = \"none\") +\n  geom_hline(yintercept = 116, linetype = 3)\n\n\n\n\n\n\n\n\n‚ÄúNon-Hibernia‚Äù - that‚Äôs interesting, actually. I wonder how the West of Ireland compares to England, let‚Äôs say. The Met Office has some great stats (going back to 1836!!), let‚Äôs grab them for the South of England (likely to be much drier). This file is a bit challenging to read in, but we can fix that. Let‚Äôs filter for the same time period (2010 onwards) too. I love how these weather guys like their three-letter month names, jeez.\n\neng_rain &lt;- read_table(\n  \"https://www.metoffice.gov.uk/pub/data/weather/uk/climate/datasets/Rainfall/date/England_S.txt\",\n  skip = 6,\n  col_names = c(\n    \"year\", \"jan\", \"feb\", \"mar\", \"apr\", \"may\", \"jun\", \"jul\", \"aug\", \"sep\",\n    \"oct\", \"nov\", \"dec\", \"win\", \"spr\", \"sum\", \"aut\", \"ann\"\n    )\n  ) |&gt; \n  select(year:mar, oct:dec) |&gt; \n  filter(year &gt;= 2010) |&gt; \n  pivot_longer(jan:dec) |&gt; \n  mutate(\n    month = case_when(\n      name == \"jan\" ~ \"01\",\n      name == \"feb\" ~ \"02\",\n      name == \"mar\" ~ \"03\",\n      name == \"oct\" ~ \"10\",\n      name == \"nov\" ~ \"11\",\n      name == \"dec\" ~ \"12\"\n    ),\n    date = glue(\"{year}-{month}-01\") |&gt; parse_date_time(\"ymd\")\n  ) |&gt; \n  select(date, value)\n\nOk, let‚Äôs see if we can get fancy here ‚Äì we‚Äôll layer the Irish data behind the English data so we can compare them. I‚Äôll use grey for the Irish data and we‚Äôll keep the same plot limits and guiding line for 2024 in Shannon:\n\nire &lt;- month_rain |&gt; \n  filter(year(m_date) &gt; 2010, winter) |&gt; \n  group_by(year = year(m_date)) |&gt; \n  summarise(avg = mean(month_sum))\n\neng &lt;- eng_rain  |&gt; \n  group_by(year = year(date)) |&gt; \n  summarise(avg = mean(value, na.rm = TRUE))\n\nggplot(data = eng, aes(x = year, y = avg)) +\n  geom_line(data = ire, colour = \"grey80\") +\n  geom_point(data = ire, colour = \"grey80\", fill = \"grey95\", shape = 21) +\n  ylim(c(50, 150)) +\n  guides(fill = \"none\") +\n  geom_hline(yintercept = 116, linetype = 3, colour = \"grey60\") +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\nThe South of England is much drier, as you might expect. Interestingly, 2023/4 was a wet year in Southern England!"
  },
  {
    "objectID": "posts/flagfillr.html",
    "href": "posts/flagfillr.html",
    "title": "Mapping Economic Partners with flagfillr",
    "section": "",
    "text": "üá®üá® üá®üáΩ üáµüáπ üá©üá¥ üá´üá≤ üá∞üá∑\nRecently I wrote a little package for R called flagfillr (you can read more details here). One of the main reasons I made this is because I had seen a few maps of economic partners, for example this one, from here:\nThese types of maps (some more here, of the US) are interesting visual summaries, but they look a little Excel to me (sorry, whoever made it!). I thought I could spice them up with flags, cos hey, who doesn‚Äôt like cool flags (like Bhutan, üáßüáπ, awesome!)"
  },
  {
    "objectID": "posts/flagfillr.html#enter-flagfillr",
    "href": "posts/flagfillr.html#enter-flagfillr",
    "title": "Mapping Economic Partners with flagfillr",
    "section": "Enter flagfillr",
    "text": "Enter flagfillr\nWith flagfillr, the above map can easily be created. Once you have a data.frame of the data (here I refer to it as DF), you can do it with one line:\nflagfillr::flag_fillr_data(data = DF, country = \"Brazil\", resolution = \"large\",\n                           size = \"250\")\nWhich, if your data corresponds to the map above, will give you this:"
  },
  {
    "objectID": "posts/flagfillr.html#mapping-economic-partners",
    "href": "posts/flagfillr.html#mapping-economic-partners",
    "title": "Mapping Economic Partners with flagfillr",
    "section": "Mapping Economic Partners",
    "text": "Mapping Economic Partners\nSo how can you do this for another country? Let‚Äôs take the example of Australia, from data available here. First of all, we‚Äôll need to put our data into a dataframe, preferably with columns named country, state and partner, just to keep things simple. Simple is good. I‚Äôll use dplyr here (don‚Äôt forget you‚Äôll have to install flagfillr, see here). Since Australia‚Äôs main trade partners by state are basically China and Japan, let‚Äôs choose the third biggest trade partner by state, just to keep things interesting.\nlibrary(dplyr)\nlibrary(flagfillr)\nlibrary(plotly)\n\noz &lt;- tibble(\n  country = \"Australia\",\n  state = c(\"Northern Territory\", \"Western Australia\", \"New South Wales\",\n            \"South Australia\", \"Victoria\", \"Queensland\", \"Tasmania\",\n            \"Australian Capital Territory\"),\n  partner = c(\"India\", \"United Kingdom\", \"South Korea\", \"Malaysia\", \"New Zealand\",\n              \"India\", \"Malaysia\", \"Malaysia\")\n)\n\nflag_fillr_data(oz, country = \"Australia\", type = \"state\", size = \"250\",\n                partner_col = oz$partner, state_col = oz$state)\n\nIt‚Äôs not surprising that we see these countries here (the principal partners are China, Japan and India). Indeed, for many countries, the main economic partners are not a surprise. Let‚Äôs take a look at the mainland United States. We can recreate these plots with flagfillr (for more detailed data see here), and make them a little prettier (they come from here).\n\n\nFirst, let‚Äôs make the data:\nstates &lt;- state.name\nimports &lt;- tibble(\n  state = states,\n  partner = NA_character_\n) %&gt;%\n  mutate(partner = case_when(\n    state %in% c(\"Oregon\", \"Indiana\") ~ \"Ireland\",\n    state == \"Louisiana\" ~ \"Saudi Arabia\",\n    state == \"Alabama\" ~ \"South Korea\",\n    state %in% c(\"South Carolina\", \"Maryland\", \"Rhode Island\") ~ \"Germany\",\n    state == \"Delaware\" ~ \"United Kingdom\",\n    state %in% c(\"Texas\", \"Arizona\", \"Utah\") ~ \"Mexico\",\n    state %in% c(\"Washington\", \"Montana\", \"Wyoming\", \"Colorado\", \"North Dakota\",\n                 \"South Dakota\", \"Iowa\", \"Oklahoma\", \"West Virginia\", \"Maine\",\n                 \"New Hampshire\", \"Vermont\", \"Massachusetts\", \"Connecticut\") ~\"Canada\",\n    TRUE ~ \"China\"\n  ))\n\n\nflag_fillr_data(imports, country = \"United States\", type = \"state\", size = \"250\",\n                partner_col = imports$partner, state_col = imports$state)\n\nMaybe that‚Äôs a slightly disturbing vision to some people haha! I like to think it‚Äôs a nice reminder of how intertwined our world is. And some surprises in there‚Ä¶Ireland, South Korea, Saudi Arabia. Seems like Ireland exports tech products to Oregon, while Louisiana imports ‚Äì you guessed it ‚Äì oil from Saudi Arabia. How about the countries that these states most export to?\nexports &lt;- tibble(\n  state = states,\n  partner = NA_character_\n) %&gt;%\n  mutate(partner = case_when(\n    state %in% c(\"Oregon\", \"Washington\", \"Louisiana\", \"South Carolina\") ~ \"China\",\n    state == \"Nevada\" ~ \"Switzerland\",\n    state %in% c(\"Utah\", \"Delaware\") ~ \"United Kingdom\",\n    state %in% c(\"Wyoming\", \"Florida\") ~ \"Brazil\",\n    state == \"Connecticut\" ~ \"France\",\n    state %in% c(\"California\", \"Texas\", \"Arizona\", \"New Mexico\", \"Nebraska\",\n                 \"Kansas\") ~ \"Mexico\",\n    TRUE ~ \"Canada\"\n  ))\n\n\nflag_fillr_data(exports, country = \"United States\", type = \"state\", size = \"250\",\n                partner_col = exports$partner, state_col = exports$state)\n\nInteresting. What does Utah export to the UK? The ‚Äústate export[s] a broad range of goods, including animal products, aircraft engines, automotive parts, electronics, personal care products, pharmaceuticals, sporting goods, and industrial goods such as valves, drilling tools and fuses‚Äù, supposedly. Brazil and Wyoming also have an interesting relationship, with Wyoming‚Äôs exports being chemicals and the like.\nAnother interesting case that makes for an arresting visualization is the internal trade of European Union. Eurostat presents it to us in this format:\n\nBut we can make it prettier! Here I skip the code, but the pattern is the same as the above two plots. You might notice French Guyana sneaking in there ‚Äì it‚Äôs officialy part of France, so it‚Äôs included in the geometry column used for plotting. In the future, it‚Äôd be great to have a clean, easy way to filter it out, but right now that means messing with geometry list-columns, and that‚Äôs a bit messy.\n\nSo these are a few examples of the things you can do with flagfillr. Maybe in a future post I‚Äôll map some state-level trade for the countries that have state flags in the package (the Netherlands, for example)."
  },
  {
    "objectID": "posts/divorce-in-ireland.html",
    "href": "posts/divorce-in-ireland.html",
    "title": "Visualizing the Irish Divorce Referendum in R",
    "section": "",
    "text": "A while ago, I wrote a blog post on visualizing the results of the UK elections in 2017 (quite a while ago!). After the Irish elections and divorce referendum on Friday last, I thought it would be a nice opportunity to do something similar with Irish political data.\nFor a little background, there were two amendments proposed in the referendum. The first was related to removing from the Constitution the length of time a married couple have lived apart before they are granted a divorce, and the second proposal was to remove the section of the constitution that does not recognise divorces registered outside Ireland. The vote passed by 82%. Before, the Irish constitution required that spouses live separately for four of the previous five years in order to be granted a divorce. The constitution had prevented people who had gotten a divorce in other countries from getting married again during the lifetime of their former spouse. So yay, all that crap is gone.\nAs for plotting this in R, it‚Äôs quite straightforward. There two interesting things in this for me: using two layers of geom_sf() and having to use Cairo/X11 because plotting this using the defaults on the Mac I‚Äôm using was taking forever.1\nWe‚Äôll use these libraries:\nI‚Äôll also use the paletteer package for something later on, but it‚Äôs not required."
  },
  {
    "objectID": "posts/divorce-in-ireland.html#getting-the-spatial-data",
    "href": "posts/divorce-in-ireland.html#getting-the-spatial-data",
    "title": "Visualizing the Irish Divorce Referendum in R",
    "section": "Getting the Spatial Data",
    "text": "Getting the Spatial Data\nThere is a pretty decent website for Open Irish Data at https://data.gov.ie/dataset, which I‚Äôve just discovered and can‚Äôt wait to explore! They have some datasets of Irish political constituencies that we can use. We can get these like so:\nconstituencies &lt;- read_sf(\"https://data-osi.opendata.arcgis.com/datasets/ef0c0924d5ea4a0d875b5407d39eea03_0.geojson\") %&gt;%\n  select(Constituency = MAX_CON_NA, geometry)\nLet‚Äôs have a look at this:\nhead(constituencies)\n\n## # A tibble: 6 x 2\n##   Constituency                                                     geometry\n##   &lt;chr&gt;                                                  &lt;MULTIPOLYGON [¬∞]&gt;\n## 1 Laois (3)           (((-7.252613 53.14369, -7.252545 53.14369, -7.252087‚Ä¶\n## 2 Carlow-Kilkenny (5) (((-7.027801 52.8097, -7.027535 52.80972, -7.027112 ‚Ä¶\n## 3 Limerick (3)        (((-8.740033 52.67553, -8.739923 52.67551, -8.739376‚Ä¶\n## 4 Limerick City (4)   (((-8.452618 52.68973, -8.452593 52.68971, -8.452294‚Ä¶\n## 5 Longford-Westmeath‚Ä¶ (((-8.015205 53.58107, -8.016249 53.58253, -8.017917‚Ä¶\n## 6 Louth (5)           (((-6.516136 53.7667, -6.516245 53.76672, -6.516609 ‚Ä¶\nNot bad, but those constituency names are going to need a clean up. Easy üòé\nconstituencies &lt;- constituencies %&gt;%\n  mutate(Constituency = str_remove(Constituency, \"\\\\)\"),\n          Constituency = str_remove(Constituency, \"\\\\(\"),\n          Constituency = str_remove(Constituency, \"[0-9]\"),\n          Constituency = str_trim(Constituency),\n          Constituency = stringi::stri_trans_general(Constituency, \"Latin-ASCII\"))"
  },
  {
    "objectID": "posts/divorce-in-ireland.html#getting-the-referendum-data",
    "href": "posts/divorce-in-ireland.html#getting-the-referendum-data",
    "title": "Visualizing the Irish Divorce Referendum in R",
    "section": "Getting the Referendum Data",
    "text": "Getting the Referendum Data\nI‚Äôm sure these will eventually be published somewhere official, but for now we can scrape ‚Äôem. The Irish Times had a nice table of results at the url below, so we can scrape these.2\nit_result &lt;- read_html(\"https://www.irishtimes.com/news/politics/abortion-referendum/results\") %&gt;%\n  html_nodes(\"#abortiontable\") %&gt;%\n  html_table(fill = TRUE) %&gt;%\n  .[[1]] %&gt;%\n  filter(Constituency != \"Total\")\nSo what does this look like?\nhead(it_result)\n\n## # A tibble: 6 x 11\n##   Constituency Verdict Electorate `Total poll` `% Turnout` `Invalid ballot‚Ä¶\n##   &lt;chr&gt;        &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;       &lt;chr&gt;\n## 1 Carlow-Kilk‚Ä¶ YES     112,704    69,860       61.99%      231\n## 2 Cavan-Monag‚Ä¶ YES     91,602     58,067       63.39%      163\n## 3 Clare        YES     83,225     53,576       64.37%      169\n## 4 Cork East    YES     85,643     54,639       63.80%      148\n## 5 Cork North-‚Ä¶ YES     84,412     52,713       62.45%      166\n## 6 Cork North-‚Ä¶ YES     68,830     45,379       65.93%      131\n## # ‚Ä¶ with 5 more variables: `Valid poll` &lt;chr&gt;, `Votes Yes` &lt;chr&gt;, `Votes\n## #   No` &lt;chr&gt;, `% Yes` &lt;chr&gt;, `% No` &lt;chr&gt;\nNot so bad, we have everything we need here, once we join it with the spatial data. There are a few small changes & clean-ups we‚Äôll make:\nit_result &lt;- it_result %&gt;%\n  mutate_all(str_remove, \"[,%]\") %&gt;%\n  mutate_at(.vars = c(3:11), .funs = as.numeric) %&gt;%\n  mutate(Constituency = ifelse(\n  Constituency == \"Limerick County\", \"Limerick\", Constituency)\n  )\ndiv_results &lt;- full_join(it_result, constituencies) %&gt;%\n  rename(perc_yes = `% Yes`, perc_no = `% No`)"
  },
  {
    "objectID": "posts/divorce-in-ireland.html#plotting-the-results",
    "href": "posts/divorce-in-ireland.html#plotting-the-results",
    "title": "Visualizing the Irish Divorce Referendum in R",
    "section": "Plotting the Results",
    "text": "Plotting the Results\nAs a quick aside, if geom_sf() is slow for you, try this:\nX11(type = \"cairo\")\nI‚Äôd like to put a map of the general area (Ireland & Great Britain) in the background, too. Since our spatial data doesn‚Äôt have any of this info, we can get it with the rnaturalearthhires package.\nireland &lt;- rnaturalearthhires::countries10 %&gt;%\n  st_as_sf() %&gt;%\n  filter(SOVEREIGNT %in% c(\"Ireland\", \"United Kingdom\"))\nWe can put this behind our data layer, creating a nice effect. I‚Äôm sure this would look a whole lot nicer if I wasn‚Äôt going through the whole plotting mess with geom_sf() on the Mac, but if you run this code on Windows, it‚Äôll probably look pretty nice (first time I‚Äôve ever recommended Windows over anything else for something R-related).\ndiv_results %&gt;%\n  ggplot() +\n  geom_sf(data = ireland, colour = \"grey 88\", size = 0.2) +\n  geom_sf(aes(fill = perc_yes), colour = \"black\", size = 0.2) +\n  coord_sf(xlim = c(-11, -5), ylim = c(51, 57)) +\n  scale_fill_viridis_c(name = \"% Yes\") +\n  theme_minimal() +\n  theme(legend.position = c(0.15, 0.8),\n        legend.background = element_blank()) +\n  labs(title = \"Irish Divorce Referendum Results\",\n        subtitle = \"Vote of May 24th, 2019\",\n        caption = \"Sources: Irish Times \\n https://www.irishtimes.com/news/politics/abortion-referendum/results\")\n\nAs you can see, the highest percentage of ‚ÄòYes‚Äô is in the Dublin area, not really a surprise. Other higher ‚ÄòYes‚Äô areas are Cork and Galway, again not a big surprise. That‚Äôs Donegal up there in the top-left corner with the lowest vote, for those of you not familiar with the geography of Ireland."
  },
  {
    "objectID": "posts/divorce-in-ireland.html#turnout",
    "href": "posts/divorce-in-ireland.html#turnout",
    "title": "Visualizing the Irish Divorce Referendum in R",
    "section": "Turnout",
    "text": "Turnout\nSomething else interesting might be the turnout. The viridis scales are great and everything, but they‚Äôre a little boring sometimes‚Ä¶how about the House of Lannister palette from the gameofthrones package? (Accessed through paletteer).\ndiv_results %&gt;%\n  ggplot() +\n  geom_sf(data = ireland, colour = \"grey 88\", size = 0.2) +\n  geom_sf(aes(fill = `% Turnout`), colour = \"black\", size = 0.2) +\n  coord_sf(xlim = c(-11, -5), ylim = c(51, 57)) +\n  paletteer::scale_fill_paletteer_c(gameofthrones, lannister) +\n  theme_minimal() +\n  theme(legend.position = c(0.15, 0.8),\n        legend.background = element_blank()) +\n  labs(title = \"Irish Divorce Referendum Results\",\n       subtitle = \"Vote of May 24th, 2019; Turnout\",\n       caption = \"Sources: Irish Times \\n https://www.irishtimes.com/news/politics/abortion-referendum/results\")\n\nTurnout was higher in the ‚Äòhigher Yes‚Äô parts of the country, suggesting those who wanted to vote ‚ÄòYes‚Äô were more motivated to do so as opposed to voting ‚ÄòNo‚Äô."
  },
  {
    "objectID": "posts/divorce-in-ireland.html#footnotes",
    "href": "posts/divorce-in-ireland.html#footnotes",
    "title": "Visualizing the Irish Divorce Referendum in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA MacBook Pro (15-inch, 2018); 2.6 GHz Intel Core i7 Processor with 16 GB of memory, so definitely not a computer issue. OSX-Rstudio-geom_sf issue. See here and all links.‚Ü©Ô∏é\nThanks for not being dicks and using JS hard-to-scrape tables, Irish Times.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/stan-irt-code.html",
    "href": "posts/stan-irt-code.html",
    "title": "Stan IRT Code",
    "section": "",
    "text": "(This turned out to be a bit of a ramble, for the code go here üòÑ)\nMy PhD thesis focused on latent variable models as a way to model legislative voting behaviour. The main model I used is called the Bayesian Item Response model, and the idea is that, from the observed votes of the legislators, we can build a scale on which we can place them, relative to one another. This is a common way of analysing legislative voting behaviour, and I‚Äôm sure you‚Äôve seen it before somewhere. There are basic ways to do it, using factor analysis or something like NOMINATE. These work well when you have lots of data (i.e.¬†lots of legislators voting often on lots of votes) and when you either suspect that there is low dimensionality (in other words, there is basically one scale on which you can place the legislators, not two or more) or you have interest in only one aspect of the voting space. In practice, most of these scales are interpreted to be between ‚Äòleft‚Äô and ‚Äòright‚Äô as commonly understood in politics. Whether this accurately captures voting behaviour is, of course, another question altogether.\nSo why didn‚Äôt I just do it the easy way? Well, apart from being a glutton for punishment, I had two reasons for diving into the world of Bayesian statistics, R, JAGS, and later Stan. The first is that Bayesian stats made sense to me the moment I first read about it, on a deep, intuitive level. Frequentist statistics just always seemed convoluted to me. Anyway, after reading Clinton, Jackman and Rivers‚Äô 2004 paper on Bayesian analysis of roll-call votes (earlier working paper here), I was convinced it was scientifically the sound thing to do as well. Secondly, when you don‚Äôt have lots of data (as was the case for me), the Bayesian version of these models is better equipped to come out with better results (see the paper above).\nI started off using Simon Jackman‚Äôs pscl package in R to model these votes and to create ideal points. I also used MCMCpack by Quinn & Martin, which turned out to be better suited to my case. However, I soon realised that more complex models were difficult or impossible with these (fast) ready-made tools. Enter JAGS and the world of probabilistic programming. Martyn Plummer has done a great job with JAGS, and personally, I like it a lot, especially the syntax, but for IRT models like the ones I was using, it was painfully slow. Days and days running models that just didn‚Äôt seem to converge (I later found out why).\nWell, that left me with two months to finish my thesis and models that wouldn‚Äôt converge. Lovely. Previously, I had tried to learn Stan but I found it unappealing for various reasons, including the difficult syntax. Actually, the syntax makes much more sense to me now that I‚Äôve learned some other programming languages, whcih shows where Stan is coming from ‚Äì it‚Äôs a fully fledged computer language, designed to be a computer language. Hence things that PhD students in non-computer science areas are not accustomed to seeing pop up, like variable type declarations and so on. But my looming deadline left me with no choice but to give Stan a shot and see if it could help me out.\nI turns out Stan did help, it runs much faster for these IRT models, and I was able to get everything done and make a thesis I‚Äôm proud of. At the time, I published some example code on a GitHub repo and this led to me regularly getting emails from students and professors around the world asking how in the hell does Stan work. I understand your pain!\nAnyway‚Ä¶this blog post was supposed to be a simple, quick note to say that I have updated said repo with new and better Stan IRT code. It‚Äôs deliberately simple and comes with example data and example R code for how to run the models and plot ideal points with ggplot2 afterwards. The plots are quite nice, I think. They look a little somethin‚Äô like this:"
  },
  {
    "objectID": "posts/rating-r-packages.html",
    "href": "posts/rating-r-packages.html",
    "title": "Rating R Packages",
    "section": "",
    "text": "The new rOpenSci package packagemetrics is a new ‚Äòmeta‚Äô package for R with info on packages: dependencies, how long issues take to be resolved, how many watchers on GitHub, and more. Let‚Äôs take a look at a few packages I use and some of my own. Install:\ninstall.packages(\"formattable\")\ndevtools::install_github(\"ropenscilabs/packagemetrics\")\nThen load the packages we‚Äôre going to use (I liked the table they have in their README, so I thought I‚Äôd keep with that style):\nlibrary(formattable)\nlibrary(packagemetrics)\nlibrary(dplyr)\nNext, let‚Äôs get the packages I‚Äôm interested in and make our nice table:\npackages &lt;- list(\"dplyr\", \"tidyr\", \"tidyRSS\",\n                 \"rstan\", \"rjags\",\n              \"electionsBR\", \"tmap\")\n\n\npd &lt;- purrr::map(packages, combine_metrics) %&gt;%\n  data.table::rbindlist(fill=TRUE) %&gt;%\n  select(package, published, dl_last_month, stars, forks,\n         last_commit, last_issue_closed,\n         depends_count, watchers) %&gt;%\n  mutate(last_commit = round(last_commit, 1),\n         last_issue_closed = round(last_issue_closed, 1))\n\npd[is.na(pd)] &lt;- \"\"\n\nformattable(pd, list(\n    package = formatter(\"span\",\n                      style = x ~ style(font.weight = \"bold\")),\n    contributors = color_tile(\"white\",\"#1CC2E3\"),\n    depends_count = color_tile(\"white\", \"#1CC2E3\"),\n    reverse_count = color_tile(\"white\", \"#1CC2E3\"),\n    tidyverse_happy = formatter(\"span\",\n                                style = x ~ style(color = ifelse(x, \"purple\",\"white\")),\n                                x ~ icontext(ifelse(x, \"glass\",\"glass\"))),\n    vignette = formatter(\"span\",\n                         style = x ~ style(color = ifelse(x, \"green\",\"white\")),\n                         x ~ icontext(ifelse(x, \"ok\",\"ok\"))),\n    has_tests =  formatter(\"span\",\n                           style = x ~ style(color = ifelse(x, \"green\",\"red\")),\n                           x ~ icontext(ifelse(x, \"ok\",\"remove\"))),\n    dl_last_month = color_bar(\"#56A33E\"),\n    forks = color_tile(\"white\", \"#56A33E\"),\n    stars = color_tile(\"white\", \"#56A33E\"),\n    last_commit = color_tile(\"#F06B13\",\"white\", na.rm=T),\n    last_issue_closed = color_tile(\"#F06B13\",\"white\", na.rm=T)\n  ))\n\nNice table. It‚Äôs not perfect ‚Äì maybe they still have some bugs to work out ‚Äì but this is a nice little package. Still, there are so many packages out there that I still use them based on cool examples I see, either on blogs, twitter, or in academic papers. I‚Äôve never much used the CRAN Task Views and I doubt I‚Äôll use packagemetrics much, but it‚Äôs interesting for those who get their R this way."
  },
  {
    "objectID": "posts/BayesianIRT.html",
    "href": "posts/BayesianIRT.html",
    "title": "Bayesian IRT in R and Stan",
    "section": "",
    "text": "This blog post is outdated and the code will not run ‚Äì for newer, cleaner IRT R code, see this github repo and this blog post.\nThe code below on Stan is also available as an RPub webpage, if you‚Äôd rather work through the examples than read all of the post.\nOne of the first areas where Bayesian modelling gained an entry point into the social sciences (and in particular political science) was in the area of legislator ideal points, with the use of the Item-Response Theory (IRT) models from the educational testing literature in psychology. This topic proved to be the perfect subject for the comparison of Bayesian and frequentist methods, since ideal point creation usually depends on nominal voting data, which may contain a lot of missing data (legislators who miss votes or abstain) and a huge number of parameters (hundreds of roll-calls by hundreds of legislators). The benefits of Bayesian methods over frequentist techniques for ideal point analysis is discussed at length elsewhere1, but here I‚Äôll talk about a side-effect of using Bayesian methods for creating ideal points from roll-call data, that is, the long time it can take to run these models on a desktop computer. (In the following discussion, I refer to ‚Äòlegislators‚Äô, but these IRT models apply to all types of response to a question, whether the ‚Äòquestion‚Äô is a vote by a politician or a judge or questions on a test or survey.)\nTo create ideal points in R, you have three or four main options if you want to use ol‚Äô Bayes. First, there is the ready-made ideal() command of the package pscl by Simon Jackman & co. pscl includes some very handy little functions for those interested in generating ideal points from legislative voting data ‚Äì summary statistics and plots are all easy to make, and come ready-made, such as party loyalty statistics, for example. However, ideal() suffers somewhat from being so ‚Äòready‚Äô: it is a bit unsuited for more complex or indivualistic models compared to some of the options mentioned later. I‚Äôve also repeatedly run into problems with ideal() when trying to use some of the pscl package options (dropList(), for example), or when estimating multidimensional models. In terms of MCMC, only one chain at a time may be run. In fact, it is what it says on the tin: it‚Äôs a Bayesian version of W-NOMINATE, which means it has the advantages of that program (easy to use) and the disadvantages (when it doesn‚Äôt work you‚Äôre not sure why‚Ä¶a bit ‚Äòblack-box‚Äô).\nThe MCMCpack package also allows for the creation of ideal points, although its output is slightly less friendly to the beginner (an mcmclist object). Its MCMCirt1d() command is pretty similar to ideal() but allows for setting ‚Äòsoft‚Äô constraints rather than the spike prior that pscl uses to pin down the position of (at least) two legislators. This is better for two reasons, in my opinion. First, it avoids a hard constraint on legislators for legislatures in which we do not have strong a priori evidence to suppose that, for example, Legislator X is an extremist to the right, or Legislator Y to the left (the use of extremist legislators on either end of the supposed scale ‚Äòanchors‚Äô it). With MCMCpack, the ideal points of the constrained legislators are drawn from a truncated normal distribution (truncated at zero) and so Legislator X (our extremist to the right) simply cannot have an ideal point on the left side of the scale and the opposite for our left-side extremist legislator (the use of these soft constraints obviates the need for them actually being extremists too). I‚Äôve also found MCMCpack to be faster, although I haven‚Äôt tested that formally. In either case, both functions are quite similar. MCMCpack also has functions for dynamic models, robust & multidimensional models, and Ordinal IRT. They‚Äôve all worked well for me with the exception of MCMCirtkd(), the multidimensional model function, which never seems to get started.\nThe next option is to use the BUGS modelling language, either with BUGS itself or its cousin JAGS, both of which have been heavily used in the literature but can be extremely slow. I don‚Äôt recommend their use for ideal points.\nNext, we have Stan, which doesn‚Äôt have the simpler syntax of JAGS & BUGS, but is simply incomparably better in terms of speed. However, since it‚Äôs newer, you won‚Äôt find the amount of resources available for BUGS, for example (like here). There are a few resources: a simple one-dimensional model can be seen on Pablo Barber√°‚Äôs github; a friend of mine, Guilherme Duarte, has an example of a dynamic model on his github too.\nThere are some other resources available, but relate to slightly different IRT models, more common in the educational-testing literature, and less so in ideal point studies: the ‚ÄòRasch‚Äô model; the 2PL model (in which a ‚Äòyes‚Äô answer has a specific associated movement in the dimensional space and the discrimination parameter only takes on postive values; in the ideal-point model of Jackman it can possess negative and positive values).\nSince there are so few Stan resources for ideal point IRT models, I thought I‚Äôd post a few models here. The code is also available as an RPub webpage, as mentioned earlier. The statistical model we‚Äôll employ is:\n\\[\ny_{ij} = \\beta_j \\bf{x_i} - \\alpha_j,\n\\]\nwhere \\(y_{ij}\\) are the votes, in binary form (1 = ‚ÄòYes‚Äô; 2 = ‚ÄòNo‚Äô); the \\(\\bf x_i\\) are the ideal points of the legislators; and \\(\\beta_j\\) and \\(\\alpha_j\\) are the discrimination and difficulty parameters of the model.\nStarting from scratch in R in a new session (you‚Äôll need a C++ compiler if you don‚Äôt have one, see here):\ninstall.packages(\"rstan\")\nlibrary(\"rstan\")\nIdeal points are created from a j  m matrix of voting data (j legislators voting on m votes), coded 1 for ‚Äòyes‚Äô and 0 for ‚Äòno‚Äô and abstentions. Missing data are NA, and are deleted out before running in Stan. We can easily simulate data for this type of thing, but let‚Äôs use a real database. This data is from the 53rd legislature of the Brazilian Federal Senate (with thanks to CEBRAP, who built the original database, this comes from an extended version I created), we‚Äôll download it from my Github repo. You‚Äôll need to install readr if you don‚Äôt have it. (I also have the bad habit of naming my data as ‚Äúdata‚Äù‚Ä¶ not generally a great idea. It‚Äôll be ok here, though.)\nlibrary(readr)\ndata &lt;- read_csv(\"https://raw.githubusercontent.com/RobertMyles/Bayesian-Ideal-Point-IRT-Models/master/Senate_Example.csv\")\ncolnames(data)\nSo let‚Äôs take a look at the data. You‚Äôll see the column names are ‚ÄúVoteNumber‚Äù, ‚ÄúSenNumber‚Äù, ‚ÄúSenatorUpper‚Äù, ‚ÄúVote‚Äù, ‚ÄúParty‚Äù, ‚ÄúGovCoalition‚Äù, ‚ÄúState‚Äù, ‚ÄúFP‚Äù, ‚ÄúOrigin‚Äù, ‚ÄúContentious‚Äù, ‚ÄúIndGov‚Äù, and ‚ÄúVoteType‚Äù. I‚Äôve kept them in this state so that we can tidy things up and manipulate things a little, stuff you‚Äôll probably have to do any time you deal with real data of this sort. We can also have a look later at different plotting options using some of these variables. First, let‚Äôs change the votes, which are in the format ‚ÄúS‚Äù (Sim, ‚ÄòYes‚Äô), ‚ÄúN‚Äù (‚ÄòNo‚Äô), ‚ÄúA‚Äù (Abstention), and ‚ÄúO‚Äù (Obstruction), to numeric format.\ndata$Vote[data$Vote==\"S\"] &lt;- 1\ndata$Vote[data$Vote==\"N\"] &lt;- 0\ndata$Vote[data$Vote  %in% c(NA,\"O\",\"A\")] &lt;- NA\ndata$Vote &lt;- as.numeric(data$Vote)\nNext, we‚Äôll create the ‚Äòvote matrix‚Äô. This is the j \\(\\times\\) m matrix that we will use to create the ideal points with Stan. The rows will be the legislators and the columns the votes. We will also need to deal with the issue of ‚Äòconstraints‚Äô: we need to identify d(d + 1) legislators in d dimensions and constrain their ideal points in some way. For now, we‚Äôll just organise our vote matrix in such a way that the two legislators that will be constrained are placed in rows 1 and 2 of the matrix. For this example, we can use Senators Agripino and Suplicy, who belong to two parties that are generally considered to be on opposite sides of the political ‚Äòspace‚Äô that we will place our ideal points upon. Organizing things in this way is not necessary but makes the Stan model code cleaner later on.\ndata$FullID &lt;- paste(data$SenatorUpper, data$Party, sep=\":\")\nNameID &lt;- unique(data$FullID)\nJ &lt;- length(unique(NameID))\nM &lt;- length(unique(data$VoteNumber))\ngrep(\"JOSE AGRIPINO:PFL\", NameID)  #34\ngrep(\"EDUARDO SUPLICY:PT\", NameID) #12\nNameID &lt;- NameID[c(34, 12, 1:11, 13:33, 35:J)]\n\ny &lt;- matrix(NA,J,M)\nRows &lt;- match(data$FullID, NameID)\nCols &lt;- unique(data$VoteNumber)\nColumns &lt;- match(data$VoteNumber, Cols)\n\nfor(i in 1:dim(data)[1]){\n  y[Rows[i], Columns[i]] &lt;- data$Vote[i]\n}\n\ndimnames(y) &lt;- list(unique(NameID), unique(data$VoteNumber))\nI presume you‚Äôre using RStudio. Clicking on the viewer should show you the vote matrix, which should look like this:\nNext we‚Äôll make a dataframe of legislator variables which we‚Äôll use later on, and one of vote characteristics.\nldata &lt;- data.frame(FullID = unique(NameID),\n                    Party = data$Party[match(unique(NameID),\n                                             data$FullID)],\n                    GovCoalition = data$GovCoalition[match(unique(NameID),\n                                                           data$FullID)],\n                    Name = data$SenatorUpper[match(unique(NameID),\n                                                   data$FullID)],\n                    State = data$State[match(unique(NameID),\n                                             data$FullID)],\n                    row.names = NULL,\n                    stringsAsFactors = FALSE)\n\nvdata &lt;- data.frame(VoteNumber = unique(data$VoteNumber),\n                    VoteType = data$VoteType[match(unique(data$VoteNumber),\n                                                   data$VoteNumber)],\n                    SenNumber = data$SenNumber[match(unique(data$VoteNumber),\n                                                     data$VoteNumber)],\n                    Origin = data$Origin[match(unique(data$VoteNumber),\n                                               data$VoteNumber)],\n                    Contentious = data$Contentious[match(unique(data$VoteNumber),\n                                                         data$VoteNumber)],\n                    IndGov = data$IndGov[match(unique(data$VoteNumber),\n                                               data$VoteNumber)],\n                    stringsAsFactors = F)\nStan is not like JAGS and BUGS in that NA is unwieldy to incorporate. The best thing to do is to delete missing data out, as can be seen in Barber√°‚Äôs script linked to earlier, which I‚Äôll copy here.\nN &lt;- length(y)\nj &lt;- rep(1:J, times=M)\nm &lt;- rep(1:M, each=J)\n\nmiss &lt;- which(is.na(y))\nN &lt;- N - length(miss)\nj &lt;- j[-miss]\nm &lt;- m[-miss]\ny &lt;- y[-miss]\nNext, we‚Äôll set our initial values. There are various ways to do this, ranging from leaving it up to Stan (i.e.¬†not setting any values) to creating lists with specific starting values for each parameter. What we‚Äôll do here is use the starting values as a way to start the parties off in separate places. This has several advantages: we already know that these parties don‚Äôt vote together very often (i.e., they are parties of the government and the opposition) and so we can speed up the model by starting the legislators off where we already know they‚Äôll be (i.e.¬†right-wing parties on the right etc.). This also has the benefit of making it less likely that we‚Äôll end up with ‚Äòsign-flips‚Äô, where a legislator with a bi-modal posterior distribution has an ideal point from the ‚Äòwrong‚Äô mode.2 For the discrimination and difficulty paramters, we‚Äôll use a random sample from normal distributions. We‚Äôll also save all this information as stan.data, which is the list of data we‚Äôll use with Stan.\nldata$ThetaStart &lt;- rnorm(J, 0, 1)\nldata$ThetaStart[ldata$Party==\"PFL\" | ldata$Party==\"PTB\" | ldata$Party==\"PSDB\" | ldata$Party==\"PPB\"] &lt;- 2\nldata$ThetaStart[ldata$Party==\"PT\" | ldata$Party==\"PSOL\" | ldata$Party==\"PCdoB\"] &lt;- -2\nThetaStart &lt;- ldata$ThetaStart\n\ninitF &lt;- function() {\n  list(theta=ThetaStart, beta=rnorm(M, 0, 2), alpha=rnorm(M, 0, 2))\n}\n\nstan.data &lt;- list(J=J, M=M, N=N, j=j, m=m, y=y, ThetaStart=ThetaStart)\nStan model code differs from those mentioned above in a few aspects. Firstly, variables need to be declared, along with their type. For example, J, which is our index for the number of senators, is declared in the following code as an integer. The parameters are likewise declared, as real numbers. The model code has three blocks: data, parameters and the model itself (there are other blocks possible, such asgenerated data, see the Stan manual. Stan code is also imperative ‚Äì the order of the blocks matters.\nstan.code &lt;- \"\n    data {\n    int&lt;lower=1&gt; J; //Senators\n    int&lt;lower=1&gt; M; //Proposals\n    int&lt;lower=1&gt; N; //no. of observations\n    int&lt;lower=1, upper=J&gt; j[N]; //Senator for observation n\n    int&lt;lower=1, upper=M&gt; m[N]; //Proposal for observation n\n    int&lt;lower=0, upper=1&gt; y[N]; //vote of observation n\n    }\n    parameters {\n    real alpha[M];\n    real beta[M];\n    real theta[J];\n    }\n    model {\n    alpha ~ normal(0,5);\n    beta ~ normal(0,5);\n    theta ~ normal(0,1);\n    theta[1] ~ normal(1, .01);\n    theta[2] ~ normal(-1, .01);\n    for (n in 1:N)\n    y[n] ~ bernoulli_logit(theta[j[n]] * beta[m[n]] - alpha[m[n]]);\n    }\"\nThis IRT model can be run using either the logistic or probit link function, however, since Stan has a built in bernoulli_logit, we‚Äôll use that. You can see from the model block above that we have specified specific prior distributions for theta[1] and theta[2]. These are our constrained legislators ‚Äì Agripino and Suplicy. We can do this using truncated normal distributions in Stan (i.e.¬†theta[1] ~ normal(1, .01)T[0,], for example), but in my experience this makes things slower and increases the number of divergent transitions reported by Stan. We then use the stan() command to run our model in Stan. Here, I‚Äôm using 1000 iterations just to show (as it doesn‚Äôt take too long); these IRT models generally need more iterations than other models, for good estimates from this data, I run 5000 iterations with 2500 burn-in. A couple of hundred iterations usually suffices in Stan, depending on the model. The number of chains and cores are linked to what I have available on my computer. You can check this with the parallel package using detectCores(). A quick way to check convergence of the chains is with a graph of Rhat, shown below.\nstan.fit &lt;- stan(model_code=stan.code, data=stan.data, iter=3000,\n                 warmup=1500, chains=4, thin=5, init=initF,\n                 verbose=TRUE, cores=4, seed=1234)\n\nstan_rhat(stan.fit, bins=60)\nValues of Rhat should be 1.03 or lower. As you can see, even from 1000 iterations, we can be confident these chains are converging."
  },
  {
    "objectID": "posts/BayesianIRT.html#graphing-ideal-points",
    "href": "posts/BayesianIRT.html#graphing-ideal-points",
    "title": "Bayesian IRT in R and Stan",
    "section": "Graphing Ideal Points",
    "text": "Graphing Ideal Points\nI find the best way to plot ideal points is by using ggplot2. It‚Äôs automatically loaded as part of rstan. I also prefer to use an mcmc.list object, simply because I‚Äôm more used to it. But you can use the stan.fit object directly if you prefer.\n\nMS &lt;- As.mcmc.list(stan.fit)\nsMS &lt;- summary(MS)\n\nThere are various things we can plot from the summary above. Of main interest is usually the ideal points, so we‚Äôll start with those first. First, let‚Äôs extract the ideal points (‚Äútheta‚Äù) from the summary, along with the lower and upper ends of the 95% credible interval:\n\nTheta &lt;- sMS$statistics[grep(\"theta\", row.names(sMS$statistics)),1]\nThetaQ &lt;- sMS$quantiles[grep(\"theta\", row.names(sMS$statistics)),c(1,5)]\nTheta &lt;- as.data.frame(cbind(Theta, ThetaQ))\nrm(ThetaQ)\nTheta$FullID &lt;- ldata$FullID\nrow.names(Theta) &lt;- NULL\ncolnames(Theta)[1:3] &lt;- c(\"Mean\", \"Lower\", \"Upper\")\nTheta &lt;- merge(Theta, ldata, by=\"FullID\")\nTheta &lt;- Theta[order(Theta$Mean),]\n\nNow we have a dataframe of legislator characteristics alng with their ideal points. Since we‚Äôre dealing with a one-dimensional model here, the most straight-forward way to plot is along a scale ranging from the lowest ideal point to the highest. Here, I‚Äôll colour the ideal points and their intervals by membership of the government coalition. I‚Äôve used some other plotting options to make this plot the way I like it, but it‚Äôs easy to change things to your taste in ggplot2.\n\nY &lt;- seq(from=1, to=length(Theta$Mean), by=1)\n\nggplot(Theta, aes(x=Mean, y=Y)) +\n  geom_point(aes(colour=GovCoalition),\n             shape=19, size=3) +\n  geom_errorbarh(aes(xmin = Lower, xmax = Upper,colour = GovCoalition),\n                 height = 0) +\n  geom_text(aes(x = Upper, label = FullID, colour = GovCoalition),\n            size = 2.5, hjust = -.05) +\n  scale_colour_manual(values = c(\"red\", \"blue\")) +\n  theme(axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title = element_blank(),\n        legend.position = \"none\",\n        panel.grid.major.y = element_blank(),\n        panel.grid.major.x = element_line(linetype = 1,\n                                          colour = \"grey\"),\n        panel.grid.minor = element_blank(),\n        panel.background = element_rect(fill = \"white\"),\n        panel.border = element_rect(colour = \"black\", fill = NA,\n                                    size = .4)) +\n  scale_x_continuous(limits = c(-2.7, 4))\n\n\nOf course, that‚Äôs not all the information we have in our ldata dataframe. We could plot things by party or by state. Let‚Äôs plot something by region (since there are a lot of states):\n\nSt &lt;- Theta[is.na(Theta$State)==FALSE,]  # take out president\nSt$Region &lt;- NA\nSE &lt;- c(\"SP\", \"RJ\", \"ES\", \"MG\")\nS &lt;- c(\"RS\", \"PR\", \"SC\")\nN &lt;- c(\"AM\", \"RO\", \"RR\", \"TO\", \"PA\", \"AC\", \"AP\")\nCW &lt;- c(\"DF\", \"GO\", \"MT\", \"MS\")\nNE &lt;- c(\"CE\", \"MA\", \"AL\", \"RN\", \"PB\", \"SE\", \"PI\", \"BA\", \"PE\")\nSt$Region[St$State %in% SE] &lt;- \"South-East\"\nSt$Region[St$State %in% S] &lt;- \"South\"\nSt$Region[St$State %in% NE] &lt;- \"North-East\"\nSt$Region[St$State %in% CW] &lt;- \"Centre-West\"\nSt$Region[St$State %in% N] &lt;- \"North\"\n\nnameorder &lt;- St$FullID[order(St$Region, St$Mean)]\nSt$FullID &lt;- factor(St$FullID, levels=nameorder)\n\nggplot(St, aes(x=Mean, y=FullID)) +\n  geom_point(size = 3, aes(colour = Region)) +\n  geom_errorbarh(aes(xmin = Lower, xmax = Upper, colour = Region),\n                 height = 0) +\n  facet_grid(Region ~ ., scales = \"free_y\") +\n  scale_colour_manual(values = c(\"orange\", \"black\", \"red\",\n                                 \"blue\", \"darkgreen\")) +\n  theme_bw()\n\n\nWe can also analyse the other parameters of the model, and run multidimensional models too. See the RPub for the code for these."
  },
  {
    "objectID": "posts/BayesianIRT.html#footnotes",
    "href": "posts/BayesianIRT.html#footnotes",
    "title": "Bayesian IRT in R and Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are many discussions on this topic, but Clinton & Jackman (2009) is a good place to start. An earlier paper by Clinton, Jackman & Rivers makes the point somewhat more forcefully.‚Ü©Ô∏é\nFor more on this point, see Jackman 2001, Rivers 2003 paper cited in the main text, or the Appendix of my PhD thesis.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/bayes-books.html",
    "href": "posts/bayes-books.html",
    "title": "Bayesian Stats: Book Recommendations",
    "section": "",
    "text": "The first time I came across Bayes‚Äô Theorem1, I must admit I was pretty confused. It was in Introductory Statistics by Neil A. Weiss, the course book in a statistics course I was taking at the time. Neither the logic of it nor the formula for it made much sense to me. For somebody new to probability, I was still trying to figure out what the hell P(A) actually meant.\nLooking back, the funny thing is that it is the branch of statistics that isn‚Äôt wont to use Bayes‚Äô Theorem that I find confusing2. Bayesian statistics now makes perfect sense to me. Indeed, it follows human intuition (even if the formula looks weird for anybody new to probability). The probability of the hypothesis we have in mind, given the data we observe (that would be the \\(P(A|B)\\) part, but we can rewrite it as Pr(Hypothesis|Data); this is also called the posterior ) is‚Ä¶ what exactly?\nWell, it‚Äôs a combination of the initial plausibility of our hypothesis (P(A), also called the prior ), multiplied by what‚Äôs called the likelihood (P(B|A) or Pr(Data|Hypothesis)), which is the data we would expect to see if our hypothesis were correct. The denominator is often called the ‚Äòevidence‚Äô or something similarly opaque, but it is merely the numerator plus its converse, that is to say, the probability that our original hypothesis is wrong by the likelihood of our data, assuming that our hypothesis is wrong. In any case, its function is just to make sure our probabilities sum to one, as they should.\nAs you can see, all of those P(|) and Pr can make things confusing ‚Äì underneath it all, it‚Äôs simpler. First of all, the Pr() notation and the denominator are often left out, making the theorem look more like this:\n\\[\n\\mathrm{posterior}\\propto\\mathrm{prior}\\times\\mathrm{likelihood}\n\\]\nThe plausibility of our idea, now that we have seen data, is proportional to its original plausibility times the likelihood. Well, reasonably simpler, but the fact is that most people are not comfortable thinking in terms of probability. Given that Bayes‚Äô Theorem is the basic foundation block for an entire body of statistical literature3, one can see how things could get out of hand pretty quickly ‚Äì hence the need for good books on the subject. I didn‚Äôt learn any Bayesian statistics in any class I ever had, I learned everything I know from reading plenty of books, sometimes the whole way through, sometimes just certain parts until I got bored, from reading on the web, from making many mistakes in R ‚Äì gradually, I found my way and built my understanding of Bayesian stats. Given that I read (or skimmed) quite a lot of books on the topic, I thought I‚Äôd share my two cents on those I came across. There are certainly many more, depending on the specific area of the sciences or on the level of technicality assumed, but these are the ones that I read and either loved, liked somewhat, got bored, or simply got lost (some of them are waaay too difficult‚Ä¶ at least for me). Let‚Äôs dive in. They‚Äôre in no particular order, by the way.\nProbably the first book that comes to mind is one of the first that I read on the topic, Simon Jackman‚Äôs Bayesian Analysis for the Social Sciences. Jackman‚Äôs book4 has a nice introduction to the topic and the first few chapters are reasonably easy to follow. However, his mathematics are detailed and even a bit pedantic (nothing wrong with that in an academic book) and things can get heavy going very quickly. Reading through derivations of the conjugacy of probability distributions convinced me I needed to a) go back and re-learn calculus again (which I did), and b) go a little further back in the tree of books on Bayesian statistics.\nThis led me down a few interesting paths, from de Finetti‚Äôs famous ‚ÄúPROBABILITY DOES NOT EXIST‚Äù statement, which I originally saw in Jackman‚Äôs book and then hunted down the original (I enjoyed the start quite a lot but then got bored), to learning JAGS to go along with Jackman‚Äôs examples, to some rather unnecessary and heavy-going books Tanner, for example). However, I decided to go the ‚Äòsource‚Äô (in a modern context) and so I started reading Bernardo and Smith‚Äôs Bayesian Theory, which really helped to give me a solid understanding of the concepts involved. It‚Äôs a detailed read, and well recommended if you want a deeper understanding of the concepts behind Bayesian statistics.\nMoving from concepts to application, I found Donald Berry‚Äôs Statistics: a Bayesian Perspective really useful for getting a grip on the basic elements. The book is a bit dated now, and is targeted at an undergraduate audience, which is actually something in its favour. Up until Kruschke and McElreath‚Äôs books (mentioned later), most Bayesian stats books seemed to be aimed at people who were already experts in statistics (or knowledgeable, at least), with the aim of convincing them why they should switch to Bayesian methods. As a result, a lot of these books dive headlong into subjects that are not appropriate for most students (see Jackman‚Äôs conjugacy discussions, above) and have the effect of turning a lot of students off the material. Berry‚Äôs book, although limited, does the opposite.\nThere are other statistics books that cover Bayesian ideas, such as Bolstad‚Äôs (I personally didn‚Äôt like his style) and deGroot & Schervish‚Äôs well-known book, which is a fine book, but very dry for my taste. There are also other introductory Bayesian statistics books, such as those by Lynch and Hoff, neither of which really stuck with me. Actually, most Bayesian stats books I read didn‚Äôt stick with me. (Lee also has an introductory text, but I haven‚Äôt read it.)\nSticking with introductory level, John Kruschke has a popular book with its quirky dog cover.\nI came a bit late to the Kruschke party, which meant that the two biggest advantages of the book (easy explanation of Bayesian stats and pre-written R functions) were not particularly useful to me, as I was already reasonably proficient in R and understood Bayesian stats quite well. Still, his book is very popular for a reason, which shows just how prevalent the problem of ‚Äòwriting-Bayesian-stats-books-for-people-who-are-already-awesome-at-statistics‚Äô is. However, I like to open things up and poke around, and his closed system of pre-written functions didn‚Äôt work so well for me (plus, the functions are quite badly written, in my opinion). Closed ecosystems of functions like this are a thing of the past (see Laplace‚Äôs Demon), the future is incorporating well-known methods and function calls with Bayesian machinery running under the hood (such as rstanarm). Anyway, for someone starting off, it‚Äôs a recommended read. Kruschke has some interesting papers too.\nThere are also some ‚Äòclassics‚Äô of Bayesian statistics, perhaps the most well-known being the canonical Bayesian Data Analysis by Andrew Gelman & co. I don‚Äôt know what keeps me away from this book. It‚Äôs very highly regarded (pretty much as the book on Bayesian stats) and well-written, and has a section on computation with Stan etc., but I just never seem to sit down and read it. Who knows why5.\nOf course, there are tons of books on this subject. I could literally fill a long blog post on the books I started and just didn‚Äôt like for whatever reason. There are some that are encyclopaedic (Congdon‚Äôs long list of Bayesian books, for example), others, designed for business students, that were just kind of ‚Äòmeh‚Äô: Marin & Robert and Rossi, for example. Others are useful for learning Bayesian methods and R, such as Jim Albert‚Äôs book, Bayesian Computation with R. This is a really useful book actually, but like I said earlier with reference to Kruschke, by the time I came to it, I was already using JAGS and on a different path and so I had no great use for Albert‚Äôs R functions. Still, it‚Äôs a well-regarded book by an acknowledged R expert. There are also good books on Bayesian econometrics (Koop) and time-series (Pole, West & Harrison, haaard) and Jeff Gill has one for the social & behavioural sciences (I really didn‚Äôt get into this); there are also many others throughout the specific fields of the sciences.\nThe emphasis on R is something that has carried through to the newer batch of Bayesian statistics books, which place more emphasis on the ‚Äòdata analysis‚Äô part (that is, being empirical instead of theoretical, and getting your hands dirty with computer programming in R from the off) than on theoretical underpinnings. You will find some targeted at a Python audience, for example, Davidson-Pilon‚Äôs book, which is available as an editable github-type-webbook on the net, as well as a printed book. (Its title will tell you a bit about the Python audience it aims for ‚Äì more computer programmers than the academic/statistician audience that use R. Indeed, most of the Python Bayesian analysis resources are found on the web as opposed to in books. Although some are just books on the web in pdf format.) For me personally, this is good news. I liked the theoretical knowledge that I gained from Bernardo & Smith, but once I had to delve into understanding matrix algebra (just to see a linear regression derivation) or complex characteristics of probability distributions, I was already thinking of how I‚Äôd rather have a beer. Programming, at least for me, is a perfect way to connect the theory and the concepts to the reality of actually doing some Bayesian analysis.\nThis brings me nicely to two books that I think really utilise this approach to good effect, one recent, one a decade or so old: Richard McElreath‚Äôs Statistical Rethinking (new) and Gelman & Hill‚Äôs Data Analysis Using Regression and Multilevel/Hierarchical Models (older). While I‚Äôve only recently started reading McElreath‚Äôs book, it seems like exactly the type of book that I would have liked to have when I started out. No complicated mathematics, just sensible advice and a heavy emphasis on doing analysis in R. The book is well-written (and very well backed up with references, there‚Äôs a ton of information to follow-up from the endnotes if you‚Äôre so inclined) and contains lucid arguments for why the author believes we need to approach statistics from a fresh angle. Although I haven‚Äôt finished it, I do recommend it already.\nIt‚Äôs similar in some ways to Gelman & Hill‚Äôs book, and one can see the influence of Andrew Gelman (particularly through his emphasis on the ‚Äòdoing‚Äô of Bayesian statistics) in Statistical Rethinking. Data Analysis Using‚Ä¶ is likewise focused on analysis, learned through computer programming. It features both frequentist and Bayesian takes on statistical methods, and contains detailed computer code for (the now somewhat dated) BUGS language (see here for why BUGS and its cousin JAGS are not always optimal for Bayesian analysis). It also contains some sage advice for researchers: try out simple models using quick methods like lm() as you build up your model (advice that I certainly needed on at least one occasion).\nSo that‚Äôs my take on Bayesian statistics/data analysis books. As befits the age we live in, you‚Äôll likely learn just as much from sites like Stack Overflow or from blog posts and RPubs and Github than you will from books. Academic papers often helped me more than books too. Still, a good book can teach you a hell of a lot in a consistent way. There are many referenced in this post, some better than others, but all have their qualities. For me specifically, someone who is not mad about reading lots of mathematics, the last two are my recommendations. For others, this will obviously be different (I know someone who loves Jackman‚Äôs book, for example).\nBy the way, there are some informal books on the subject, such as Nate Silver‚Äôs The Signal and the Noise, or McGrayne‚Äôs The Theory That Would Not Die. I‚Äôve given Amazon or publisher links for all these books, bar a few, but they can be found in other places too‚Ä¶you know what I‚Äôm talking about. Buy the ones you like, though! üëÆ"
  },
  {
    "objectID": "posts/bayes-books.html#footnotes",
    "href": "posts/bayes-books.html#footnotes",
    "title": "Bayesian Stats: Book Recommendations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMaybe this formula wouldn‚Äôt have made much sense to the ol‚Äô Reverend Thomas Bayes either, since he used the Newtonian style of geometric exposition. For the history of the theorem, see McGrayne. For a history of statistics in general, including Bayes (and Laplace, who probably did much more to develop Bayesian statistics than Bayes ever did) see the fantastic Stigler.‚Ü©Ô∏é\nThat would be ‚Äòfrequentist‚Äô or ‚Äòclassical‚Äô (or ‚Äòtraditional‚Äô) statistics (take your pick of adjective). Most of the Bayesian books above will have sections comparing the traditions. McElreath doesn‚Äôt bother, which is a nice development in its own way.‚Ü©Ô∏é\nAlthough there is only ever one core method: posterior‚ÄÑ.‚Ü©Ô∏é\nI‚Äôve learned much more from Jackman‚Äôs various political science articles, in which he uses Bayesian methods, than this book, to be honest.‚Ü©Ô∏é\nSpeaking of Gelman, he has a literal treasure trove of papers and web discussions on the subject of Bayesian data analysis. See his site for many links to those.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/update-r-from-inside-r.html",
    "href": "posts/update-r-from-inside-r.html",
    "title": "Update R from inside R",
    "section": "",
    "text": "I was just about to update R a while ago when I thought to myself that there must be a way to do this inside of R (RStudio, I mean). A quick Google search brought me to the installr package. Very nice, but I use a Mac. Hmmm‚Ä¶\nA bit more searching and I found Andrea Cirillo‚Äôs updateR package, which was made for OS X, fantastic. I tried it out, and although it worked great, I still had to leave RStudio to check to see if the latest version installed. I really liked Andrea‚Äôs package, so I thought I would suggest a few changes to him.\nBasically, now the function prints out a few more informative messages and makes it unnecessary to have to leave RStudio. I‚Äôve used it since then and all works great, so next time you need to update R on your Mac, you can do it from inside Rstudio ;-)"
  },
  {
    "objectID": "posts/showcol.html",
    "href": "posts/showcol.html",
    "title": "One liner to show all colours available in R",
    "section": "",
    "text": "Some years ago, I came across a great little repo that contained R code to display all the colours available in R. You can source it as so:\nsource(\"https://raw.githubusercontent.com/hdugan/rColorTable/master/rColorTable.R\")\nIt creates a two-page PDF that looks like this:\n\nSuper nice.\nBack then, I thought it would be cool to see how to this with ggplot2, so I wrote a script that does that, you can see it here. It prints a plot that looks like this:\n\nLovely. You have to love those names‚Ä¶‚Äòmistyrose‚Äô, ‚Äòwhitesmoke‚Äô‚Ä¶so nice. üë®üé®\nAnyway, I recently wanted to show the colours for a specific palette in the R console and I thought that there must be something new in the R universe to make this simpler, so after a little research, I realised the scales package has a lovely function, show_col(), which does exactly what I needed. I was comparing the colours #E8AE68 and #4D5061, which we can do easily:\nscales::show_col(c(\"#E8AE68\", \"#4D5061\"))\n\nFantabulous üé®\nThen I thought, this could be one-line replacement for all that code I used to use to see all the R colours! Which it is:\nscales::show_col(colours(), cex_label = .35)\n\nWowzers. You might have to mess with the cex_label parameter to get it juuuust right, but that‚Äôs not bad for one line of code. üëå"
  },
  {
    "objectID": "posts/test-train.html",
    "href": "posts/test-train.html",
    "title": "Avoiding the tiresome training & test data split",
    "section": "",
    "text": "I really don‚Äôt like splitting data into ‚Äòtrain‚Äô and ‚Äòtest‚Äô. I don‚Äôt mean that I‚Äôm against the idea of it, though you could say it‚Äôs a waste of data that could be used to better your model, but I mean that actual assignment in R of ‚Äòtrain‚Äô and ‚Äòtest‚Äô. I always liked destructuring in Python, and I like it a lot in 2018 JavaScript, so when I remembered that the zeallot package has it, I thought it would be a good opportunity to see how that could fit in a tidy modelling pipeline. Much to my delight, one little helper function later, it works perfectly.\nLoading packages and data:\nlibrary(recipes); library(rsample); library(tidyverse); library(zeallot)\n\n# load some data:\ndata(\"credit_data\")\nNext comes our little helper function:\nm_bake &lt;- function(recipe_object, data){\n  cd &lt;- initial_split(credit_data)\n  tr &lt;- training(cd)\n  te &lt;- testing(cd)\n  x1 &lt;- bake(recipe_object, tr)\n  x2 &lt;- bake(recipe_object, te)\n  return(list(x1, x2))\n}\nNow you get a nice clean pipeline with a train/test split as a result, using zeallot‚Äôs great %-&gt;% operator:\nrecipe(Status ~ ., data = credit_data) %&gt;%\n  step_knnimpute(all_predictors()) %&gt;%\n  step_center(all_numeric()) %&gt;%\n  step_dummy(-all_numeric()) %&gt;%\n  prep() %&gt;%\n  m_bake(data = credit_data) %-&gt;% c(train, test)\n\nls()\n## [1] \"credit_data\" \"m_bake\"      \"test\"        \"train\"\nhead(train); head(test)\n## # A tibble: 6 x 23\n##   Seniority  Time    Age Expenses Income Assets  Debt Amount Price\n##       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n## 1      1.01  13.6  -7.08    17.4   -12.6 -5429. -343. -239.  -617.\n## 2      9.01  13.6  20.9     -7.57  -10.6 -5429. -343.  -38.9  195.\n## 3     -7.99  13.6 -13.1      7.43   40.4 -2929. -343. -139.  -138.\n## 4     -7.99 -10.4 -11.1     -9.57  -34.6 -5429. -343. -729.  -553.\n## 5     -6.99  13.6  -1.08    19.4    72.4 -1929. -343. -389.   182.\n## 6     21.0   13.6   6.92    19.4   -16.6  4571. -343.  561.   337.\n## # ‚Ä¶ with 14 more variables: Home_other &lt;dbl&gt;, Home_owner &lt;dbl&gt;,\n## #   Home_parents &lt;dbl&gt;, Home_priv &lt;dbl&gt;, Home_rent &lt;dbl&gt;,\n## #   Marital_married &lt;dbl&gt;, Marital_separated &lt;dbl&gt;, Marital_single &lt;dbl&gt;,\n## #   Marital_widow &lt;dbl&gt;, Records_yes &lt;dbl&gt;, Job_freelance &lt;dbl&gt;,\n## #   Job_others &lt;dbl&gt;, Job_partime &lt;dbl&gt;, Status_good &lt;dbl&gt;\n## # A tibble: 6 x 23\n##   Seniority   Time      Age Expenses Income Assets   Debt Amount  Price\n##       &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n## 1      2.01 -10.4    8.92      34.4   58.4  -2429.  -343.   961.  1522.\n## 2     -1.99   1.56  -3.08       4.43 -16.6  -1429.  -343.   111.   114.\n## 3     25.0  -22.4   30.9        9.43  58.4   -429.  1657.  -439.  -113.\n## 4     -6.99  13.6   -6.08     -20.6   -4.65 -5429.  -343.   211.  -210.\n## 5     -7.99   1.56  -0.0804   -20.6  -23.8  -4629.  -343.   461.   387.\n## 6      6.01 -22.4   13.9       19.4   56.4  -4429.  -343.  -589.  -813.\n## # ‚Ä¶ with 14 more variables: Home_other &lt;dbl&gt;, Home_owner &lt;dbl&gt;,\n## #   Home_parents &lt;dbl&gt;, Home_priv &lt;dbl&gt;, Home_rent &lt;dbl&gt;,\n## #   Marital_married &lt;dbl&gt;, Marital_separated &lt;dbl&gt;, Marital_single &lt;dbl&gt;,\n## #   Marital_widow &lt;dbl&gt;, Records_yes &lt;dbl&gt;, Job_freelance &lt;dbl&gt;,\n## #   Job_others &lt;dbl&gt;, Job_partime &lt;dbl&gt;, Status_good &lt;dbl&gt;\nLovely.\n(Ok, I still had to split it (!!), but once you write this function, you can just call this.)"
  },
  {
    "objectID": "posts/gh-blog-rstudio-hugo.html",
    "href": "posts/gh-blog-rstudio-hugo.html",
    "title": "How to make a GitHub pages blog with RStudio and Hugo",
    "section": "",
    "text": "Update: for some people who may have some issues setting up the blog the way I‚Äôve set out here, see Kate‚Äôs helpful comments below.\nSince April or so of last year, I‚Äôve had a personal website on GitHub pages, where I keep this blog and a few other things. Setting it up was at times frustrating, but a good learning process (I especially picked up a lot of Git through that experience). Actually writing, not so good. I picked a theme I liked, all good, but I soon realised that writing as I often do in R Markdown was a little less convenient in the GitHub-flavoured markdown \\(\\rightarrow\\) jekyll \\(\\rightarrow\\) theme \\(\\rightarrow\\) website process. This post is an explanation of why I moved to Hugo and specifically to blogdown. I found the guides on the process (blogdown-focused, that is) to have some errors1, so I will explain each step in getting your own GitHub pages blog, built by blogdown in RStudio, for anybody who is interested. Feel free to skip to the setup guide to get started."
  },
  {
    "objectID": "posts/gh-blog-rstudio-hugo.html#how-to-set-up-your-own-github-pages-blog-and-post-using-blogdown",
    "href": "posts/gh-blog-rstudio-hugo.html#how-to-set-up-your-own-github-pages-blog-and-post-using-blogdown",
    "title": "How to make a GitHub pages blog with RStudio and Hugo",
    "section": "How to set up your own GitHub pages blog and post using blogdown",
    "text": "How to set up your own GitHub pages blog and post using blogdown\n1: This is not a Git tutorial. Lots of those exist already; here I will be using git commands in Terminal (I write on a macbook) without much explanation.\n2: Neither is this a Hugo or R Markdown tutorial. Those exist already, too.\nOk, let‚Äôs get started. I‚Äôm assuming you have RStudio and all that installed.\n\nblogdown and Hugo\nFirst, open up RStudio. If you don‚Äôt have the devtools package installed, install it (install.packages(\"devtools\")). Now, using this, we install blogdown (devtools::install_github('rstudio/blogdown')).\nNext, library(blogdown). We can use blogdown to install Hugo, but if you are using a mac, you may need to install homebrew first. It‚Äôs easy, just paste /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" into Terminal. If you don‚Äôt use Terminal or the command line much, now is a good time to start. It can be extremely useful.\nNow we can run install_hugo(). Next, we need to do some Git things.\n\n\nGitHub\nIf you do not have a GitHub account, you can open one here. Choose a good username, it will be the name of your GitHub pages website, unless you pay for a seperate domain name.\nOn your GitHub home page, there is a + sign with a downward arrow in the upper-right-hand corner. Click this and go to ‚ÄòNew Repository‚Äô. This new repo will be the repo for your website. For example, mine is RobertMyles.github.io, since my username is RobertMyles. (So the entire repo address is RobertMyles/RobertMyles.github.io). Click ‚ÄòCreate Repository‚Äô and we‚Äôre good to ‚Äúclone‚Äù.\nOn the page of the repo, there is an option to ‚ÄúClone or Download‚Äù on the right-hand side. Clone with HTTPS by copying the url of the repo to your clipboard. In Terminal, or whatever command line tool you use, type: - git clone &lt;url&gt;, where &lt;url&gt; is the url you just copied. (By the way, for first time Git use, you need to authenticate your machine. Details here.) - cd &lt;directory&gt;, where &lt;directory&gt; is the name of the folder that you just copied from GitHub. If you don‚Äôt know how cd works, you will need to be in the enclosing folder for that bit of script to work (cd moves the working directory like setwd() in R). What we‚Äôre doing is moving into the new folder that we have just cloned.\nThis folder may or may not have a ‚ÄúREADME.md‚Äù file, depending on whether you chose that option when you set it up. It will have an invisible .git folder. If it does have a ‚ÄúREADME‚Äù, delete it, as blogdown needs an empty directory to start. If you do delete this file, you will need to run the following git commands afterwards:\ngit add .\ngit commit -m 'delete README.md'\ngit push -u origin master\nBefore we move back to blogdown, you need to create another repository, one that will host all of the hugo/blogdown content. I called this one ‚Äúwebsite-hugo‚Äù, but‚Äôs what it‚Äôs called isn‚Äôt important (short and easy to type is better). Like we did with the first repo, clone this one on your computer (a ‚Äúlocal‚Äù repo, in Git-world)."
  },
  {
    "objectID": "posts/gh-blog-rstudio-hugo.html#blogdown-again",
    "href": "posts/gh-blog-rstudio-hugo.html#blogdown-again",
    "title": "How to make a GitHub pages blog with RStudio and Hugo",
    "section": "Blogdown (again)",
    "text": "Blogdown (again)\nIn Rstudio, either set your working directory to the Hugo folder that we just cloned, or open an RStudio project in this folder. I don‚Äôt think it‚Äôs necessary to open a project, and anyway I keep getting problems trying to shut down projects, but remember to set the working directory each time you want to do something on the website (setwd(\"path/to/your/folder\")).\nOk, so in the console in RStudio, type new_site(). This will create a site for you with a theme adapted by Yihui that you can interactively tinker with. I quite like this theme, but if you fancy using another, there is a list here. The relevant function is install_theme(). All of the folders of the site should be now in the Hugo folder (not the username.github.io folder). If you‚Äôre happy with the theme (for now, it‚Äôs easy to get a new theme), then return to Terminal, cd to the Hugo folder if you are not already there (it might be easiest to return to your home folder first, with cd ~) and type rm -rf public.\n\nGit Submodules\nThis is where Git Submodules come in, and they are really useful. Forget about the other approach using orphaned masters and such, this is the best way to do it (and it‚Äôs here on the Hugo website). The idea of a Git submodule is that the repo you want to use can make use of a folder from another repo. Simple. So in our case, our website repo (e.g.¬†RobertMyles.github.io) will use the public folder from our Hugo repo as a submodule (that is, a folder inside it). There will be very little in your Hugo folder repo on GitHub, it should say something like ‚Äúpublic @ f8fdbff‚Äù where the public folder is.\nSo, the git command to get all this running is:\ngit submodule add -b master git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git public\nwhere &lt;username&gt; is your username.\nSo now I think it‚Äôs a good idea to write a little test blog post on your nice new blog. The blogdown command (there is more than one, see ?hugo_cmd) is new_post(). This function has some options for the name of the author and the type of file created (markdown or R Markdown). I didn‚Äôt want to set these things every time, so I opened up my .Rprofile file (you can create it on your home folder if you don‚Äôt have one. Use Terminal: cd ~; touch '.Rprofile') and added the following lines to it:\n## blogdown options:\noptions(blogdown.use.rmd = TRUE)\noptions(blogdown.author = 'Robert McDonnell')\nNow new_post() will automatically create an . Rmd and use me as the author. There are a couple of options you can use in the YAML front matter (the stuff between the --- lines at the top of the post). The ones I used for this post are below, and there is a full list here. I used draft: true while I was working on the post (no more _drafts crap, yay!), but when you want this to actually appear on your site, delete the line or set it to false.\n---\ntitle: How to make a GitHub pages blog with RStudio and Hugo\nauthor: Robert McDonnell\ndate: '2017-02-01'\ncategories:\n  - R\n  - GitHub\n  - Hugo\ntags:\n  - R\n  - GitHub\ndescription: 'How I produce this blog'\ndraft: true\n---\nOnce you‚Äôve written something, it‚Äôs time to go back to our buddy Terminal. There are some suggestions for pushing to GitHub using custom builds in RStudio here, but I found that this was more effort than it was worth (like I said, knowing the command line is a good thing) and anyway gave errors about permissions (I fully admit this may have been my own fault). The Hugo tutorial I linked to earlier suggests using a small bash script, but since it‚Äôs so small we can just as easily do it in Terminal. First, we move to the public folder (remember, we are in the Hugo repo folder) and add our changes to be committed:\ncd public\ngit add -A\nNext, we commit and push to GitHub (that‚Äôs not so hard now, is it?)\ngit commit -m 'lovely new site'\ngit push origin master\nNow go check out your https://&lt;username&gt;.github.io website!\nPosts are easy, just run the new_post() command again. You can preview the page with serve_site() (or click on ‚ÄúLive Preview Site‚Äù on the Addins button in Rstudio). build_site() then builds the site with our pal Hugo, and the git commands above push it all to your repo and to the site.\n\n\nMigrating from jekyll\nIf you already have a jekyll-powered GitHub pages blog, you will have to make a few adjustments for Hugo. First of all, you may need to change any {% raw %} Liquid tags you had (I had to ), and secondly (and more importantly), you will have to include url: in the YAML to point to the old urls where the webpages currently (or previously, if you‚Äôve been reckless and already set up your new Hugo blog). For example, I had a blog post at robertmyles.github.io/ElectionsBR.html. After setting up this new version of the site with blogdown and Hugo, the blog post was at something like https://robertmyles.github.io/posts/ElectionsBR.html (I can‚Äôt remember exactly). So any links to the original post will now be broken, which is a shame since that post was quite popular on a Brazilian facebook group (‚ÄúM√©todos‚Äù, full of nice people who are interested in social science methods and stuff like R). So the YAML for the new version of the post now contains:\nurl: /ElectionsBR.html\nNote the leading slash. Perhaps aliases will be of use, it depends on the structure of your previous site.\n(Note: these repos no longer exist [May 2019]) To see the exact layout of my two repos, see https://github.com/RobertMyles/website-hugo and https://github.com/RobertMyles/RobertMyles.github.io. And please leave a comment or question if you have one. I‚Äôm sure I‚Äôll customise this theme I‚Äôve chosen, the comments are too dark in the code sections, for example. But that can wait for another day."
  },
  {
    "objectID": "posts/gh-blog-rstudio-hugo.html#footnotes",
    "href": "posts/gh-blog-rstudio-hugo.html#footnotes",
    "title": "How to make a GitHub pages blog with RStudio and Hugo",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are three guides that I found on the net, not including the Hugo tutorial, and a big thanks to the authors for taking the time to write those posts and share their experience. In the end I think they confused me more than they helped, but it was a learning process! One is here, by Jente Hidskes, which has some curious items in the bash scripts he provides (why the ‚ÄúGrabbing one file from the $SOURCE¬†branch so that a commit can be made‚Äù part, when ‚Äútouch file‚Äù would have been far easier, for example? And anyway, it didn‚Äôt work, for me, there is an error with git rm --cached from what I can figure out, and some other lines didn‚Äôt work.) Another is here, which is closest to what I did (apart from the Hugo tutorial), except it‚Äôs missing a whole host of info on blogdown, Hugo and particularly GitHub. It also takes the project-custom-build approach, which gave me problems. But nice site, Mark. The last one is here, which is lovely and detailed (thanks Amber for taking the time), although it depends on the (I think) faulty bash scripts of Jente. It is also overly complicated, there is no need for the orphan master process. Anyway, they all helped, so thanks folks.‚Ü©Ô∏é\nIf you are not using the default blogdown Hugo theme, you will have to enable mathjax, and include this piece of html in one of the html files in your layouts/partials files, preferably one that is loaded with every blog post, like footer or header. I put mine in footer.html.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/TFW-you-have-to-copy-and-paste-something.html",
    "href": "posts/TFW-you-have-to-copy-and-paste-something.html",
    "title": "TFW you have to copy and paste something into R‚Ä¶",
    "section": "",
    "text": "From time to time, you might need to copy and paste something into R and turn it into a character string. Maybe it‚Äôs something from the output of an error message, or from someone else‚Äôs malformed data, or something copied from a document or the internet. If it‚Äôs something small, then it‚Äôs usually OK to just manually insert \"\" around the strings and , between them. For something large, that‚Äôs just a nightmare.\nFor example, I ran into the global variables problem (again) recently with a package I was making. Only this time, it wasn‚Äôt the \".\" from a dplyr-style pipe chain that was giving me trouble, it was a ton of variables (You can see the mess here). Here‚Äôs a snippet of what I had to copy and paste:\nsenator_bills_details senator_commissions_date_joined\nsenator_commissions_name senator_date_of_birth senator_name\nsenator_office_address senator_party_date_joined senator_party_name\nsenator_positions_commission_date_start senator_suplente_name\nsenator_titular_name senator_vote session_date session_description\nsession_number session_type sigla sit_description situation_date\nsituation_place sponsor_abbr sponsor_name sponsor_party topic_general\ntopic_specific unit_name update_date vote_date vote_secret\ncountry_of_birth event_description event_type head id_rollcall\nSo what did I do? Well, a little hackiness with strsplit, gsub and cat got me sorted. First I put it all into a string:\nx &lt;- c(\"senator_bills_details senator_commissions_date_joined senator_commissions_name senator_date_of_birth senator_name senator_office_address senator_party_date_joined senator_party_name senator_positions_commission_date_start senator_suplente_name senator_titular_name senator_vote session_date session_description session_number session_type sigla sit_description situation_date situation_place sponsor_abbr sponsor_name sponsor_party topic_general topic_specific unit_name update_date vote_date vote_secret country_of_birth event_description event_type head id_rollcall\")\nThen we can use strsplit and unlist to get a character vector of the variables:\nx &lt;- strsplit(x, split = \" \")\nx &lt;- unlist(x)\nx[1:5]\n## [1] \"senator_bills_details\"           \"senator_commissions_date_joined\"\n## [3] \"\\nsenator_commissions_name\"      \"senator_date_of_birth\"\n## [5] \"senator_name\"\nAs you can see, some of these were separated by \" \", and others had \"\\n\" (newline) between them. With gsub, we can easily take that out. Don‚Äôt forget \\ is a special character in R and so needs to be escaped with another \\:\nx &lt;- gsub(\"\\\\n\", \"\", x)\nx[1:5]\n## [1] \"senator_bills_details\"           \"senator_commissions_date_joined\"\n## [3] \"senator_commissions_name\"        \"senator_date_of_birth\"\n## [5] \"senator_name\"\nOk, almost there. We could use print(x) and copy-and-paste the result into our desired result, but we would still have [1], [2] etc., and no commas between the terms. To get over this, we can use cat with its sep argument. \", \" will insert quotation marks at the end of the words and a comma between them:\ncat(x, sep = '\", \"')\n## senator_bills_details\", \"senator_commissions_date_joined\", \"senator_commissions_name\", \"senator_date_of_birth\", \"senator_name\", \"senator_office_address\", \"senator_party_date_joined\", \"senator_party_namesenator_positions_commission_date_start\", \"senator_suplente_namesenator_titular_name\", \"senator_vote\", \"session_date\", \"session_descriptionsession_number\", \"session_type\", \"sigla\", \"sit_description\", \"situation_datesituation_place\", \"sponsor_abbr\", \"sponsor_name\", \"sponsor_party\", \"topic_generaltopic_specific\", \"unit_name\", \"update_date\", \"vote_date\", \"vote_secretcountry_of_birth\", \"event_description\", \"event_type\", \"head\", \"id_rollcall\nThe only part left after this is to copy the output and place a comma at the start and the end. Irritating problem solved!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rob McDonnell:Blog",
    "section": "",
    "text": "Uk Elections 2024\n\n\n\n\n\n\npolitical science\n\n\n\n\n\n\n\n\n\nJul 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHow wet was winter 2023/2024?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nmodelscript\n\n\n\n\n\n\nmodelling\n\n\n\n\n\n\n\n\n\nJun 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do the mtcars actually look like?\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nMay 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nRender RMarkdown Code Chunks Based on Output Document Type\n\n\n\n\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\nApr 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFrom R to Gatsby\n\n\n\n\n\n\ngatsby.js\n\n\nblogging\n\n\n\n\n\n\n\n\n\nMar 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nImages as column headers in R\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nOne liner to show all colours available in R\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nFeb 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Docker for Data Science\n\n\n\n\n\n\ndocker\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBrazilian Legislative Data with congressbr\n\n\n\n\n\n\ndataViz\n\n\npolitical science\n\n\n\n\n\n\n\n\n\nJan 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nUK Elections 2019\n\n\n\n\n\n\npolitical science\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nDec 13, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nEasily Use Python and R together with {reticulate}\n\n\n\n\n\n\nR\n\n\npython\n\n\n\n\n\n\n\n\n\nNov 27, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTaking RStudio‚Äôs renv for a spin\n\n\n\n\n\n\npackage management\n\n\n\n\n\n\n\n\n\nAug 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nImproving your DataViz\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nJul 23, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing the Irish Divorce Referendum in R\n\n\n\n\n\n\ndataViz\n\n\npolitics\n\n\n\n\n\n\n\n\n\nMay 27, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nShuffling Strings in R\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMay 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nAvoiding the tiresome training & test data split\n\n\n\n\n\n\nmodelling\n\n\n\n\n\n\n\n\n\nSep 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Economic Partners with flagfillr\n\n\n\n\n\n\ndataViz\n\n\neconomics\n\n\n\n\n\n\n\n\n\nFeb 10, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nStan IRT Code\n\n\n\n\n\n\nIRT\n\n\nStan\n\n\n\n\n\n\n\n\n\nJan 5, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCustomize Interactive R Visuals in Power BI\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nDec 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nGauge-style plots with ggplot2\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nOct 24, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nUK Elections 2017\n\n\n\n\n\n\npolitical science\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Prison Data in R\n\n\n\n\n\n\ndataViz\n\n\npolitical science\n\n\n\n\n\n\n\n\n\nJul 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nTFW you have to copy and paste something into R‚Ä¶\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nApr 22, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate R from inside R\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nPeace, Bread and Data!\n\n\n\n\n\n\ndataViz\n\n\neconomics\n\n\n\n\n\n\n\n\n\nFeb 19, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to make a GitHub pages blog with RStudio and Hugo\n\n\n\n\n\n\nblogging\n\n\n\n\n\n\n\n\n\nFeb 1, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nTips and Tricks for R Markdown html\n\n\n\n\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\nJan 2, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSuicides in Ireland\n\n\n\n\n\n\npolitics\n\n\n\n\n\n\n\n\n\nDec 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nTheme-Specific Voting in the European Parliament\n\n\n\n\n\n\ndataViz\n\n\npolitical science\n\n\n\n\n\n\n\n\n\nOct 20, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nMap-making with R and electionsBR\n\n\n\n\n\n\npolitical science\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nOct 9, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nRe-creating Plots from The Economist in R and ggplot2\n\n\n\n\n\n\ndataViz\n\n\neconomics\n\n\n\n\n\n\n\n\n\nAug 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nInhaling/Boozing Earth\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nRating R Packages\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nAug 13, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nGeo-reference an image in R\n\n\n\n\n\n\ndataViz\n\n\n\n\n\n\n\n\n\nAug 13, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nEasier web scraping in R\n\n\n\n\n\n\nwebscraping\n\n\n\n\n\n\n\n\n\nAug 5, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian IRT in R and Stan\n\n\n\n\n\n\nIRT\n\n\nStan\n\n\nMCMC\n\n\n\n\n\n\n\n\n\nMay 21, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Stats: Book Recommendations\n\n\n\n\n\n\nBayesian stats\n\n\n\n\n\n\n\n\n\nMay 3, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Navigation in R with RSelenium\n\n\n\n\n\n\nwebscraping\n\n\n\n\n\n\n\n\n\nApr 27, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nWrite your thesis or paper in R Markdown!\n\n\n\n\n\n\nrmarkdown\n\n\n\n\n\n\n\n\n\nApr 15, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nStan or JAGS for Bayesian ideal-point IRT?\n\n\n\n\n\n\nIRT\n\n\nStan\n\n\nMCMC\n\n\n\n\n\n\n\n\n\nApr 13, 2016\n\n\n\n\n\n\nNo matching items"
  }
]